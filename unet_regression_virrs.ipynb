{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aV1xZ1CPi3Nw"
      },
      "source": [
        "<table class=\"ee-notebook-buttons\" align=\"left\"><td>\n",
        "<a target=\"_blank\"  href=\"http://colab.research.google.com/github/google/earthengine-community/blob/master/guides/linked/UNET_regression_demo.ipynb\">\n",
        "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" /> Run in Google Colab</a>\n",
        "</td><td>\n",
        "<a target=\"_blank\"  href=\"https://github.com/google/earthengine-community/blob/master/guides/linked/UNET_regression_demo.ipynb\"><img width=32px src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" /> View source on GitHub</a></td></table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# VIIRS High Resolution Nightlights Prediction \n",
        "with Landsat 8, Sentinel-2, and UNET"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## To Do\n",
        "\n",
        "Decide a model to Use\n",
        "- [x] Figure out how to extract image patches for model training\n",
        "- [x] Extract Sentinel 2 and Planet Data\n",
        "- [x] Figure out which type of model to use\n",
        "- [x] Figure out how to run one of these scripts  \n",
        "\n",
        "Get a Demo Notebook Running\n",
        "- [x] Get the notebook running locally with the tfrecords saved locally  \n",
        "- [x] Train a model with small subset of the data  \n",
        "- [x] Get the model prediction and inference code to run with local files\n",
        "- [x] Get a simple version of the model trained  \n",
        "\n",
        "Modify Notebook to Run with Nightlights\n",
        "- [x] Substitute NLCD with nightlights  \n",
        "- [x] Retrain a simple model with the new data  \n",
        "\n",
        "Get the Model Running on the Lab Computer\n",
        "- [x] Set up a python env on the lab computer  \n",
        "- [x] Set up SSH on the lab computer\n",
        "- [x] Adapt script to run on lab computer\n",
        "- [x] Run on lab computer  \n",
        "\n",
        "Improve the Model \n",
        "- [x] Visualize the model accuracy by epoch\n",
        "- [x] Compare a prediction to the actual image\n",
        "- [x] Create a difference image\n",
        "- [x] Scale up the model with original image patches\n",
        "\n",
        "Figure out why the model didn't train well..\n",
        "- [x] Resetup model to run with the small image patches - got to RMSE 0.16\n",
        "- [x] Try to set up with image patches in SSA - won't learn :/\n",
        "- [x] Try adjusting the optimizer\n",
        "- [x] Try with smaller image patches in SSA just arond cities\n",
        "\n",
        "Last Round:\n",
        "- [x] Update Africa large to evaluate on small image patches\n",
        "- [x] Compare performance of large and small model\n",
        "- [x] Try with 16 cities with same samples per city\n",
        "\n",
        "Generate Imagery and Figures:\n",
        "- [ ] Run predictions on the validation sets as well\n",
        "\n",
        "Final Parameters:\n",
        "- \\# of epochs\n",
        "- \\# of training samples\n",
        "- \\# size of training patches\n",
        "\n",
        "- [ ] Create a standardized test set\n",
        "- [ ] Try training with the small image patches, compare to the large patch validation\n",
        "- [ ] Actually, evaluate the large model on the small patches to compare performance\n",
        "- [ ] Try doubling the number of patches with more cities\n",
        "- [ ] Try adding in rural areas\n",
        "\n",
        "\n",
        "\n",
        "Rescale\n",
        "- [ ] Setup with larger image patches again\n",
        "- [ ] Substitute Landsat 8 with Sentinel 2  \n",
        "- [ ] Determine hyperparameters  \n",
        "\n",
        "If Time Allows...\n",
        "- [ ] Consider running again with Planet  \n",
        "- [ ] Consider running again with a ViTs\n",
        "- [ ] Look wealth instead of nightlights data  \n",
        "- [ ] Look at effect of dataset size on model performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SHAc5qbiR8l"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "This is an Earth Engine <> TensorFlow notebook.  Suppose you want to predict a continuous output (regression) from a stack of continuous inputs.  In this example, the output is VIIRS Day/Night Band from [VIIRS](https://developers.google.com/earth-engine/datasets/catalog/NOAA_VIIRS_DNB_MONTHLY_V1_VCMSLCFG) and the input is a Landsat 8 composite.  The model is a [fully convolutional neural network (FCNN)](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Long_Fully_Convolutional_Networks_2015_CVPR_paper.pdf), specifically [U-net](https://arxiv.org/abs/1505.04597). In this notebook, we:\n",
        "\n",
        "1.   Export train/test patches from Earth Engine, suitable for training an FCNN model.\n",
        "2.   Preprocess the data. \n",
        "3.   Train and validating an FCNN model.\n",
        "4.   Maki predictions with the trained model and import them to Earth Engine."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MJ4kW1pEhwP"
      },
      "source": [
        "# Setup software libraries\n",
        "\n",
        "Authenticate and import as necessary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OrderedDict([('GCS_PROJECT', 'master-thesis-ilg'), ('BUCKET', 'csci5922-proj'), ('HOME_PATH', '/home/isly9493/'), ('LOCAL_PATH', '/local/isly9493/'), ('DATA_PATH', 'csci5922/proj/data/'), ('FOLDER', 'viirs-africa-big/'), ('GCLOUD_PATH', '/home/isly9493/google-cloud-sdk/bin/'), ('CUDA_VISIBLE_DEVICES', '1')])\n"
          ]
        }
      ],
      "source": [
        "# Read in the .env file\n",
        "from dotenv import dotenv_values, load_dotenv\n",
        "\n",
        "# load env variables\n",
        "config = dotenv_values(\".env\")\n",
        "load_dotenv()\n",
        "\n",
        "# set env variables\n",
        "GCS_PROJECT = config['GCS_PROJECT']\n",
        "BUCKET = config['BUCKET']\n",
        "HOME_PATH = config['HOME_PATH']\n",
        "LOCAL_PATH = config['LOCAL_PATH']\n",
        "DATA_PATH = config['DATA_PATH']\n",
        "FOLDER = config['FOLDER']\n",
        "GCLOUD_PATH = config['GCLOUD_PATH']\n",
        "CUDA_VISIBLE_DEVICES = config['CUDA_VISIBLE_DEVICES']\n",
        "\n",
        "print(config)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jat01FEoUMqg"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-12-11 22:49:19.587367: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-12-11 22:49:19.587394: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-12-11 22:49:19.588047: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-12-11 22:49:19.591783: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-12-11 22:49:20.091414: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.15.0\n",
            "0.15.1\n"
          ]
        }
      ],
      "source": [
        "# Import, authenticate and initialize the Earth Engine library.\n",
        "import ee\n",
        "ee.Authenticate()\n",
        "ee.Initialize()\n",
        "# Tensorflow setup.\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "# Folium setup.\n",
        "import folium\n",
        "print(folium.__version__)\n",
        "# TODO substitude with geemap"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iT8ycmzClYwf"
      },
      "source": [
        "# Variables\n",
        "\n",
        "Declare the variables that will be in use throughout the notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKs6HuxOzjMl"
      },
      "source": [
        "## Specify your Cloud Storage Bucket\n",
        "You must have write access to a bucket to run this notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmfKLl9XcnGJ"
      },
      "source": [
        "## Set other global variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "psz7wJKalaoj"
      },
      "outputs": [],
      "source": [
        "# variables for image names\n",
        "TRAINING_BASE = 'training_patches'\n",
        "EVAL_BASE = 'eval_patches'\n",
        "\n",
        "# Specify inputs (Landsat bands) to the model and the response variable.\n",
        "opticalBands = ['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7']\n",
        "thermalBands = ['B10', 'B11']\n",
        "BANDS = opticalBands + thermalBands\n",
        "RESPONSE = 'avg_rad' ##### \n",
        "FEATURES = BANDS + [RESPONSE]\n",
        "\n",
        "# Specify the size and shape of patches expected by the model.\n",
        "KERNEL_SIZE = 256\n",
        "KERNEL_SHAPE = [KERNEL_SIZE, KERNEL_SIZE]\n",
        "COLUMNS = [\n",
        "  tf.io.FixedLenFeature(shape=KERNEL_SHAPE, dtype=tf.float32) for k in FEATURES\n",
        "]\n",
        "FEATURES_DICT = dict(zip(FEATURES, COLUMNS))\n",
        "\n",
        "# Sizes of the training and evaluation datasets.\n",
        "TRAIN_SIZE = 48000 # orignally 16000, use 800 for small version, 8000 for medium version\n",
        "EVAL_SIZE = 200 # originally 8000, use 200 for small version\n",
        "\n",
        "# Specify model training parameters.\n",
        "BATCH_SIZE = 160 # orignally 16\n",
        "EPOCHS = 5 # originally 10\n",
        "BUFFER_SIZE = 2000 # originally 2000\n",
        "OPTIMIZER = 'SGD'\n",
        "LOSS = 'MeanSquaredError'\n",
        "METRICS = ['RootMeanSquaredError']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgoDc7Hilfc4"
      },
      "source": [
        "# Create and Stack Input + Target Imagery\n",
        "\n",
        "Gather and setup the imagery to use for inputs (predictors).  This is a three-year, cloud-free, Landsat 8 composite.  Display it in the notebook for a sanity check."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "-IlgXu-vcUEY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B10', 'B11']\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div style=\"width:100%;\"><div style=\"position:relative;width:100%;height:0;padding-bottom:60%;\"><span style=\"color:#565656\">Make this Notebook Trusted to load map: File -> Trust Notebook</span><iframe srcdoc=\"&lt;!DOCTYPE html&gt;\n",
              "&lt;html&gt;\n",
              "&lt;head&gt;\n",
              "    \n",
              "    &lt;meta http-equiv=&quot;content-type&quot; content=&quot;text/html; charset=UTF-8&quot; /&gt;\n",
              "    \n",
              "        &lt;script&gt;\n",
              "            L_NO_TOUCH = false;\n",
              "            L_DISABLE_3D = false;\n",
              "        &lt;/script&gt;\n",
              "    \n",
              "    &lt;style&gt;html, body {width: 100%;height: 100%;margin: 0;padding: 0;}&lt;/style&gt;\n",
              "    &lt;style&gt;#map {position:absolute;top:0;bottom:0;right:0;left:0;}&lt;/style&gt;\n",
              "    &lt;script src=&quot;https://cdn.jsdelivr.net/npm/leaflet@1.9.3/dist/leaflet.js&quot;&gt;&lt;/script&gt;\n",
              "    &lt;script src=&quot;https://code.jquery.com/jquery-3.7.1.min.js&quot;&gt;&lt;/script&gt;\n",
              "    &lt;script src=&quot;https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/js/bootstrap.bundle.min.js&quot;&gt;&lt;/script&gt;\n",
              "    &lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.js&quot;&gt;&lt;/script&gt;\n",
              "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/leaflet@1.9.3/dist/leaflet.css&quot;/&gt;\n",
              "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/css/bootstrap.min.css&quot;/&gt;\n",
              "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://netdna.bootstrapcdn.com/bootstrap/3.0.0/css/bootstrap.min.css&quot;/&gt;\n",
              "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.2.0/css/all.min.css&quot;/&gt;\n",
              "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.css&quot;/&gt;\n",
              "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/gh/python-visualization/folium/folium/templates/leaflet.awesome.rotate.min.css&quot;/&gt;\n",
              "    \n",
              "            &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width,\n",
              "                initial-scale=1.0, maximum-scale=1.0, user-scalable=no&quot; /&gt;\n",
              "            &lt;style&gt;\n",
              "                #map_98f86ff79a995d2d7dc5be9165346802 {\n",
              "                    position: relative;\n",
              "                    width: 100.0%;\n",
              "                    height: 100.0%;\n",
              "                    left: 0.0%;\n",
              "                    top: 0.0%;\n",
              "                }\n",
              "                .leaflet-container { font-size: 1rem; }\n",
              "            &lt;/style&gt;\n",
              "        \n",
              "&lt;/head&gt;\n",
              "&lt;body&gt;\n",
              "    \n",
              "    \n",
              "            &lt;div class=&quot;folium-map&quot; id=&quot;map_98f86ff79a995d2d7dc5be9165346802&quot; &gt;&lt;/div&gt;\n",
              "        \n",
              "&lt;/body&gt;\n",
              "&lt;script&gt;\n",
              "    \n",
              "    \n",
              "            var map_98f86ff79a995d2d7dc5be9165346802 = L.map(\n",
              "                &quot;map_98f86ff79a995d2d7dc5be9165346802&quot;,\n",
              "                {\n",
              "                    center: [-1.0, 37.0],\n",
              "                    crs: L.CRS.EPSG3857,\n",
              "                    zoom: 6,\n",
              "                    zoomControl: true,\n",
              "                    preferCanvas: false,\n",
              "                }\n",
              "            );\n",
              "\n",
              "            \n",
              "\n",
              "        \n",
              "    \n",
              "            var tile_layer_aa25b81dcb4a8c142ed27afd112f73ce = L.tileLayer(\n",
              "                &quot;https://tile.openstreetmap.org/{z}/{x}/{y}.png&quot;,\n",
              "                {&quot;attribution&quot;: &quot;\\u0026copy; \\u003ca href=\\&quot;https://www.openstreetmap.org/copyright\\&quot;\\u003eOpenStreetMap\\u003c/a\\u003e contributors&quot;, &quot;detectRetina&quot;: false, &quot;maxNativeZoom&quot;: 19, &quot;maxZoom&quot;: 19, &quot;minZoom&quot;: 0, &quot;noWrap&quot;: false, &quot;opacity&quot;: 1, &quot;subdomains&quot;: &quot;abc&quot;, &quot;tms&quot;: false}\n",
              "            );\n",
              "        \n",
              "    \n",
              "            tile_layer_aa25b81dcb4a8c142ed27afd112f73ce.addTo(map_98f86ff79a995d2d7dc5be9165346802);\n",
              "        \n",
              "    \n",
              "            var tile_layer_fd0c2021973f89b5558ef860ef9575d7 = L.tileLayer(\n",
              "                &quot;https://earthengine.googleapis.com/v1/projects/earthengine-legacy/maps/310f3b2426b7bf7f16371db8e9f1c0fa-e7d709cf4bfe825c49c980c349f8e16c/tiles/{z}/{x}/{y}&quot;,\n",
              "                {&quot;attribution&quot;: &quot;Map Data \\u0026copy; \\u003ca href=\\&quot;https://earthengine.google.com/\\&quot;\\u003eGoogle Earth Engine\\u003c/a\\u003e&quot;, &quot;detectRetina&quot;: false, &quot;maxNativeZoom&quot;: 18, &quot;maxZoom&quot;: 18, &quot;minZoom&quot;: 0, &quot;noWrap&quot;: false, &quot;opacity&quot;: 1, &quot;subdomains&quot;: &quot;abc&quot;, &quot;tms&quot;: false}\n",
              "            );\n",
              "        \n",
              "    \n",
              "            tile_layer_fd0c2021973f89b5558ef860ef9575d7.addTo(map_98f86ff79a995d2d7dc5be9165346802);\n",
              "        \n",
              "    \n",
              "            var tile_layer_70c0cb727a44bc9ac73da1589800ba37 = L.tileLayer(\n",
              "                &quot;https://earthengine.googleapis.com/v1/projects/earthengine-legacy/maps/7a203266941255964bbbf00c04322f82-239d5e1ca1d917ee7350e53a7f38e3c8/tiles/{z}/{x}/{y}&quot;,\n",
              "                {&quot;attribution&quot;: &quot;Map Data \\u0026copy; \\u003ca href=\\&quot;https://earthengine.google.com/\\&quot;\\u003eGoogle Earth Engine\\u003c/a\\u003e&quot;, &quot;detectRetina&quot;: false, &quot;maxNativeZoom&quot;: 18, &quot;maxZoom&quot;: 18, &quot;minZoom&quot;: 0, &quot;noWrap&quot;: false, &quot;opacity&quot;: 1, &quot;subdomains&quot;: &quot;abc&quot;, &quot;tms&quot;: false}\n",
              "            );\n",
              "        \n",
              "    \n",
              "            tile_layer_70c0cb727a44bc9ac73da1589800ba37.addTo(map_98f86ff79a995d2d7dc5be9165346802);\n",
              "        \n",
              "    \n",
              "            var layer_control_d07000d2483517738148513affff7239_layers = {\n",
              "                base_layers : {\n",
              "                    &quot;openstreetmap&quot; : tile_layer_aa25b81dcb4a8c142ed27afd112f73ce,\n",
              "                },\n",
              "                overlays :  {\n",
              "                    &quot;median composite&quot; : tile_layer_fd0c2021973f89b5558ef860ef9575d7,\n",
              "                    &quot;thermal&quot; : tile_layer_70c0cb727a44bc9ac73da1589800ba37,\n",
              "                },\n",
              "            };\n",
              "            let layer_control_d07000d2483517738148513affff7239 = L.control.layers(\n",
              "                layer_control_d07000d2483517738148513affff7239_layers.base_layers,\n",
              "                layer_control_d07000d2483517738148513affff7239_layers.overlays,\n",
              "                {&quot;autoZIndex&quot;: true, &quot;collapsed&quot;: true, &quot;position&quot;: &quot;topright&quot;}\n",
              "            ).addTo(map_98f86ff79a995d2d7dc5be9165346802);\n",
              "\n",
              "        \n",
              "&lt;/script&gt;\n",
              "&lt;/html&gt;\" style=\"position:absolute;width:100%;height:100%;left:0;top:0;border:none !important;\" allowfullscreen webkitallowfullscreen mozallowfullscreen></iframe></div></div>"
            ],
            "text/plain": [
              "<folium.folium.Map at 0x7f1070f26d10>"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Use Landsat 8 surface reflectance data.\n",
        "l8sr = ee.ImageCollection('LANDSAT/LC08/C01/T1_SR')\n",
        "\n",
        "# Cloud masking function.\n",
        "def maskL8sr(image):\n",
        "  cloudShadowBitMask = ee.Number(2).pow(3).int()\n",
        "  cloudsBitMask = ee.Number(2).pow(5).int()\n",
        "  qa = image.select('pixel_qa')\n",
        "  mask1 = qa.bitwiseAnd(cloudShadowBitMask).eq(0).And(\n",
        "    qa.bitwiseAnd(cloudsBitMask).eq(0))\n",
        "  mask2 = image.mask().reduce('min')\n",
        "  mask3 = image.select(opticalBands).gt(0).And(\n",
        "          image.select(opticalBands).lt(10000)).reduce('min')\n",
        "  mask = mask1.And(mask2).And(mask3)\n",
        "  return image.select(opticalBands).divide(10000).addBands(\n",
        "          image.select(thermalBands).divide(10).clamp(273.15, 373.15)\n",
        "            .subtract(273.15).divide(100)).updateMask(mask)\n",
        "\n",
        "# The image input data is a cloud-masked median composite.\n",
        "image = l8sr.filterDate('2020-01-01', '2022-12-31').map(maskL8sr).median()\n",
        "\n",
        "# printn out the bands\n",
        "print(image.bandNames().getInfo())\n",
        "\n",
        "# Use folium to visualize the imagery.\n",
        "mapid = image.getMapId({'bands': ['B4', 'B3', 'B2'], 'min': 0, 'max': 0.3})\n",
        "map = folium.Map(location=[-1, 37], zoom_start=6)\n",
        "folium.TileLayer(\n",
        "    tiles=mapid['tile_fetcher'].url_format,\n",
        "    attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "    overlay=True,\n",
        "    name='median composite',\n",
        "  ).add_to(map)\n",
        "\n",
        "mapid = image.getMapId({'bands': ['B10'], 'min': 0, 'max': 0.5})\n",
        "folium.TileLayer(\n",
        "    tiles=mapid['tile_fetcher'].url_format,\n",
        "    attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "    overlay=True,\n",
        "    name='thermal',\n",
        "  ).add_to(map)\n",
        "map.add_child(folium.LayerControl())\n",
        "map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHznnctkJsZJ"
      },
      "source": [
        "Prepare the response (what we want to predict).  This is nighttime brightness (in nanoWatts/sr/cm^2) from the VIIRS dataset.  Display to check."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "e0wHDyxVirec"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div style=\"width:100%;\"><div style=\"position:relative;width:100%;height:0;padding-bottom:60%;\"><span style=\"color:#565656\">Make this Notebook Trusted to load map: File -> Trust Notebook</span><iframe srcdoc=\"&lt;!DOCTYPE html&gt;\n",
              "&lt;html&gt;\n",
              "&lt;head&gt;\n",
              "    \n",
              "    &lt;meta http-equiv=&quot;content-type&quot; content=&quot;text/html; charset=UTF-8&quot; /&gt;\n",
              "    \n",
              "        &lt;script&gt;\n",
              "            L_NO_TOUCH = false;\n",
              "            L_DISABLE_3D = false;\n",
              "        &lt;/script&gt;\n",
              "    \n",
              "    &lt;style&gt;html, body {width: 100%;height: 100%;margin: 0;padding: 0;}&lt;/style&gt;\n",
              "    &lt;style&gt;#map {position:absolute;top:0;bottom:0;right:0;left:0;}&lt;/style&gt;\n",
              "    &lt;script src=&quot;https://cdn.jsdelivr.net/npm/leaflet@1.9.3/dist/leaflet.js&quot;&gt;&lt;/script&gt;\n",
              "    &lt;script src=&quot;https://code.jquery.com/jquery-3.7.1.min.js&quot;&gt;&lt;/script&gt;\n",
              "    &lt;script src=&quot;https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/js/bootstrap.bundle.min.js&quot;&gt;&lt;/script&gt;\n",
              "    &lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.js&quot;&gt;&lt;/script&gt;\n",
              "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/leaflet@1.9.3/dist/leaflet.css&quot;/&gt;\n",
              "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/css/bootstrap.min.css&quot;/&gt;\n",
              "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://netdna.bootstrapcdn.com/bootstrap/3.0.0/css/bootstrap.min.css&quot;/&gt;\n",
              "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.2.0/css/all.min.css&quot;/&gt;\n",
              "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.css&quot;/&gt;\n",
              "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/gh/python-visualization/folium/folium/templates/leaflet.awesome.rotate.min.css&quot;/&gt;\n",
              "    \n",
              "            &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width,\n",
              "                initial-scale=1.0, maximum-scale=1.0, user-scalable=no&quot; /&gt;\n",
              "            &lt;style&gt;\n",
              "                #map_f9b66e68143b414d3b2df7ce8fef3a52 {\n",
              "                    position: relative;\n",
              "                    width: 100.0%;\n",
              "                    height: 100.0%;\n",
              "                    left: 0.0%;\n",
              "                    top: 0.0%;\n",
              "                }\n",
              "                .leaflet-container { font-size: 1rem; }\n",
              "            &lt;/style&gt;\n",
              "        \n",
              "&lt;/head&gt;\n",
              "&lt;body&gt;\n",
              "    \n",
              "    \n",
              "            &lt;div class=&quot;folium-map&quot; id=&quot;map_f9b66e68143b414d3b2df7ce8fef3a52&quot; &gt;&lt;/div&gt;\n",
              "        \n",
              "&lt;/body&gt;\n",
              "&lt;script&gt;\n",
              "    \n",
              "    \n",
              "            var map_f9b66e68143b414d3b2df7ce8fef3a52 = L.map(\n",
              "                &quot;map_f9b66e68143b414d3b2df7ce8fef3a52&quot;,\n",
              "                {\n",
              "                    center: [-1.0, 37.0],\n",
              "                    crs: L.CRS.EPSG3857,\n",
              "                    zoom: 6,\n",
              "                    zoomControl: true,\n",
              "                    preferCanvas: false,\n",
              "                }\n",
              "            );\n",
              "\n",
              "            \n",
              "\n",
              "        \n",
              "    \n",
              "            var tile_layer_063d634ae30115daad2d93a6edd99da6 = L.tileLayer(\n",
              "                &quot;https://tile.openstreetmap.org/{z}/{x}/{y}.png&quot;,\n",
              "                {&quot;attribution&quot;: &quot;\\u0026copy; \\u003ca href=\\&quot;https://www.openstreetmap.org/copyright\\&quot;\\u003eOpenStreetMap\\u003c/a\\u003e contributors&quot;, &quot;detectRetina&quot;: false, &quot;maxNativeZoom&quot;: 19, &quot;maxZoom&quot;: 19, &quot;minZoom&quot;: 0, &quot;noWrap&quot;: false, &quot;opacity&quot;: 1, &quot;subdomains&quot;: &quot;abc&quot;, &quot;tms&quot;: false}\n",
              "            );\n",
              "        \n",
              "    \n",
              "            tile_layer_063d634ae30115daad2d93a6edd99da6.addTo(map_f9b66e68143b414d3b2df7ce8fef3a52);\n",
              "        \n",
              "    \n",
              "            var tile_layer_71b00f201e0dfa415b5eed175f099770 = L.tileLayer(\n",
              "                &quot;https://earthengine.googleapis.com/v1/projects/earthengine-legacy/maps/cd3a34ef7b3cb61d05ee334012d7a307-65128e95d0ea259b94e21a26cd12b690/tiles/{z}/{x}/{y}&quot;,\n",
              "                {&quot;attribution&quot;: &quot;Map Data \\u0026copy; \\u003ca href=\\&quot;https://earthengine.google.com/\\&quot;\\u003eGoogle Earth Engine\\u003c/a\\u003e&quot;, &quot;detectRetina&quot;: false, &quot;maxNativeZoom&quot;: 18, &quot;maxZoom&quot;: 18, &quot;minZoom&quot;: 0, &quot;noWrap&quot;: false, &quot;opacity&quot;: 1, &quot;subdomains&quot;: &quot;abc&quot;, &quot;tms&quot;: false}\n",
              "            );\n",
              "        \n",
              "    \n",
              "            tile_layer_71b00f201e0dfa415b5eed175f099770.addTo(map_f9b66e68143b414d3b2df7ce8fef3a52);\n",
              "        \n",
              "    \n",
              "            var layer_control_da026cbe2f063ba698e6bfdf1536903e_layers = {\n",
              "                base_layers : {\n",
              "                    &quot;openstreetmap&quot; : tile_layer_063d634ae30115daad2d93a6edd99da6,\n",
              "                },\n",
              "                overlays :  {\n",
              "                    &quot;viirs dnb&quot; : tile_layer_71b00f201e0dfa415b5eed175f099770,\n",
              "                },\n",
              "            };\n",
              "            let layer_control_da026cbe2f063ba698e6bfdf1536903e = L.control.layers(\n",
              "                layer_control_da026cbe2f063ba698e6bfdf1536903e_layers.base_layers,\n",
              "                layer_control_da026cbe2f063ba698e6bfdf1536903e_layers.overlays,\n",
              "                {&quot;autoZIndex&quot;: true, &quot;collapsed&quot;: true, &quot;position&quot;: &quot;topright&quot;}\n",
              "            ).addTo(map_f9b66e68143b414d3b2df7ce8fef3a52);\n",
              "\n",
              "        \n",
              "&lt;/script&gt;\n",
              "&lt;/html&gt;\" style=\"position:absolute;width:100%;height:100%;left:0;top:0;border:none !important;\" allowfullscreen webkitallowfullscreen mozallowfullscreen></iframe></div></div>"
            ],
            "text/plain": [
              "<folium.folium.Map at 0x7f1070ecbad0>"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "viirs = ee.ImageCollection('NOAA/VIIRS/DNB/MONTHLY_V1/VCMSLCFG').select('avg_rad').filterDate('2020-01-01', '2022-12-31').median()\n",
        "viirs = viirs.divide(2).float() # normalize to 0-1 range\n",
        "\n",
        "mapid = viirs.getMapId({'min': 0, 'max': 1}) # normally max 1.0\n",
        "map = folium.Map(location=[-1, 37], zoom_start=6)\n",
        "folium.TileLayer(\n",
        "    tiles=mapid['tile_fetcher'].url_format,\n",
        "    attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "    overlay=True,\n",
        "    name='viirs dnb',\n",
        "  ).add_to(map)\n",
        "map.add_child(folium.LayerControl())\n",
        "map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTS7_ZzPDhhg"
      },
      "source": [
        "Stack the 2D images (Landsat composite and VIIRS DNB image) to create a single image from which samples can be taken.  Convert the image into an array image in which each pixel stores 256x256 patches of pixels for each band.  This is a key step that bears emphasis: to export training patches, convert a multi-band image to [an array image](https://developers.google.com/earth-engine/arrays_array_images#array-images) using [`neighborhoodToArray()`](https://developers.google.com/earth-engine/api_docs#eeimageneighborhoodtoarray), then sample the image at points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "eGHYsdAOipa4"
      },
      "outputs": [],
      "source": [
        "featureStack = ee.Image.cat([\n",
        "  image.select(BANDS),\n",
        "  viirs.select(RESPONSE)\n",
        "]).float()\n",
        "\n",
        "list = ee.List.repeat(1, KERNEL_SIZE)\n",
        "lists = ee.List.repeat(list, KERNEL_SIZE)\n",
        "kernel = ee.Kernel.fixed(KERNEL_SIZE, KERNEL_SIZE, lists)\n",
        "\n",
        "arrays = featureStack.neighborhoodToArray(kernel)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4djSxBRG2el"
      },
      "source": [
        "# Define Training and Evaluation Areas\n",
        "Use some pre-made geometries to sample the stack in strategic locations.  Specifically, these are hand-made polygons in which to take the 256x256 samples.  Display the sampling polygons on a map, red for training polygons, blue for evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ure_WaD0itQY"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div style=\"width:100%;\"><div style=\"position:relative;width:100%;height:0;padding-bottom:60%;\"><span style=\"color:#565656\">Make this Notebook Trusted to load map: File -> Trust Notebook</span><iframe srcdoc=\"&lt;!DOCTYPE html&gt;\n",
              "&lt;html&gt;\n",
              "&lt;head&gt;\n",
              "    \n",
              "    &lt;meta http-equiv=&quot;content-type&quot; content=&quot;text/html; charset=UTF-8&quot; /&gt;\n",
              "    \n",
              "        &lt;script&gt;\n",
              "            L_NO_TOUCH = false;\n",
              "            L_DISABLE_3D = false;\n",
              "        &lt;/script&gt;\n",
              "    \n",
              "    &lt;style&gt;html, body {width: 100%;height: 100%;margin: 0;padding: 0;}&lt;/style&gt;\n",
              "    &lt;style&gt;#map {position:absolute;top:0;bottom:0;right:0;left:0;}&lt;/style&gt;\n",
              "    &lt;script src=&quot;https://cdn.jsdelivr.net/npm/leaflet@1.9.3/dist/leaflet.js&quot;&gt;&lt;/script&gt;\n",
              "    &lt;script src=&quot;https://code.jquery.com/jquery-3.7.1.min.js&quot;&gt;&lt;/script&gt;\n",
              "    &lt;script src=&quot;https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/js/bootstrap.bundle.min.js&quot;&gt;&lt;/script&gt;\n",
              "    &lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.js&quot;&gt;&lt;/script&gt;\n",
              "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/leaflet@1.9.3/dist/leaflet.css&quot;/&gt;\n",
              "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/css/bootstrap.min.css&quot;/&gt;\n",
              "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://netdna.bootstrapcdn.com/bootstrap/3.0.0/css/bootstrap.min.css&quot;/&gt;\n",
              "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.2.0/css/all.min.css&quot;/&gt;\n",
              "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.css&quot;/&gt;\n",
              "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/gh/python-visualization/folium/folium/templates/leaflet.awesome.rotate.min.css&quot;/&gt;\n",
              "    \n",
              "            &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width,\n",
              "                initial-scale=1.0, maximum-scale=1.0, user-scalable=no&quot; /&gt;\n",
              "            &lt;style&gt;\n",
              "                #map_58ccad1874e50230161e8a88c58d5cb8 {\n",
              "                    position: relative;\n",
              "                    width: 100.0%;\n",
              "                    height: 100.0%;\n",
              "                    left: 0.0%;\n",
              "                    top: 0.0%;\n",
              "                }\n",
              "                .leaflet-container { font-size: 1rem; }\n",
              "            &lt;/style&gt;\n",
              "        \n",
              "&lt;/head&gt;\n",
              "&lt;body&gt;\n",
              "    \n",
              "    \n",
              "            &lt;div class=&quot;folium-map&quot; id=&quot;map_58ccad1874e50230161e8a88c58d5cb8&quot; &gt;&lt;/div&gt;\n",
              "        \n",
              "&lt;/body&gt;\n",
              "&lt;script&gt;\n",
              "    \n",
              "    \n",
              "            var map_58ccad1874e50230161e8a88c58d5cb8 = L.map(\n",
              "                &quot;map_58ccad1874e50230161e8a88c58d5cb8&quot;,\n",
              "                {\n",
              "                    center: [0.0, 12.0],\n",
              "                    crs: L.CRS.EPSG3857,\n",
              "                    zoom: 5,\n",
              "                    zoomControl: true,\n",
              "                    preferCanvas: false,\n",
              "                }\n",
              "            );\n",
              "\n",
              "            \n",
              "\n",
              "        \n",
              "    \n",
              "            var tile_layer_681c518d15df4f420b702eb772c072e4 = L.tileLayer(\n",
              "                &quot;https://tile.openstreetmap.org/{z}/{x}/{y}.png&quot;,\n",
              "                {&quot;attribution&quot;: &quot;\\u0026copy; \\u003ca href=\\&quot;https://www.openstreetmap.org/copyright\\&quot;\\u003eOpenStreetMap\\u003c/a\\u003e contributors&quot;, &quot;detectRetina&quot;: false, &quot;maxNativeZoom&quot;: 19, &quot;maxZoom&quot;: 19, &quot;minZoom&quot;: 0, &quot;noWrap&quot;: false, &quot;opacity&quot;: 1, &quot;subdomains&quot;: &quot;abc&quot;, &quot;tms&quot;: false}\n",
              "            );\n",
              "        \n",
              "    \n",
              "            tile_layer_681c518d15df4f420b702eb772c072e4.addTo(map_58ccad1874e50230161e8a88c58d5cb8);\n",
              "        \n",
              "    \n",
              "            var tile_layer_8265c1470072dc11b92d5142ca37e13d = L.tileLayer(\n",
              "                &quot;https://earthengine.googleapis.com/v1/projects/earthengine-legacy/maps/12b73c403ff43107948a94200cbc594e-09ab74e18021f30a03c934ab241cb03c/tiles/{z}/{x}/{y}&quot;,\n",
              "                {&quot;attribution&quot;: &quot;Map Data \\u0026copy; \\u003ca href=\\&quot;https://earthengine.google.com/\\&quot;\\u003eGoogle Earth Engine\\u003c/a\\u003e&quot;, &quot;detectRetina&quot;: false, &quot;maxNativeZoom&quot;: 18, &quot;maxZoom&quot;: 18, &quot;minZoom&quot;: 0, &quot;noWrap&quot;: false, &quot;opacity&quot;: 1, &quot;subdomains&quot;: &quot;abc&quot;, &quot;tms&quot;: false}\n",
              "            );\n",
              "        \n",
              "    \n",
              "            tile_layer_8265c1470072dc11b92d5142ca37e13d.addTo(map_58ccad1874e50230161e8a88c58d5cb8);\n",
              "        \n",
              "    \n",
              "            var layer_control_b8ce20a97d69e81d8a4b9420a1e225a1_layers = {\n",
              "                base_layers : {\n",
              "                    &quot;openstreetmap&quot; : tile_layer_681c518d15df4f420b702eb772c072e4,\n",
              "                },\n",
              "                overlays :  {\n",
              "                    &quot;training polygons&quot; : tile_layer_8265c1470072dc11b92d5142ca37e13d,\n",
              "                },\n",
              "            };\n",
              "            let layer_control_b8ce20a97d69e81d8a4b9420a1e225a1 = L.control.layers(\n",
              "                layer_control_b8ce20a97d69e81d8a4b9420a1e225a1_layers.base_layers,\n",
              "                layer_control_b8ce20a97d69e81d8a4b9420a1e225a1_layers.overlays,\n",
              "                {&quot;autoZIndex&quot;: true, &quot;collapsed&quot;: true, &quot;position&quot;: &quot;topright&quot;}\n",
              "            ).addTo(map_58ccad1874e50230161e8a88c58d5cb8);\n",
              "\n",
              "        \n",
              "&lt;/script&gt;\n",
              "&lt;/html&gt;\" style=\"position:absolute;width:100%;height:100%;left:0;top:0;border:none !important;\" allowfullscreen webkitallowfullscreen mozallowfullscreen></iframe></div></div>"
            ],
            "text/plain": [
              "<folium.folium.Map at 0x7f1070433110>"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# trainingPolys = ee.FeatureCollection('projects/google/DemoTrainingGeometries')\n",
        "# evalPolys = ee.FeatureCollection('projects/google/DemoEvalGeometries')\n",
        "# # trainingPolys = ee.FeatureCollection('projects/'+GCS_PROJECT+'/assets/trainingPolys')\n",
        "# # evalPolys = ee.FeatureCollection('projects/'+GCS_PROJECT+'/assets/evalPolys')\n",
        "# trainingPolys = ee.FeatureCollection('projects/'+GCS_PROJECT+'/assets/trainingPolysAfrica')\n",
        "# evalPolys = ee.FeatureCollection('projects/'+GCS_PROJECT+'/assets/evalPolysAfrica')\n",
        "trainingPolys = ee.FeatureCollection('projects/'+GCS_PROJECT+'/assets/trainingPolysAfricaBig')\n",
        "evalPolys = ee.FeatureCollection('projects/'+GCS_PROJECT+'/assets/evalPolysAfricaSmall')\n",
        "\n",
        "polyImage = ee.Image(0).byte().paint(trainingPolys, 1).paint(evalPolys, 2)\n",
        "polyImage = polyImage.updateMask(polyImage)\n",
        "\n",
        "mapid = polyImage.getMapId({'min': 1, 'max': 2, 'palette': ['gold', 'blue']})\n",
        "map = folium.Map(location=[0, 12], zoom_start=5)\n",
        "folium.TileLayer(\n",
        "    tiles=mapid['tile_fetcher'].url_format,\n",
        "    attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "    overlay=True,\n",
        "    name='training polygons',\n",
        "  ).add_to(map)\n",
        "map.add_child(folium.LayerControl())\n",
        "map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZV890gPHeZqz"
      },
      "source": [
        "# Sampling\n",
        "\n",
        "The mapped data look reasonable so take a sample from each polygon and merge the results into a single export.  The key step is sampling the array image at points, to get all the pixels in a 256x256 neighborhood at each point.  It's worth noting that to build the training and testing data for the FCNN, you export a single TFRecord file that contains patches of pixel values in each record.  You do NOT need to export each training/testing patch to a different image.  Since each record potentially contains a lot of data (especially with big patches or many input bands), some manual sharding of the computation is necessary to avoid the `computed value too large` error.  Specifically, the following code takes multiple (smaller) samples within each geometry, merging the results to get a single export."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "cellView": "both",
        "id": "FyRpvwENxE-A"
      },
      "outputs": [],
      "source": [
        "# Convert the feature collections to lists for iteration.\n",
        "trainingPolysList = trainingPolys.toList(trainingPolys.size())\n",
        "evalPolysList = evalPolys.toList(evalPolys.size())\n",
        "\n",
        "# These numbers determined experimentally.\n",
        "n = 200 # Number of shards in each polygon. # originally 200\n",
        "N = 2000 # Total sample size in each polygon. # orignally 2000\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Export Training & Evaluation Data from Earth Engine to Google Cloud Storage\n",
        "This can be modified to export to local storage, just adjust the file paths."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Export all the training data (in many pieces), with one task\n",
        "# per geometry.\n",
        "for g in range(trainingPolys.size().getInfo()):\n",
        "  geomSample = ee.FeatureCollection([])\n",
        "  for i in range(n):\n",
        "    sample = arrays.sample(\n",
        "      region = ee.Feature(trainingPolysList.get(g)).geometry(),\n",
        "      scale = 30,\n",
        "      numPixels = N / n, # Size of the shard.\n",
        "      seed = i,\n",
        "      tileScale = 8\n",
        "    )\n",
        "    geomSample = geomSample.merge(sample)\n",
        "  \n",
        "  desc = TRAINING_BASE + '_g' + str(g)\n",
        "  print(g, FOLDER, BUCKET, desc)\n",
        "  \n",
        "  task = ee.batch.Export.table.toCloudStorage(\n",
        "    collection = geomSample,\n",
        "    description = desc,\n",
        "    bucket = BUCKET,\n",
        "    fileNamePrefix = FOLDER + desc,\n",
        "    fileFormat = 'TFRecord',\n",
        "    selectors = BANDS + [RESPONSE]\n",
        "  )\n",
        "  task.start()\n",
        "print('Training imagery tasks submitted. Check on the Tasks in GEE for their status (https://code.earthengine.google.com/tasks).')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Export all the evaluation data.\n",
        "for g in range(evalPolys.size().getInfo()):\n",
        "  geomSample = ee.FeatureCollection([])\n",
        "  for i in range(n):\n",
        "    sample = arrays.sample(\n",
        "      region = ee.Feature(evalPolysList.get(g)).geometry(),\n",
        "      scale = 30,\n",
        "      numPixels = N / n,\n",
        "      seed = i,\n",
        "      tileScale = 8\n",
        "    )\n",
        "    geomSample = geomSample.merge(sample)\n",
        "\n",
        "  desc = EVAL_BASE + '_g' + str(g)\n",
        "  print(g, FOLDER, BUCKET, desc)\n",
        "  \n",
        "  task = ee.batch.Export.table.toCloudStorage(\n",
        "    collection = geomSample,\n",
        "    description = desc,\n",
        "    bucket = BUCKET,\n",
        "    fileNamePrefix = FOLDER + desc,\n",
        "    fileFormat = 'TFRecord',\n",
        "    selectors = BANDS + [RESPONSE]\n",
        "  )\n",
        "  task.start()\n",
        "print('Eval imagery tasks submitted. Check on the Tasks in GEE for their status (https://code.earthengine.google.com/tasks).')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Copy Files from Google Cloud Storage to Local Storage\n",
        "This is necessary to run the model locally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Copy from your bucket to local path (note -r is for recursive call)\n",
        "!{GCLOUD_PATH}gsutil -m cp -r 'gs://'{BUCKET}'/'{FOLDER}{TRAINING_BASE}'*' {LOCAL_PATH}{DATA_PATH}{FOLDER}\n",
        "!{GCLOUD_PATH}gsutil -m cp -r 'gs://'{BUCKET}'/'{FOLDER}{EVAL_BASE}'*' {LOCAL_PATH}{DATA_PATH}{FOLDER}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Configure GPU Usage\n",
        "\n",
        "Depending on how many GPUs are needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# see what devices are available\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
        "print(\"GPUs Available: \", tf.config.list_physical_devices('GPU'))\n",
        "!nvidia-smi\n",
        "# tf.debugging.set_log_device_placement(False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# configure the strategy to use all GPUs\n",
        "# gpus = tf.config.list_logical_devices('GPU')\n",
        "# gpu_strategy = tf.distribute.MirroredStrategy(gpus)\n",
        "\n",
        "# gpu_strategy = tf.device('/GPU:1')\n",
        "\n",
        "# import os\n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWXrvBE4607G"
      },
      "source": [
        "# Training and Evaluation Data\n",
        "\n",
        "Load the data exported from Earth Engine into a `tf.data.Dataset`.  The following are helper functions for that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WWZ0UXCVMyJP"
      },
      "outputs": [],
      "source": [
        "# from google.cloud import storage\n",
        "\n",
        "def parse_tfrecord(example_proto):\n",
        "  \"\"\"The parsing function.\n",
        "  Read a serialized example into the structure defined by FEATURES_DICT.\n",
        "  Args:\n",
        "    example_proto: a serialized Example.\n",
        "  Returns:\n",
        "    A dictionary of tensors, keyed by feature name.\n",
        "  \"\"\"\n",
        "  return tf.io.parse_single_example(example_proto, FEATURES_DICT)\n",
        "\n",
        "\n",
        "def to_tuple(inputs):\n",
        "  \"\"\"Function to convert a dictionary of tensors to a tuple of (inputs, outputs).\n",
        "  Turn the tensors returned by parse_tfrecord into a stack in HWC shape.\n",
        "  Args:\n",
        "    inputs: A dictionary of tensors, keyed by feature name.\n",
        "  Returns:\n",
        "    A tuple of (inputs, outputs).\n",
        "  \"\"\"\n",
        "  inputsList = [inputs.get(key) for key in FEATURES]\n",
        "  stacked = tf.stack(inputsList, axis=0)\n",
        "  # Convert from CHW to HWC\n",
        "  stacked = tf.transpose(stacked, [1, 2, 0])\n",
        "  return stacked[:,:,:len(BANDS)], stacked[:,:,len(BANDS):]\n",
        "\n",
        "\n",
        "def get_dataset(path, prefix, n_files):\n",
        "  \"\"\"Function to read, parse and format to tuple a set of input tfrecord files.\n",
        "  Get all the files matching the pattern, parse and convert to tuple.\n",
        "  Args:\n",
        "    pattern: A file pattern to match in a Cloud Storage bucket.\n",
        "  Returns:\n",
        "    A tf.data.Dataset\n",
        "  \"\"\"\n",
        "  # original code\n",
        "  # glob = tf.io.gfile.glob(pattern)\n",
        "  \n",
        "  # from the cloud\n",
        "  # https://cloud.google.com/appengine/docs/legacy/standard/python/googlecloudstorageclient/read-write-to-cloud-storage\n",
        "  # storage_client = storage.Client()\n",
        "  # bucket = storage_client.get_bucket(BUCKET)\n",
        "  # blobs = storage_client.list_blobs(BUCKET, prefix=pattern)\n",
        "  # glob = []\n",
        "  # for blob in blobs:\n",
        "  #   globs.append(blob.name)\n",
        "  # print(globs)\n",
        "  # # append bucket name to globs\n",
        "  # glob = ['gs://' + BUCKET + '/' + f for f in globs]\n",
        "\n",
        "  # read local tfrecord.gz files from unet-mini\n",
        "  glob = []\n",
        "  for i in range(0, n_files):\n",
        "    glob = path + prefix + '_g' + str(i) + '.tfrecord.gz'\n",
        "\n",
        "  # glob = [\n",
        "  #   '/home/isly9493/csci5922/proj/data/unet-mini/eval_patches_g0.tfrecord.gz',\n",
        "  #   '/home/isly9493/csci5922/proj/data/unet-mini/eval_patches_g1.tfrecord.gz'\n",
        "  # ]\n",
        "  dataset = tf.data.TFRecordDataset(glob, compression_type='GZIP')\n",
        "  dataset = dataset.map(parse_tfrecord, num_parallel_calls=5)\n",
        "  dataset = dataset.map(to_tuple, num_parallel_calls=5)\n",
        "  return dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xg1fa18336D2"
      },
      "source": [
        "Use the helpers to read in the training dataset.  Print the first record to check."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rm0qRF0fAYcC"
      },
      "outputs": [],
      "source": [
        "# with tf.device('/GPU:1'):\n",
        "# with gpu_strategy.scope():\n",
        "training = get_dataset(LOCAL_PATH + DATA_PATH + FOLDER, TRAINING_BASE, 8)\n",
        "training = training.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
        "# print(iter(training.take(1)).next())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-cQO5RL6vob"
      },
      "source": [
        "# Evaluation data\n",
        "\n",
        "Now do the same thing to get an evaluation dataset.  Note that unlike the training dataset, the evaluation dataset has a batch size of 1, is not repeated and is not shuffled."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ieKTCGiJ6xzo"
      },
      "outputs": [],
      "source": [
        "# with tf.device('/GPU:1')\n",
        "# with gpu_strategy.scope():\n",
        "evaluation = get_dataset(LOCAL_PATH + DATA_PATH + FOLDER, EVAL_BASE, 2)\n",
        "evaluation = evaluation.batch(1).repeat()\n",
        "# print(iter(evaluation.take(1)).next()[0][0,:,:,1:3])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# visualize of the of the training / evaluation image patch pairs\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# # Display a batch of data\n",
        "def display_batch_of_images(databatch):\n",
        "    \"\"\"Display a batch of images.\n",
        "    Args:\n",
        "        databatch: A batch of data\n",
        "    \"\"\"\n",
        "    imgs, labels = databatch\n",
        "    \n",
        "    # show landsat in RGD with blue as band 2, green as band 3, and red as band 4\n",
        "    plt.figure(figsize=(10,10))\n",
        "    label = labels.numpy().astype(np.float32)\n",
        "    for i in range(25):\n",
        "        plt.subplot(5,5,i+1)\n",
        "        plt.imshow(imgs[i,:,:,1:4]*6)\n",
        "        # plt.imshow(tf.concat([\n",
        "        #     imgs[i,:,:,4],\n",
        "        #     imgs[i,:,:,3],\n",
        "        #     imgs[i,:,:,2]\n",
        "        # ]))\n",
        "        # plt.title('LandSat 8')\n",
        "        plt.axis('off')\n",
        "    plt.show()\n",
        "    # show viirs in grayscale\n",
        "    plt.figure(figsize=(10,10))\n",
        "    for i in range(25):\n",
        "        plt.subplot(5,5,i+1)\n",
        "        plt.imshow(labels[i,:,:,0], cmap='gray')\n",
        "        # plt.title('VIIRS DNB')\n",
        "        plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Display a batch of data\n",
        "display_batch_of_images(iter(training.take(1)).next())\n",
        "# display_batch_of_images(iter(evaluation.take(1)).next())\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JIE7Yl87lgU"
      },
      "source": [
        "# Model\n",
        "\n",
        "Here we use the Keras implementation of the U-Net model.  The U-Net model takes 256x256 pixel patches as input and outputs per-pixel class probability, label or a continuous output.  We can implement the model essentially unmodified, but will use mean squared error loss on the sigmoidal output since we are treating this as a regression problem, rather than a classification problem.  Since impervious surface fraction is constrained to [0,1], with many values close to zero or one, a saturating activation function is suitable here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wsnnnz56yS3l"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "from keras import losses\n",
        "from keras import models\n",
        "from keras import metrics\n",
        "from keras import optimizers\n",
        "\n",
        "def conv_block(input_tensor, num_filters):\n",
        "\tencoder = layers.Conv2D(num_filters, (3, 3), padding='same')(input_tensor)\n",
        "\tencoder = layers.BatchNormalization()(encoder)\n",
        "\tencoder = layers.Activation('relu')(encoder)\n",
        "\tencoder = layers.Conv2D(num_filters, (3, 3), padding='same')(encoder)\n",
        "\tencoder = layers.BatchNormalization()(encoder)\n",
        "\tencoder = layers.Activation('relu')(encoder)\n",
        "\treturn encoder\n",
        "\n",
        "def encoder_block(input_tensor, num_filters):\n",
        "\tencoder = conv_block(input_tensor, num_filters)\n",
        "\tencoder_pool = layers.MaxPooling2D((2, 2), strides=(2, 2))(encoder)\n",
        "\treturn encoder_pool, encoder\n",
        "\n",
        "\n",
        "def decoder_block(input_tensor, concat_tensor, num_filters):\n",
        "\tdecoder = layers.Conv2DTranspose(num_filters, (2, 2), strides=(2, 2), padding='same')(input_tensor)\n",
        "\tdecoder = layers.concatenate([concat_tensor, decoder], axis=-1)\n",
        "\tdecoder = layers.BatchNormalization()(decoder)\n",
        "\tdecoder = layers.Activation('relu')(decoder)\n",
        "\tdecoder = layers.Conv2D(num_filters, (3, 3), padding='same')(decoder)\n",
        "\tdecoder = layers.BatchNormalization()(decoder)\n",
        "\tdecoder = layers.Activation('relu')(decoder)\n",
        "\tdecoder = layers.Conv2D(num_filters, (3, 3), padding='same')(decoder)\n",
        "\tdecoder = layers.BatchNormalization()(decoder)\n",
        "\tdecoder = layers.Activation('relu')(decoder)\n",
        "\treturn decoder\n",
        "\n",
        "def get_model():\n",
        "\tinputs = layers.Input(shape=[None, None, len(BANDS)]) # 256\n",
        "\tencoder0_pool, encoder0 = encoder_block(inputs, 32) # 128\n",
        "\tencoder1_pool, encoder1 = encoder_block(encoder0_pool, 64) # 64\n",
        "\tencoder2_pool, encoder2 = encoder_block(encoder1_pool, 128) # 32\n",
        "\tencoder3_pool, encoder3 = encoder_block(encoder2_pool, 256) # 16\n",
        "\tencoder4_pool, encoder4 = encoder_block(encoder3_pool, 512) # 8\n",
        "\tcenter = conv_block(encoder4_pool, 1024) # center\n",
        "\tdecoder4 = decoder_block(center, encoder4, 512) # 16\n",
        "\tdecoder3 = decoder_block(decoder4, encoder3, 256) # 32\n",
        "\tdecoder2 = decoder_block(decoder3, encoder2, 128) # 64\n",
        "\tdecoder1 = decoder_block(decoder2, encoder1, 64) # 128\n",
        "\tdecoder0 = decoder_block(decoder1, encoder0, 32) # 256\n",
        "\toutputs = layers.Conv2D(1, (1, 1), activation='sigmoid')(decoder0)\n",
        "\n",
        "\tmodel = models.Model(inputs=[inputs], outputs=[outputs])\n",
        "\n",
        "\tmodel.compile(\n",
        "\t\toptimizer=optimizers.get(OPTIMIZER),\n",
        "\t\tloss=losses.get(LOSS),\n",
        "\t\tmetrics=[metrics.get(metric) for metric in METRICS])\n",
        "\n",
        "\treturn model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uu_E7OTDBCoS"
      },
      "source": [
        "# Training the model\n",
        "\n",
        "You train a Keras model by calling `.fit()` on it.  Here we're going to train for 10 epochs, which is suitable for demonstration purposes.  For production use, you probably want to optimize this parameter, for example through [hyperparamter tuning](https://cloud.google.com/ml-engine/docs/tensorflow/using-hyperparameter-tuning)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NzzaWxOhSxBy"
      },
      "outputs": [],
      "source": [
        "# with tf.device('/GPU:1')\n",
        "# with gpu_strategy.scope():\n",
        "m = get_model()\n",
        "history = m.fit(\n",
        "    x=training,\n",
        "    epochs=20,\n",
        "    steps_per_epoch=int(TRAIN_SIZE / BATCH_SIZE),\n",
        "    validation_data=evaluation,\n",
        "    validation_steps=EVAL_SIZE\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "MODEL_NAME = '/local/isly9493/csci5922/proj/data/unet-viirs/unet_viirs_e50.keras'\n",
        "HISTORY_NAME = ''\n",
        "ASSET_SUFFIX = 'unet_viirs_e50' \n",
        "EPOCHS = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# save the model to a local file \n",
        "# m.save(LOCAL_PATH + DATA_PATH + FOLDER + MODEL_NAME)\n",
        "# m.save(MODEL_NAME)\n",
        "\n",
        "# # save the history variable to a local file with pickle\n",
        "# import pickle\n",
        "# # with open(LOCAL_PATH + DATA_PATH + FOLDER + HISTORY_NAME, 'wb') as f:\n",
        "# with open(HISTORY_NAME, 'wb') as f:\n",
        "#   pickle.dump(history, f)\n",
        "# issue: doesn't save the history field of the history object"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2XrwZHp66j4"
      },
      "source": [
        "Note that the notebook VM is sometimes not heavy-duty enough to get through a whole training job, especially if you have a large buffer size or a large number of epochs.  You can still use this notebook for training, but may need to set up an alternative VM ([learn more](https://research.google.com/colaboratory/local-runtimes.html)) for production use.  Alternatively, you can package your code for running large training jobs on Google's AI Platform [as described here](https://cloud.google.com/ml-engine/docs/tensorflow/trainer-considerations).  The following code loads a pre-trained model, which you can use for predictions right away."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-RJpNfEUS1qp"
      },
      "outputs": [],
      "source": [
        "# MODEL_DIR = 'gs://ee-docs-demos/fcnn-demo/trainer/model'\n",
        "# m = tf.keras.models.load_model('/local/isly9493/csci5922/proj/data/viirs-africa/unet_viirs_e50.keras')\n",
        "m = tf.keras.models.load_model(MODEL_NAME)\n",
        "# m.summary()\n",
        "\n",
        "# # load the history variable from a local file with pickle\n",
        "# import pickle\n",
        "# with open(HISTORY_NAME, 'rb') as f:\n",
        "#   history = pickle.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluation\n",
        "Let's look at how the model performed in terms of loss while it trained."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# plot the training and validation loss\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history['root_mean_squared_error'])\n",
        "plt.plot(history.history['val_root_mean_squared_error'])\n",
        "plt.title('Model RMSE for ' + ASSET_SUFFIX + ' for ' + str(EPOCHS) + ' epochs')\n",
        "plt.ylabel('RMSE')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Training RMSE', 'Validation RMSE'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## evaluate the model on the evaluation dataset to get the loss and RMSE\n",
        "loss, rmse = m.evaluate(evaluation, steps=EVAL_SIZE)\n",
        "print('Model loss: {:0.4f}. RMSE: {:0.4f}'.format(loss, rmse))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "loss, rmse = m.evaluate(training, steps=10)\n",
        "print('Model loss: {:0.4f}. RMSE: {:0.4f}'.format(loss, rmse))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1ySNup0xCqN"
      },
      "source": [
        "# Prediction\n",
        "\n",
        "The prediction pipeline is:\n",
        "\n",
        "1.  Export imagery on which to do predictions from Earth Engine in TFRecord format to a Cloud Storage bucket.\n",
        "2.  Use the trained model to make the predictions.\n",
        "3.  Write the predictions to a TFRecord file in a Cloud Storage.\n",
        "4.  Upload the predictions TFRecord file to Earth Engine.\n",
        "\n",
        "The following functions handle this process.  It's useful to separate the export from the predictions so that you can experiment with different models without running the export every time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Specify Region and Outputs for Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FPANwc7B1-TS"
      },
      "outputs": [],
      "source": [
        "# Output assets folder: YOUR FOLDER\n",
        "ee_user_folder = 'projects/'+GCS_PROJECT+'/assets' # INSERT YOUR FOLDER HERE.\n",
        "\n",
        "# # Base file name to use for TFRecord files and assets.\n",
        "# pred_image_base = 'FCNN_demo_beijing_384_'\n",
        "# # Half this will extend on the sides of each patch.\n",
        "# pred_kernel_buffer = [128, 128]\n",
        "# # Beijing\n",
        "# pred_region = ee.Geometry.Polygon(\n",
        "#         [[[115.9662455210937, 40.121362012835235],\n",
        "#           [115.9662455210937, 39.64293313749715],\n",
        "#           [117.01818643906245, 39.64293313749715],\n",
        "#           [117.01818643906245, 40.121362012835235]]], None, False)\n",
        "\n",
        "# kinshasa center: 15.267681, -4.286977\n",
        "pred_image_base = 'FCNN_demo_kinshasa_384_'\n",
        "# Half this will extend on the sides of each patch.\n",
        "pred_kernel_buffer = [128, 128]\n",
        "# Kinshasa\n",
        "pred_region = ee.Geometry.Polygon(\n",
        "  [[[15.02, -4.04],\n",
        "    [15.02, -4.54],\n",
        "    [15.52, -4.54],\n",
        "    [15.52, -4.04]]], \n",
        "  None, \n",
        "  False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Export Sample Imagery for Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M3WDAa-RUpXP"
      },
      "outputs": [],
      "source": [
        "def doExport(out_image_base, kernel_buffer, region):\n",
        "  \"\"\"Run the image export task.  Block until complete.\n",
        "  \"\"\"\n",
        "  task = ee.batch.Export.image.toCloudStorage(\n",
        "    image = image.select(BANDS),\n",
        "    description = out_image_base,\n",
        "    bucket = BUCKET,\n",
        "    fileNamePrefix = FOLDER + out_image_base,\n",
        "    region = region.getInfo()['coordinates'],\n",
        "    scale = 30,\n",
        "    fileFormat = 'TFRecord',\n",
        "    maxPixels = 1e10,\n",
        "    formatOptions = {\n",
        "      'patchDimensions': KERNEL_SHAPE,\n",
        "      'kernelSize': kernel_buffer,\n",
        "      'compressed': True,\n",
        "      'maxFileSize': 104857600\n",
        "    }\n",
        "  )\n",
        "  task.start()\n",
        "  print('Prediction image export task submitted. Check on the task in GEE for its status (https://code.earthengine.google.com/tasks).')\n",
        "\n",
        "  # Block until the task completes.\n",
        "  # import time\n",
        "  # while task.active():\n",
        "  #   time.sleep(30)\n",
        "\n",
        "  # Error condition\n",
        "  # if task.status()['state'] != 'COMPLETED':\n",
        "  #   print('Error with image export.')\n",
        "  # else:\n",
        "  #   print('Image export completed.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lLNEOLkXWvSi"
      },
      "outputs": [],
      "source": [
        "# Run the export.\n",
        "doExport(pred_image_base, pred_kernel_buffer, pred_region)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# copy the file from cloud storage to local\n",
        "# !{GCLOUD_PATH}gsutil -m cp \\\n",
        "#   'gs://'{BUCKET}'/'{FOLDER}{pred_image_base}'00000.tfrecord.gz' \\\n",
        "#   'gs://'{BUCKET}'/'{FOLDER}{pred_image_base}'00001.tfrecord.gz' \\\n",
        "#   'gs://'{BUCKET}'/'{FOLDER}{pred_image_base}'00002.tfrecord.gz' \\\n",
        "#   'gs://'{BUCKET}'/'{FOLDER}{pred_image_base}'00003.tfrecord.gz' \\\n",
        "#   'gs://'{BUCKET}'/'{FOLDER}{pred_image_base}'00004.tfrecord.gz' \\\n",
        "#   'gs://'{BUCKET}'/'{FOLDER}{pred_image_base}'mixer.json' \\\n",
        "#   {LOCAL_PATH}{DATA_PATH}{FOLDER}\n",
        "\n",
        "# !{GCLOUD_PATH}gsutil -m cp -r 'gs://csci5922-proj/viirs-africa/FCNN_demo_kinshasa*' 'gs://csci5922-proj/viirs-africa-small/'\n",
        "\n",
        "!{GCLOUD_PATH}gsutil -m cp -r 'gs://'{BUCKET}'/'{FOLDER}{pred_image_base}'*' {LOCAL_PATH}{DATA_PATH}{FOLDER}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define Code for Running Model on Prediction Data\n",
        "Modified to run on local data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zb_9_FflygVw"
      },
      "outputs": [],
      "source": [
        "def doPrediction(out_image_base, kernel_buffer):\n",
        "  \"\"\"Perform inference on exported imagery, upload to Earth Engine.\n",
        "  \"\"\"\n",
        "\n",
        "  print('Looking for TFRecord files...')\n",
        "\n",
        "  # # Get a list of all the files in the output bucket.\n",
        "  # filesList = !gsutil ls 'gs://'{BUCKET}'/'{FOLDER}\n",
        "  # Get a list of all local files in /data/unet-mini\n",
        "  filesList = !ls {LOCAL_PATH}{DATA_PATH}{FOLDER}\n",
        "\n",
        "  # Get only the files generated by the image export.\n",
        "  exportFilesList = [s for s in filesList if out_image_base in s]\n",
        "\n",
        "  # Get the list of image files and the JSON mixer file.\n",
        "  imageFilesList = []\n",
        "  jsonFile = None\n",
        "  for f in exportFilesList:\n",
        "    if f.endswith('.tfrecord.gz'):\n",
        "      imageFilesList.append(f)\n",
        "    elif f.endswith('.json'):\n",
        "      jsonFile = f\n",
        "\n",
        "  # Make sure the files are in the right order.\n",
        "  imageFilesList.sort()\n",
        "\n",
        "  from pprint import pprint\n",
        "  pprint(imageFilesList)\n",
        "  print('here I am')\n",
        "  print(jsonFile)\n",
        "\n",
        "  import json\n",
        "  # Load the contents of the mixer file to a JSON object.\n",
        "  # jsonText = !{GCLOUD_PATH}gsutil cat {jsonFile}  \n",
        "  # jsonText = !cat '/home/isly9493/csci5922/proj/data/viirs-usa/FCNN_demo_beijing_384_mixer.json'\n",
        "  jsonText = !cat {LOCAL_PATH}{DATA_PATH}{FOLDER}{jsonFile}\n",
        "  print(jsonText)\n",
        "  # Get a single string w/ newlines from the IPython.utils.text.SList\n",
        "  mixer = json.loads(jsonText.nlstr)\n",
        "  pprint(mixer)\n",
        "  patches = mixer['totalPatches']\n",
        "\n",
        "\n",
        "\n",
        "  buffered_shape = [\n",
        "      KERNEL_SHAPE[0] + kernel_buffer[0],\n",
        "      KERNEL_SHAPE[1] + kernel_buffer[1]]\n",
        "\n",
        "  imageColumns = [\n",
        "    tf.io.FixedLenFeature(shape=buffered_shape, dtype=tf.float32)\n",
        "      for k in BANDS\n",
        "  ]\n",
        "\n",
        "  imageFeaturesDict = dict(zip(BANDS, imageColumns))\n",
        "\n",
        "  def parse_image(example_proto):\n",
        "    return tf.io.parse_single_example(example_proto, imageFeaturesDict)\n",
        "\n",
        "  def toTupleImage(inputs):\n",
        "    inputsList = [inputs.get(key) for key in BANDS]\n",
        "    stacked = tf.stack(inputsList, axis=0)\n",
        "    stacked = tf.transpose(stacked, [1, 2, 0])\n",
        "    return stacked\n",
        "\n",
        "  # append absolute path to imageFilesList\n",
        "  imageFilesList = [LOCAL_PATH + DATA_PATH + FOLDER + f for f in imageFilesList]\n",
        "  print(imageFilesList)\n",
        "  \n",
        "  # Create a dataset from the TFRecord file(s) in Cloud Storage.\n",
        "  imageDataset = tf.data.TFRecordDataset(imageFilesList, compression_type='GZIP')\n",
        "  imageDataset = imageDataset.map(parse_image, num_parallel_calls=5)\n",
        "  imageDataset = imageDataset.map(toTupleImage).batch(1)\n",
        "\n",
        "  # print(imageDataset)\n",
        "\n",
        "  # Perform inference.\n",
        "  print('Running predictions...')\n",
        "  predictions = m.predict(imageDataset, steps=patches, verbose=1)\n",
        "  # print(predictions[0])\n",
        "  return predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def writePredictions(predictions, out_image_base, kernel_buffer):\n",
        "  print('Writing predictions...')\n",
        "  out_image_file = 'gs://' + BUCKET + '/' + FOLDER + out_image_base + '.TFRecord'\n",
        "  # out_image_file = LOCAL_PATH + DATA_PATH + FOLDER + out_image_base + '.TFRecord'\n",
        "\n",
        "  # Get set up for prediction.\n",
        "  x_buffer = int(kernel_buffer[0] / 2)\n",
        "  y_buffer = int(kernel_buffer[1] / 2)\n",
        "  \n",
        "  \n",
        "  writer = tf.io.TFRecordWriter(out_image_file)\n",
        "  patches = 0\n",
        "  for predictionPatch in predictions:\n",
        "    # print('Writing patch ' + str(patches) + '...')\n",
        "    predictionPatch = predictionPatch[\n",
        "        x_buffer:x_buffer+KERNEL_SIZE, y_buffer:y_buffer+KERNEL_SIZE]\n",
        "\n",
        "    # Create an example.\n",
        "    example = tf.train.Example(\n",
        "      features=tf.train.Features(\n",
        "        feature={\n",
        "          'impervious': tf.train.Feature(\n",
        "            float_list=tf.train.FloatList(\n",
        "              value=predictionPatch.flatten()\n",
        "            )\n",
        "          )\n",
        "        }\n",
        "      )\n",
        "    )\n",
        "    # Write the example.\n",
        "    writer.write(example.SerializeToString())\n",
        "    patches += 1\n",
        "\n",
        "  print('Done writing predictions.')\n",
        "  writer.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def uploadPredictionsToEE(out_image_base, ee_user_folder):\n",
        "  # Start the upload.\n",
        "  print('Uploading to Google Earth Egnine...')\n",
        "  \n",
        "  # define file paths\n",
        "  out_image_file = 'gs://' + BUCKET + '/' + FOLDER + out_image_base + '.TFRecord'\n",
        "  out_image_asset = ee_user_folder + '/' + out_image_base + ASSET_SUFFIX\n",
        "  jsonFile = 'gs://' + BUCKET + '/' + FOLDER + out_image_base + 'mixer.json'\n",
        "  # jsonFile = '/home/isly9493/csci5922/proj/data/unet-mini/FCNN_demo_beijing_384_mixer.json'\n",
        "  \n",
        "  print('out_image_asset: ', out_image_asset)\n",
        "  print('out_image_file: ', out_image_file)\n",
        "  print('jsonFile: ', jsonFile)\n",
        "  \n",
        "  # upload file to google earth engine from gcs\n",
        "  # !earthengine upload image --asset_id={out_image_asset} {out_image_file} {jsonFile}\n",
        "  !earthengine upload image --asset_id={out_image_asset} {out_image_file} {jsonFile}\n",
        "\n",
        "  print('Prediction image upload task submitted. Check on the task in GEE for its status (https://code.earthengine.google.com/tasks).')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZqlymOehnQO"
      },
      "source": [
        "Now there's all the code needed to run the prediction pipeline, all that remains is to specify the output region in which to do the prediction, the names of the output files, where to put them, and the shape of the outputs.  In terms of the shape, the model is trained on 256x256 patches, but can work (in theory) on any patch that's big enough with even dimensions ([reference](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Long_Fully_Convolutional_Networks_2015_CVPR_paper.pdf)).  Because of tile boundary artifacts, give the model slightly larger patches for prediction, then clip out the middle 256x256 patch.  This is controlled with a kernel buffer, half the size of which will extend beyond the kernel buffer.  For example, specifying a 128x128 kernel will append 64 pixels on each side of the patch, to ensure that the pixels in the output are taken from inputs completely covered by the kernel."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Run Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KxACnxKFrQ_J"
      },
      "outputs": [],
      "source": [
        "# Run the prediction.\n",
        "predictions = doPrediction(pred_image_base, pred_kernel_buffer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Write the predictions.\n",
        "writePredictions(predictions, pred_image_base, pred_kernel_buffer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# upload to earth engine\n",
        "uploadPredictionsToEE(pred_image_base, ee_user_folder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uj_G9OZ1xH6K"
      },
      "source": [
        "## Display the Prediction\n",
        "\n",
        "One the data has been exported, the model has made predictions and the predictions have been written to a file, and the image imported to Earth Engine, it's possible to display the resultant Earth Engine asset.  Here, display the impervious area predictions over Beijing, China."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jgco6HJ4R5p2"
      },
      "outputs": [],
      "source": [
        "map = folium.Map(location=[-4.29, 15.27], zoom_start=11)\n",
        "\n",
        "\n",
        "# add landsat layer clipped to prediction region\n",
        "landsat = image\n",
        "landsat = landsat.clip(pred_region)\n",
        "mapid = landsat.getMapId({'bands': ['B4', 'B3', 'B2'], 'min': 0, 'max': 0.3})\n",
        "folium.TileLayer(\n",
        "    tiles=mapid['tile_fetcher'].url_format,\n",
        "    attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "    overlay=True,\n",
        "    name='landsat',\n",
        "  ).add_to(map)\n",
        "\n",
        "# actual viirs\n",
        "viirs = ee.ImageCollection('NOAA/VIIRS/DNB/MONTHLY_V1/VCMSLCFG').select('avg_rad').filterDate('2021-01-01', '2022-12-31').median().divide(30).float()\n",
        "viirs = viirs.clip(pred_region)\n",
        "# actual_image = viirs\n",
        "mapid = viirs.getMapId({'min': 0, 'max': 1}) # normally max 1.0\n",
        "folium.TileLayer(\n",
        "    tiles=mapid['tile_fetcher'].url_format,\n",
        "    attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "    overlay=True,\n",
        "    name='viirs dnb',\n",
        "  ).add_to(map)\n",
        "\n",
        "# prediction\n",
        "out_image = ee.Image(ee_user_folder + '/' + pred_image_base + ASSET_SUFFIX)\n",
        "mapid = out_image.getMapId({'min': 0, 'max': 1})\n",
        "folium.TileLayer(\n",
        "    tiles=mapid['tile_fetcher'].url_format,\n",
        "    attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "    overlay=True,\n",
        "    name='predicted nightlights',\n",
        "  ).add_to(map)\n",
        "\n",
        "\n",
        "# difference\n",
        "diff = out_image.subtract(viirs)\n",
        "mapid = diff.getMapId({'min': -0.5, 'max': 0.5, 'palette': ['FF0000', '000000', '00FF00']})\n",
        "folium.TileLayer(\n",
        "    tiles=mapid['tile_fetcher'].url_format,\n",
        "    attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "    overlay=True,\n",
        "    name='difference',\n",
        "  ).add_to(map)\n",
        "\n",
        "# visualize the map\n",
        "map.add_child(folium.LayerControl())\n",
        "map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Export to HTML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# export to HTML for webpage\n",
        "import os\n",
        "\n",
        "# os.system('jupyter nbconvert --to html mod1.ipynb')\n",
        "os.system('jupyter nbconvert --to html unet_regression_virrs.ipynb --HTMLExporter.theme=dark')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
