{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fSIfBsgi8dNK"
      },
      "outputs": [],
      "source": [
        "#@title Copyright 2023 Google LLC. { display-mode: \"form\" }\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aV1xZ1CPi3Nw"
      },
      "source": [
        "<table class=\"ee-notebook-buttons\" align=\"left\"><td>\n",
        "<a target=\"_blank\"  href=\"http://colab.research.google.com/github/google/earthengine-community/blob/master/guides/linked/Earth_Engine_training_patches_computePixels.ipynb\">\n",
        "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" /> Run in Google Colab</a>\n",
        "</td><td>\n",
        "<a target=\"_blank\"  href=\"https://github.com/google/earthengine-community/blob/master/guides/linked/Earth_Engine_training_patches_computePixels.ipynb\"><img width=32px src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" /> View source on GitHub</a></td></table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SV-E0p6PpGr"
      },
      "source": [
        "# Download training patches from Earth Engine\n",
        "\n",
        "This demonstration shows how to get patches of imagery from Earth Engine for training ML models.  Specifically, use `computePixels` calls in parallel to quickly and efficiently write a TFRecord file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvlyhBESPQKW"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "rppQiHjZPX_y"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google.colab'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgoogle\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcolab\u001b[39;00m \u001b[39mimport\u001b[39;00m auth\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgoogle\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapi_core\u001b[39;00m \u001b[39mimport\u001b[39;00m retry\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mIPython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdisplay\u001b[39;00m \u001b[39mimport\u001b[39;00m Image\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
          ]
        }
      ],
      "source": [
        "from google.colab import auth\n",
        "from google.api_core import retry\n",
        "from IPython.display import Image\n",
        "from matplotlib import pyplot as plt\n",
        "from numpy.lib import recfunctions as rfn\n",
        "\n",
        "import concurrent\n",
        "import ee\n",
        "import google\n",
        "import io\n",
        "import multiprocessing\n",
        "import numpy as np\n",
        "import requests\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbLzoz4klKwH"
      },
      "source": [
        "## Authentication and initialization\n",
        "\n",
        "Use the Colab auth widget to get credentials, then use them to initialize Earth Engine.  During initialization, be sure to specify a project and Earth Engine's [high-volume endpoint](https://developers.google.com/earth-engine/cloud/highvolume), in order to make automated requests."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HN5H25U_JBdp"
      },
      "outputs": [],
      "source": [
        "# REPLACE WITH YOUR PROJECT!\n",
        "PROJECT = 'your-project'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TLmI05-wT_GD"
      },
      "outputs": [],
      "source": [
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c5bEkwQHUDPS"
      },
      "outputs": [],
      "source": [
        "credentials, _ = google.auth.default()\n",
        "ee.Initialize(credentials, project=PROJECT, opt_url='https://earthengine-highvolume.googleapis.com')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7rHLQsPuwyb"
      },
      "source": [
        "## Define variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hj_ZujvvFlGR"
      },
      "outputs": [],
      "source": [
        "# REPLACE WITH YOUR BUCKET!\n",
        "OUTPUT_FILE = 'gs://your-bucket/your-file.tfrecord.gz'\n",
        "\n",
        "# Output resolution in meters.\n",
        "SCALE = 10\n",
        "\n",
        "# Pre-compute a geographic coordinate system.\n",
        "proj = ee.Projection('EPSG:4326').atScale(SCALE).getInfo()\n",
        "\n",
        "# Get scales in degrees out of the transform.\n",
        "SCALE_X = proj['transform'][0]\n",
        "SCALE_Y = -proj['transform'][4]\n",
        "\n",
        "# Patch size in pixels.\n",
        "PATCH_SIZE = 128\n",
        "\n",
        "# Offset to the upper left corner.\n",
        "OFFSET_X = -SCALE_X * PATCH_SIZE / 2\n",
        "OFFSET_Y = -SCALE_Y * PATCH_SIZE / 2\n",
        "\n",
        "# Request template.\n",
        "REQUEST = {\n",
        "      'fileFormat': 'NPY',\n",
        "      'grid': {\n",
        "          'dimensions': {\n",
        "              'width': PATCH_SIZE,\n",
        "              'height': PATCH_SIZE\n",
        "          },\n",
        "          'affineTransform': {\n",
        "              'scaleX': SCALE_X,\n",
        "              'shearX': 0,\n",
        "              'shearY': 0,\n",
        "              'scaleY': SCALE_Y,\n",
        "          },\n",
        "          'crsCode': proj['crs']\n",
        "      }\n",
        "  }\n",
        "\n",
        "# Blue, green, red, NIR, AOT.\n",
        "FEATURES = ['B2_median', 'B3_median', 'B4_median', 'B8_median', 'AOT_median']\n",
        "\n",
        "# Bay area.\n",
        "TEST_ROI = ee.Geometry.Rectangle(\n",
        "    [-123.05832753906247, 37.03109527141115,\n",
        "     -121.14121328124997, 38.24468432993584])\n",
        "# San Francisco.\n",
        "TEST_COORDS = [-122.43519674072265, 37.78010979412811]\n",
        "\n",
        "TEST_DATE = ee.Date('2021-06-01')\n",
        "\n",
        "# Number of samples per ROI, and per TFRecord file.\n",
        "N = 64\n",
        "\n",
        "# Specify the size and shape of patches expected by the model.\n",
        "KERNEL_SHAPE = [PATCH_SIZE, PATCH_SIZE]\n",
        "COLUMNS = [\n",
        "  tf.io.FixedLenFeature(shape=KERNEL_SHAPE, dtype=tf.float32) for k in FEATURES\n",
        "]\n",
        "FEATURES_DICT = dict(zip(FEATURES, COLUMNS))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbEM4nlUOmQn"
      },
      "source": [
        "## Image retrieval functions\n",
        "\n",
        "This section includes functions to compute a Sentinel-2 median composite and get a pacth of pixels from the composite, centered on the provided coordinates, as either a numpy array or a JPEG thumbnail (for visualization).  The functions that request patches are retriable and you can do that automatically by decorating the functions with [Retry](https://googleapis.dev/python/google-api-core/latest/retry.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VMBgRRUARTH1"
      },
      "outputs": [],
      "source": [
        "def get_s2_composite(roi, date):\n",
        "  \"\"\"Get a two-month Sentinel-2 median composite in the ROI.\"\"\"\n",
        "  start = date.advance(-1, 'month')\n",
        "  end = date.advance(1, 'month')\n",
        "\n",
        "  s2 = ee.ImageCollection('COPERNICUS/S2_HARMONIZED')\n",
        "  s2c = ee.ImageCollection('COPERNICUS/S2_CLOUD_PROBABILITY')\n",
        "  s2Sr = ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED')\n",
        "\n",
        "  s2c = s2c.filterBounds(roi).filterDate(start, end)\n",
        "  s2Sr = s2Sr.filterDate(start, end).filterBounds(roi)\n",
        "\n",
        "  def indexJoin(collectionA, collectionB, propertyName):\n",
        "    joined = ee.ImageCollection(ee.Join.saveFirst(propertyName).apply(\n",
        "        primary=collectionA,\n",
        "        secondary=collectionB,\n",
        "        condition=ee.Filter.equals(\n",
        "            leftField='system:index',\n",
        "            rightField='system:index'\n",
        "        ))\n",
        "    )\n",
        "    return joined.map(lambda image : image.addBands(ee.Image(image.get(propertyName))))\n",
        "\n",
        "  def maskImage(image):\n",
        "    s2c = image.select('probability')\n",
        "    return image.updateMask(s2c.lt(50))\n",
        "\n",
        "  withCloudProbability = indexJoin(s2Sr, s2c, 'cloud_probability')\n",
        "  masked = ee.ImageCollection(withCloudProbability.map(maskImage))\n",
        "  return masked.reduce(ee.Reducer.median(), 8)\n",
        "\n",
        "\n",
        "@retry.Retry()\n",
        "def get_patch(coords, image):\n",
        "  \"\"\"Get a patch centered on the coordinates, as a numpy array.\"\"\"\n",
        "  request = dict(REQUEST)\n",
        "  request['expression'] = image\n",
        "  request['grid']['affineTransform']['translateX'] = coords[0] + OFFSET_X\n",
        "  request['grid']['affineTransform']['translateY'] = coords[1] + OFFSET_Y\n",
        "  return np.load(io.BytesIO(ee.data.computePixels(request)))\n",
        "\n",
        "\n",
        "@retry.Retry()\n",
        "def get_display_image(coords, image):\n",
        "  \"\"\"Helper to display a patch using notebook widgets.\"\"\"\n",
        "  point = ee.Geometry.Point(coords)\n",
        "  region = point.buffer(64 * 10).bounds()\n",
        "  url = image.getThumbURL({\n",
        "      'region': region,\n",
        "      'dimensions': '128x128',\n",
        "      'format': 'jpg',\n",
        "      'min': 0, 'max': 5000,\n",
        "      'bands': ['B4_median', 'B3_median', 'B2_median']\n",
        "  })\n",
        "\n",
        "  r = requests.get(url, stream=True)\n",
        "  if r.status_code != 200:\n",
        "    raise google.api_core.exceptions.from_http_response(r)\n",
        "\n",
        "  return r.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o6FH8sIlHElY"
      },
      "outputs": [],
      "source": [
        "TEST_IMAGE = get_s2_composite(TEST_ROI, TEST_DATE)\n",
        "image = get_display_image(TEST_COORDS, TEST_IMAGE)\n",
        "Image(image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nQ60n8pMaRur"
      },
      "outputs": [],
      "source": [
        "np_array = get_patch(TEST_COORDS, TEST_IMAGE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QZFZud6Ia_n7"
      },
      "outputs": [],
      "source": [
        "# This is a structured array.\n",
        "print(np_array['B4_median'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eG9aK0dh-IaK"
      },
      "outputs": [],
      "source": [
        "display_array = rfn.structured_to_unstructured(np_array[['B4_median', 'B3_median', 'B2_median']])/5000\n",
        "plt.imshow(display_array)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7fC63m4Ow8e"
      },
      "source": [
        "## Sampling functions\n",
        "\n",
        "These are helper functions to get a random sample as a list of coordinates,  sample the composite (using `computePixels`) at each coordinate, serialize numpy arrays to `tf.Example` protos and write them into a file.  The sampling is handled in multiple threads using a `ThreadPoolExecutor`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NeKS5M-kRT4r"
      },
      "outputs": [],
      "source": [
        "def get_sample_coords(roi, n):\n",
        "  \"\"\"\"Get a random sample of N points in the ROI.\"\"\"\n",
        "  points = ee.FeatureCollection.randomPoints(region=roi, points=n, maxError=1)\n",
        "  return points.aggregate_array('.geo').getInfo()\n",
        "\n",
        "\n",
        "def array_to_example(structured_array):\n",
        "  \"\"\"\"Serialize a structured numpy array into a tf.Example proto.\"\"\"\n",
        "  feature = {}\n",
        "  for f in FEATURES:\n",
        "    feature[f] = tf.train.Feature(\n",
        "        float_list = tf.train.FloatList(\n",
        "            value = structured_array[f].flatten()))\n",
        "  return tf.train.Example(\n",
        "      features = tf.train.Features(feature = feature))\n",
        "\n",
        "\n",
        "def write_dataset(image, sample_points, file_name):\n",
        "  \"\"\"\"Write patches at the sample points into a TFRecord file.\"\"\"\n",
        "  future_to_point = {\n",
        "    EXECUTOR.submit(get_patch, point['coordinates'], image): point for point in sample_points\n",
        "  }\n",
        "\n",
        "  # Optionally compress files.\n",
        "  writer = tf.io.TFRecordWriter(file_name)\n",
        "\n",
        "  for future in concurrent.futures.as_completed(future_to_point):\n",
        "      point = future_to_point[future]\n",
        "      try:\n",
        "          np_array = future.result()\n",
        "          example_proto = array_to_example(np_array)\n",
        "          writer.write(example_proto.SerializeToString())\n",
        "          writer.flush()\n",
        "      except Exception as e:\n",
        "          print(e)\n",
        "          pass\n",
        "\n",
        "  writer.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hs_FozNIQFXI"
      },
      "outputs": [],
      "source": [
        "EXECUTOR = concurrent.futures.ThreadPoolExecutor(max_workers=N)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1dDAqyH5dZBK"
      },
      "outputs": [],
      "source": [
        "# These could come from anywhere.  Here is just a random sample.\n",
        "sample_points = get_sample_coords(TEST_ROI, N)\n",
        "\n",
        "# Sample patches from the image at each point.  Each sample is\n",
        "# fetched in parallel using the ThreadPoolExecutor.\n",
        "write_dataset(TEST_IMAGE, sample_points, OUTPUT_FILE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AyoEjI31O67O"
      },
      "source": [
        "## Check the written file\n",
        "\n",
        "Load and inspect the written file by visualizing a few patches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H3_SQsQCu9bh"
      },
      "outputs": [],
      "source": [
        "def parse_tfrecord(example_proto):\n",
        "  \"\"\"Parse a serialized example.\"\"\"\n",
        "  return tf.io.parse_single_example(example_proto, FEATURES_DICT)\n",
        "\n",
        "dataset = tf.data.TFRecordDataset(OUTPUT_FILE)\n",
        "dataset = dataset.map(parse_tfrecord, num_parallel_calls=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MBlWwC0_SycO"
      },
      "outputs": [],
      "source": [
        "take_20 = dataset.take(20)\n",
        "\n",
        "for data in take_20:\n",
        "  rgb = np.stack([\n",
        "      data['B4_median'].numpy(),\n",
        "      data['B3_median'].numpy(),\n",
        "      data['B2_median'].numpy()], 2) / 5000\n",
        "  plt.imshow(rgb)\n",
        "  plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwcryQrV5E8m"
      },
      "source": [
        "## Where to go next\n",
        "\n",
        " - Learn about how to scale training data generation pipelines with Apache Beam in [this demo](https://github.com/GoogleCloudPlatform/python-docs-samples/tree/main/people-and-planet-ai/land-cover-classification).\n",
        " - Learn about training models on Vertex AI in [this doc](/earth-engine/guides/tf_examples#semantic-segmentation-with-an-fcnn-trained-and-hosted-on-vertex-ai)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
