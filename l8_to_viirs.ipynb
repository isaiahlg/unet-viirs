{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Earth at Night from Day: Predicting Nighttime Radiance from Daytime Satellite Imagery\n",
        "Isaiah Lyons-Galante  \n",
        "University of Colorado Boulder\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SHAc5qbiR8l"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "The imprint of human activity on Earth has become unmistakable, offering a unique vantage point for population studies through the lens of space. Access to global high-resolution datasets, such as Landsat(1), has been made possible for the public through platforms like Google Earth Engine (3). These datasets open a window to observe and analyze human settlements, providing invaluable insights into the dynamics of our changing world. The image below is from Google Earth, showing the twin cities of Brazzaville and Kishasa on the banks of the Congo River. One can clearly see the densely package settlements on the banks, along with the unpopulated islands in the river.\n",
        "\n",
        "![brazzaville](./figs/brazzaville.png)\n",
        "\n",
        "During the night, human presence becomes even more apparent as settlements cast a faint glow into space. This nocturnal illumination is captured by the VIIRS instrument aboard the Suomi National Polar-orbiting Partnership (NPP) satellite. Despite its significance, the relatively weak signals limit the resolution to about 500 meters squared. This limitation poses a challenge to extracting fine-grained information about the distribution and characteristics of human settlements. The image below is of the same area as the one above, but at night. The resolution is significantly lower, with the pixels clearly visible, while the island in the river is completely dark.\n",
        "\n",
        "![brazzaville night](./figs/brazzavilleatnight.png)\n",
        "\n",
        "Compelling research underscores the correlation between night lights and mean household wealth, as measured by ground surveys. This correlation establishes night lights as a proxy for gauging livelihoods, making them a valuable tool for socio-economic analyses. This becomes particularly crucial in regions like Sub-Saharan Africa, where traditional population surveys are sparse. The scarcity of on-the-ground data highlights the potential of night lights as a surrogate measure for understanding the economic landscapes of communities. The graph below shows the relationship between night lights and mean household wealth in Sub-Saharan Africa. The data is from the Demographic and Health Surveys (DHS) Program, which conducts household surveys in developing countries. The graph shows a clear correlation between night lights and mean household wealth.\n",
        "\n",
        "![night lights and wealth](./figs/wealthcorrelations.jpg)\n",
        "\n",
        "The demand for higher-resolution night lights becomes evident when considering the need for detailed impact assessments. Such assessments can unravel the intricate ways in which individual communities are affected by economic development programs or global events. As technology advances, the quest for sharper and more detailed night light data gains significance, promising to enhance our understanding of human settlements and socio-economic patterns on a global scale. One example of this comes from a paper in Nature just last year that use wealth maps to assess the economic impact of access to electricity. High resolution nightlights would open up even finer grained analysis.\n",
        "\n",
        "![wealth map](./figs/WealthMap.png)\n",
        "\n",
        "Embarking on this exploration of human settlements and socio-economic patterns, I employ cutting-edge methodologies to surmount the resolution challenges posed by existing night light data. Leveraging the power of machine learning, my approach involves training a model to predict night lights from high-resolution daytime imagery. By undertaking this innovative process, I aim to transcend the limitations of the existing data and generate high-resolution nighttime imagery. This fusion of technological advancements with geospatial analysis not only promises to enrich our understanding of human settlements but also sets the stage for a more nuanced examination of the impact of economic development programs and global events on individual communities. This notebook will serve as a guide for the entire process, from method design to model training and evaluation, and finally, the results and conclusions. The code is available on [GitHub](https://github.com/isaiahlg/csci5922/blob/main/proj/unet_regression_virrs.ipynb).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Methods\n",
        "Because I are trying to predict one image from another, I can leverage a Fully Convolutional Network (6) with no dense layers. This allow us to use input imagery of any resolution. The U-Net (7) architecture, named for the shape of the diagram, is the most popular FCN and is well suited to our task. The output layer is modified to perform regression instead of classification so that I get a continuous radiance value instead of segment class. Below is a diagram of U-Net from the original paper -- notice the distintive U shape.\n",
        "\n",
        "![unet](./figs/unet.png)\n",
        "\n",
        "With the model in place, I then need to define our training data. In this project, four different training datasets were tested: both large training regions with 1000 image patch samples from each, as well as small regions with 100 image patch samples from each. Each of these were pulled from the United States and the African continent.  For each, there were 8 unique training regions, and 2 evaluation regions. For both areas, the polygons were centered around urban areas, as these have the greatest range of values in nighttime radiance. For consistency, the evaluation areas were also urban areas. Because the target is to predict nightlights in under-surveyed areas, the evaluation sites were Abuja, Nigeria, and Nairobi, Kenya. On the left, I have the large image patches from North America, and on the right from the African. Training patches are in yellow, and evaluation regions are in blue. The aim of having different continents as inputs was to test how transferrable the results are. Do well-lit areas in the US look like well-lit areas in Africa? \n",
        "\n",
        "American Cities             |  African Cities\n",
        ":-------------------------:|:-------------------------:\n",
        "![](./figs/usatrain.png)  |  ![](./figs/africatrain.png)\n",
        "\n",
        "\n",
        "For daytime imagery inputs, I pulled 9 bands from Landsat, 7 optical bands and 2 thermal bands. I created a cloud-masked image composite to create high-quality, cloud free input. For nightlight output, I also create a median composite of the average radiance band from stray-light corrected nightlights.8 The aim is that by pairing each daytime image patch of 256 x 256 with the corresponding nighttime image patch, I can predict nighttime image patches of 128x128 when the original resolution was about 16x16. This is almost 60X improvement in resolution.  Below on the left, I have a set of example input images from Landsat in false-color. The level of detail is significant compared with the coarse nightlight image patches on the right. \n",
        "\n",
        "Sample Training Inputs            |  Sample Training Labels\n",
        ":-------------------------:|:-------------------------:\n",
        "![](./figs/trainpatches.png)  |  ![](./figs/labelpatches.png)\n",
        "\n",
        "The other parameters that were experimented with were the number of epochs and the optimizer algorithm. For epochs, values between 1 and 50 were tried for most models. For the optimizer, a single experiment was run with one of the top performing models to see how it would compare. The imagery was compiled in Google Earth Engine via their Python API (9), exported to a Linux computer with a 48GB GPU. The model was built with tensorflow (10), code is available on GitHub (11).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MJ4kW1pEhwP"
      },
      "source": [
        "# Code for Implementing the Model\n",
        "\n",
        "Below are the chunks of code to gather the data. To run it yourself, copy and paste the .env.example file to .env and fill in your own file paths and Google Earth Engine credentials. You need an account on Google Cloud Platform with Storage Buckets and access to the Google Earth Engine API to run this, but it can be adapted to export to Google Drive instead before copying it locally."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configure the Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OrderedDict([('GCS_PROJECT', 'master-thesis-ilg'), ('BUCKET', 'csci5922-proj'), ('HOME_PATH', '/home/isly9493/'), ('LOCAL_PATH', '/local/isly9493/'), ('DATA_PATH', 'unet-viirs/data/'), ('FOLDER', 'viirs-africa-many/'), ('GCLOUD_PATH', '/home/isly9493/google-cloud-sdk/bin/'), ('CUDA_VISIBLE_DEVICES', '1')])\n"
          ]
        }
      ],
      "source": [
        "# Read in the .env file\n",
        "from dotenv import dotenv_values, load_dotenv\n",
        "\n",
        "# load env variables\n",
        "config = dotenv_values(\".env\")\n",
        "load_dotenv()\n",
        "\n",
        "# set env variables\n",
        "GCS_PROJECT = config['GCS_PROJECT']\n",
        "BUCKET = config['BUCKET']\n",
        "HOME_PATH = config['HOME_PATH']\n",
        "LOCAL_PATH = config['LOCAL_PATH']\n",
        "DATA_PATH = config['DATA_PATH']\n",
        "FOLDER = config['FOLDER']\n",
        "GCLOUD_PATH = config['GCLOUD_PATH']\n",
        "CUDA_VISIBLE_DEVICES = config['CUDA_VISIBLE_DEVICES']\n",
        "\n",
        "print(config)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import Key Libraries and Authenticate to Google Earth Engine\n",
        "\n",
        "We use Earth Engine to fetch, process, and display our imagery. We use tensorflow to build and train our model. Finally, we use folium to create interactive maps to display our results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "jat01FEoUMqg"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.15.0\n",
            "0.15.1\n"
          ]
        }
      ],
      "source": [
        "# Import, authenticate and initialize the Earth Engine library.\n",
        "import ee\n",
        "ee.Authenticate()\n",
        "ee.Initialize()\n",
        "\n",
        "# Tensorflow setup.\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "# Folium setup.\n",
        "import folium\n",
        "print(folium.__version__)\n",
        "\n",
        "# supress warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmfKLl9XcnGJ"
      },
      "source": [
        "## Set other Global Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "psz7wJKalaoj"
      },
      "outputs": [],
      "source": [
        "# variables for image names\n",
        "TRAINING_BASE = 'training_patches'\n",
        "EVAL_BASE = 'eval_patches'\n",
        "\n",
        "# Specify inputs (Landsat bands) to the model and the response variable.\n",
        "opticalBands = ['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7']\n",
        "thermalBands = ['B10', 'B11']\n",
        "BANDS = opticalBands + thermalBands\n",
        "RESPONSE = 'avg_rad' ##### \n",
        "FEATURES = BANDS + [RESPONSE]\n",
        "\n",
        "# Specify the size and shape of patches expected by the model.\n",
        "KERNEL_SIZE = 256\n",
        "KERNEL_SHAPE = [KERNEL_SIZE, KERNEL_SIZE]\n",
        "COLUMNS = [\n",
        "  tf.io.FixedLenFeature(shape=KERNEL_SHAPE, dtype=tf.float32) for k in FEATURES\n",
        "]\n",
        "FEATURES_DICT = dict(zip(FEATURES, COLUMNS))\n",
        "\n",
        "# Sizes of the training and evaluation datasets.\n",
        "TRAIN_SIZE = 4800\n",
        "EVAL_SIZE = 200\n",
        "\n",
        "# Specify model training parameters.\n",
        "BATCH_SIZE = 48 \n",
        "EPOCHS = 1\n",
        "BUFFER_SIZE = 2000\n",
        "OPTIMIZER = 'SGD'\n",
        "LOSS = 'MeanSquaredError'\n",
        "METRICS = ['RootMeanSquaredError']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgoDc7Hilfc4"
      },
      "source": [
        "# Data Preparation\n",
        "\n",
        "Gather and setup the imagery to use for inputs (predictors).  This is a three-year, cloud-free, Landsat 8 composite.  Display it in the notebook for a sanity check."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "-IlgXu-vcUEY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B10', 'B11']\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div style=\"width:100%;\"><div style=\"position:relative;width:100%;height:0;padding-bottom:60%;\"><span style=\"color:#565656\">Make this Notebook Trusted to load map: File -> Trust Notebook</span><iframe srcdoc=\"&lt;!DOCTYPE html&gt;\n",
              "&lt;html&gt;\n",
              "&lt;head&gt;\n",
              "    \n",
              "    &lt;meta http-equiv=&quot;content-type&quot; content=&quot;text/html; charset=UTF-8&quot; /&gt;\n",
              "    \n",
              "        &lt;script&gt;\n",
              "            L_NO_TOUCH = false;\n",
              "            L_DISABLE_3D = false;\n",
              "        &lt;/script&gt;\n",
              "    \n",
              "    &lt;style&gt;html, body {width: 100%;height: 100%;margin: 0;padding: 0;}&lt;/style&gt;\n",
              "    &lt;style&gt;#map {position:absolute;top:0;bottom:0;right:0;left:0;}&lt;/style&gt;\n",
              "    &lt;script src=&quot;https://cdn.jsdelivr.net/npm/leaflet@1.9.3/dist/leaflet.js&quot;&gt;&lt;/script&gt;\n",
              "    &lt;script src=&quot;https://code.jquery.com/jquery-3.7.1.min.js&quot;&gt;&lt;/script&gt;\n",
              "    &lt;script src=&quot;https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/js/bootstrap.bundle.min.js&quot;&gt;&lt;/script&gt;\n",
              "    &lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.js&quot;&gt;&lt;/script&gt;\n",
              "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/leaflet@1.9.3/dist/leaflet.css&quot;/&gt;\n",
              "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/css/bootstrap.min.css&quot;/&gt;\n",
              "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://netdna.bootstrapcdn.com/bootstrap/3.0.0/css/bootstrap.min.css&quot;/&gt;\n",
              "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.2.0/css/all.min.css&quot;/&gt;\n",
              "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.css&quot;/&gt;\n",
              "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/gh/python-visualization/folium/folium/templates/leaflet.awesome.rotate.min.css&quot;/&gt;\n",
              "    \n",
              "            &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width,\n",
              "                initial-scale=1.0, maximum-scale=1.0, user-scalable=no&quot; /&gt;\n",
              "            &lt;style&gt;\n",
              "                #map_ee9fe783913592fba9d3859f6647b142 {\n",
              "                    position: relative;\n",
              "                    width: 100.0%;\n",
              "                    height: 100.0%;\n",
              "                    left: 0.0%;\n",
              "                    top: 0.0%;\n",
              "                }\n",
              "                .leaflet-container { font-size: 1rem; }\n",
              "            &lt;/style&gt;\n",
              "        \n",
              "&lt;/head&gt;\n",
              "&lt;body&gt;\n",
              "    \n",
              "    \n",
              "            &lt;div class=&quot;folium-map&quot; id=&quot;map_ee9fe783913592fba9d3859f6647b142&quot; &gt;&lt;/div&gt;\n",
              "        \n",
              "&lt;/body&gt;\n",
              "&lt;script&gt;\n",
              "    \n",
              "    \n",
              "            var map_ee9fe783913592fba9d3859f6647b142 = L.map(\n",
              "                &quot;map_ee9fe783913592fba9d3859f6647b142&quot;,\n",
              "                {\n",
              "                    center: [-1.0, 37.0],\n",
              "                    crs: L.CRS.EPSG3857,\n",
              "                    zoom: 6,\n",
              "                    zoomControl: true,\n",
              "                    preferCanvas: false,\n",
              "                }\n",
              "            );\n",
              "\n",
              "            \n",
              "\n",
              "        \n",
              "    \n",
              "            var tile_layer_5431b82b0e40afa3f82460705737168f = L.tileLayer(\n",
              "                &quot;https://tile.openstreetmap.org/{z}/{x}/{y}.png&quot;,\n",
              "                {&quot;attribution&quot;: &quot;\\u0026copy; \\u003ca href=\\&quot;https://www.openstreetmap.org/copyright\\&quot;\\u003eOpenStreetMap\\u003c/a\\u003e contributors&quot;, &quot;detectRetina&quot;: false, &quot;maxNativeZoom&quot;: 19, &quot;maxZoom&quot;: 19, &quot;minZoom&quot;: 0, &quot;noWrap&quot;: false, &quot;opacity&quot;: 1, &quot;subdomains&quot;: &quot;abc&quot;, &quot;tms&quot;: false}\n",
              "            );\n",
              "        \n",
              "    \n",
              "            tile_layer_5431b82b0e40afa3f82460705737168f.addTo(map_ee9fe783913592fba9d3859f6647b142);\n",
              "        \n",
              "    \n",
              "            var tile_layer_afcf938b844e627d77d32df5243edbc2 = L.tileLayer(\n",
              "                &quot;https://earthengine.googleapis.com/v1/projects/earthengine-legacy/maps/310f3b2426b7bf7f16371db8e9f1c0fa-e7746ad408d26337479251c2df23fe03/tiles/{z}/{x}/{y}&quot;,\n",
              "                {&quot;attribution&quot;: &quot;Map Data \\u0026copy; \\u003ca href=\\&quot;https://earthengine.google.com/\\&quot;\\u003eGoogle Earth Engine\\u003c/a\\u003e&quot;, &quot;detectRetina&quot;: false, &quot;maxNativeZoom&quot;: 18, &quot;maxZoom&quot;: 18, &quot;minZoom&quot;: 0, &quot;noWrap&quot;: false, &quot;opacity&quot;: 1, &quot;subdomains&quot;: &quot;abc&quot;, &quot;tms&quot;: false}\n",
              "            );\n",
              "        \n",
              "    \n",
              "            tile_layer_afcf938b844e627d77d32df5243edbc2.addTo(map_ee9fe783913592fba9d3859f6647b142);\n",
              "        \n",
              "    \n",
              "            var tile_layer_1833bf4a01dacf369b1de97067915d0a = L.tileLayer(\n",
              "                &quot;https://earthengine.googleapis.com/v1/projects/earthengine-legacy/maps/7a203266941255964bbbf00c04322f82-30e191e040935872f4efa29226f9041e/tiles/{z}/{x}/{y}&quot;,\n",
              "                {&quot;attribution&quot;: &quot;Map Data \\u0026copy; \\u003ca href=\\&quot;https://earthengine.google.com/\\&quot;\\u003eGoogle Earth Engine\\u003c/a\\u003e&quot;, &quot;detectRetina&quot;: false, &quot;maxNativeZoom&quot;: 18, &quot;maxZoom&quot;: 18, &quot;minZoom&quot;: 0, &quot;noWrap&quot;: false, &quot;opacity&quot;: 1, &quot;subdomains&quot;: &quot;abc&quot;, &quot;tms&quot;: false}\n",
              "            );\n",
              "        \n",
              "    \n",
              "            tile_layer_1833bf4a01dacf369b1de97067915d0a.addTo(map_ee9fe783913592fba9d3859f6647b142);\n",
              "        \n",
              "    \n",
              "            var layer_control_a59bc6a193eb389a4e83ff3f2f3e827b_layers = {\n",
              "                base_layers : {\n",
              "                    &quot;openstreetmap&quot; : tile_layer_5431b82b0e40afa3f82460705737168f,\n",
              "                },\n",
              "                overlays :  {\n",
              "                    &quot;median composite&quot; : tile_layer_afcf938b844e627d77d32df5243edbc2,\n",
              "                    &quot;thermal&quot; : tile_layer_1833bf4a01dacf369b1de97067915d0a,\n",
              "                },\n",
              "            };\n",
              "            let layer_control_a59bc6a193eb389a4e83ff3f2f3e827b = L.control.layers(\n",
              "                layer_control_a59bc6a193eb389a4e83ff3f2f3e827b_layers.base_layers,\n",
              "                layer_control_a59bc6a193eb389a4e83ff3f2f3e827b_layers.overlays,\n",
              "                {&quot;autoZIndex&quot;: true, &quot;collapsed&quot;: true, &quot;position&quot;: &quot;topright&quot;}\n",
              "            ).addTo(map_ee9fe783913592fba9d3859f6647b142);\n",
              "\n",
              "        \n",
              "&lt;/script&gt;\n",
              "&lt;/html&gt;\" style=\"position:absolute;width:100%;height:100%;left:0;top:0;border:none !important;\" allowfullscreen webkitallowfullscreen mozallowfullscreen></iframe></div></div>"
            ],
            "text/plain": [
              "<folium.folium.Map at 0x7ffa4d693c50>"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Use Landsat 8 surface reflectance data.\n",
        "l8sr = ee.ImageCollection('LANDSAT/LC08/C01/T1_SR')\n",
        "\n",
        "# Cloud masking function.\n",
        "def maskL8sr(image):\n",
        "  cloudShadowBitMask = ee.Number(2).pow(3).int()\n",
        "  cloudsBitMask = ee.Number(2).pow(5).int()\n",
        "  qa = image.select('pixel_qa')\n",
        "  mask1 = qa.bitwiseAnd(cloudShadowBitMask).eq(0).And(\n",
        "    qa.bitwiseAnd(cloudsBitMask).eq(0))\n",
        "  mask2 = image.mask().reduce('min')\n",
        "  mask3 = image.select(opticalBands).gt(0).And(\n",
        "          image.select(opticalBands).lt(10000)).reduce('min')\n",
        "  mask = mask1.And(mask2).And(mask3)\n",
        "  return image.select(opticalBands).divide(10000).addBands(\n",
        "          image.select(thermalBands).divide(10).clamp(273.15, 373.15)\n",
        "            .subtract(273.15).divide(100)).updateMask(mask)\n",
        "\n",
        "# The image input data is a cloud-masked median composite.\n",
        "image = l8sr.filterDate('2020-01-01', '2022-12-31').map(maskL8sr).median()\n",
        "\n",
        "# printn out the bands\n",
        "print(image.bandNames().getInfo())\n",
        "\n",
        "# Use folium to visualize the imagery.\n",
        "mapid = image.getMapId({'bands': ['B4', 'B3', 'B2'], 'min': 0, 'max': 0.3})\n",
        "map = folium.Map(location=[-1, 37], zoom_start=6)\n",
        "folium.TileLayer(\n",
        "    tiles=mapid['tile_fetcher'].url_format,\n",
        "    attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "    overlay=True,\n",
        "    name='median composite',\n",
        "  ).add_to(map)\n",
        "\n",
        "mapid = image.getMapId({'bands': ['B10'], 'min': 0, 'max': 0.5})\n",
        "folium.TileLayer(\n",
        "    tiles=mapid['tile_fetcher'].url_format,\n",
        "    attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "    overlay=True,\n",
        "    name='thermal',\n",
        "  ).add_to(map)\n",
        "map.add_child(folium.LayerControl())\n",
        "map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHznnctkJsZJ"
      },
      "source": [
        "Prepare the response (what we want to predict).  This is nighttime brightness (in nanoWatts/sr/cm^2) from the VIIRS dataset.  Display to check."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "e0wHDyxVirec"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div style=\"width:100%;\"><div style=\"position:relative;width:100%;height:0;padding-bottom:60%;\"><span style=\"color:#565656\">Make this Notebook Trusted to load map: File -> Trust Notebook</span><iframe srcdoc=\"&lt;!DOCTYPE html&gt;\n",
              "&lt;html&gt;\n",
              "&lt;head&gt;\n",
              "    \n",
              "    &lt;meta http-equiv=&quot;content-type&quot; content=&quot;text/html; charset=UTF-8&quot; /&gt;\n",
              "    \n",
              "        &lt;script&gt;\n",
              "            L_NO_TOUCH = false;\n",
              "            L_DISABLE_3D = false;\n",
              "        &lt;/script&gt;\n",
              "    \n",
              "    &lt;style&gt;html, body {width: 100%;height: 100%;margin: 0;padding: 0;}&lt;/style&gt;\n",
              "    &lt;style&gt;#map {position:absolute;top:0;bottom:0;right:0;left:0;}&lt;/style&gt;\n",
              "    &lt;script src=&quot;https://cdn.jsdelivr.net/npm/leaflet@1.9.3/dist/leaflet.js&quot;&gt;&lt;/script&gt;\n",
              "    &lt;script src=&quot;https://code.jquery.com/jquery-3.7.1.min.js&quot;&gt;&lt;/script&gt;\n",
              "    &lt;script src=&quot;https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/js/bootstrap.bundle.min.js&quot;&gt;&lt;/script&gt;\n",
              "    &lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.js&quot;&gt;&lt;/script&gt;\n",
              "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/leaflet@1.9.3/dist/leaflet.css&quot;/&gt;\n",
              "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/css/bootstrap.min.css&quot;/&gt;\n",
              "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://netdna.bootstrapcdn.com/bootstrap/3.0.0/css/bootstrap.min.css&quot;/&gt;\n",
              "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.2.0/css/all.min.css&quot;/&gt;\n",
              "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.css&quot;/&gt;\n",
              "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/gh/python-visualization/folium/folium/templates/leaflet.awesome.rotate.min.css&quot;/&gt;\n",
              "    \n",
              "            &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width,\n",
              "                initial-scale=1.0, maximum-scale=1.0, user-scalable=no&quot; /&gt;\n",
              "            &lt;style&gt;\n",
              "                #map_1e79cb6910a4eaa2ee1e03cf1dd83da3 {\n",
              "                    position: relative;\n",
              "                    width: 100.0%;\n",
              "                    height: 100.0%;\n",
              "                    left: 0.0%;\n",
              "                    top: 0.0%;\n",
              "                }\n",
              "                .leaflet-container { font-size: 1rem; }\n",
              "            &lt;/style&gt;\n",
              "        \n",
              "&lt;/head&gt;\n",
              "&lt;body&gt;\n",
              "    \n",
              "    \n",
              "            &lt;div class=&quot;folium-map&quot; id=&quot;map_1e79cb6910a4eaa2ee1e03cf1dd83da3&quot; &gt;&lt;/div&gt;\n",
              "        \n",
              "&lt;/body&gt;\n",
              "&lt;script&gt;\n",
              "    \n",
              "    \n",
              "            var map_1e79cb6910a4eaa2ee1e03cf1dd83da3 = L.map(\n",
              "                &quot;map_1e79cb6910a4eaa2ee1e03cf1dd83da3&quot;,\n",
              "                {\n",
              "                    center: [-1.0, 37.0],\n",
              "                    crs: L.CRS.EPSG3857,\n",
              "                    zoom: 6,\n",
              "                    zoomControl: true,\n",
              "                    preferCanvas: false,\n",
              "                }\n",
              "            );\n",
              "\n",
              "            \n",
              "\n",
              "        \n",
              "    \n",
              "            var tile_layer_b2f8f94a9f507c21611dcc67a7d7b62b = L.tileLayer(\n",
              "                &quot;https://tile.openstreetmap.org/{z}/{x}/{y}.png&quot;,\n",
              "                {&quot;attribution&quot;: &quot;\\u0026copy; \\u003ca href=\\&quot;https://www.openstreetmap.org/copyright\\&quot;\\u003eOpenStreetMap\\u003c/a\\u003e contributors&quot;, &quot;detectRetina&quot;: false, &quot;maxNativeZoom&quot;: 19, &quot;maxZoom&quot;: 19, &quot;minZoom&quot;: 0, &quot;noWrap&quot;: false, &quot;opacity&quot;: 1, &quot;subdomains&quot;: &quot;abc&quot;, &quot;tms&quot;: false}\n",
              "            );\n",
              "        \n",
              "    \n",
              "            tile_layer_b2f8f94a9f507c21611dcc67a7d7b62b.addTo(map_1e79cb6910a4eaa2ee1e03cf1dd83da3);\n",
              "        \n",
              "    \n",
              "            var tile_layer_3a04264e4d643a477900feb4d65d1329 = L.tileLayer(\n",
              "                &quot;https://earthengine.googleapis.com/v1/projects/earthengine-legacy/maps/f3c845191b5cfa88f91236496b3beae3-1d7ff4615f717b8ff76021ed8341e026/tiles/{z}/{x}/{y}&quot;,\n",
              "                {&quot;attribution&quot;: &quot;Map Data \\u0026copy; \\u003ca href=\\&quot;https://earthengine.google.com/\\&quot;\\u003eGoogle Earth Engine\\u003c/a\\u003e&quot;, &quot;detectRetina&quot;: false, &quot;maxNativeZoom&quot;: 18, &quot;maxZoom&quot;: 18, &quot;minZoom&quot;: 0, &quot;noWrap&quot;: false, &quot;opacity&quot;: 1, &quot;subdomains&quot;: &quot;abc&quot;, &quot;tms&quot;: false}\n",
              "            );\n",
              "        \n",
              "    \n",
              "            tile_layer_3a04264e4d643a477900feb4d65d1329.addTo(map_1e79cb6910a4eaa2ee1e03cf1dd83da3);\n",
              "        \n",
              "    \n",
              "            var layer_control_fb657e3a709a4a640335a579217b492f_layers = {\n",
              "                base_layers : {\n",
              "                    &quot;openstreetmap&quot; : tile_layer_b2f8f94a9f507c21611dcc67a7d7b62b,\n",
              "                },\n",
              "                overlays :  {\n",
              "                    &quot;viirs dnb&quot; : tile_layer_3a04264e4d643a477900feb4d65d1329,\n",
              "                },\n",
              "            };\n",
              "            let layer_control_fb657e3a709a4a640335a579217b492f = L.control.layers(\n",
              "                layer_control_fb657e3a709a4a640335a579217b492f_layers.base_layers,\n",
              "                layer_control_fb657e3a709a4a640335a579217b492f_layers.overlays,\n",
              "                {&quot;autoZIndex&quot;: true, &quot;collapsed&quot;: true, &quot;position&quot;: &quot;topright&quot;}\n",
              "            ).addTo(map_1e79cb6910a4eaa2ee1e03cf1dd83da3);\n",
              "\n",
              "        \n",
              "&lt;/script&gt;\n",
              "&lt;/html&gt;\" style=\"position:absolute;width:100%;height:100%;left:0;top:0;border:none !important;\" allowfullscreen webkitallowfullscreen mozallowfullscreen></iframe></div></div>"
            ],
            "text/plain": [
              "<folium.folium.Map at 0x7ffa4d4d75d0>"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "viirs = ee.ImageCollection('NOAA/VIIRS/DNB/MONTHLY_V1/VCMSLCFG').select('avg_rad').filterDate('2020-01-01', '2022-12-31').median()\n",
        "\n",
        "mapid = viirs.getMapId({'min': 0, 'max': 1}) \n",
        "map = folium.Map(location=[-1, 37], zoom_start=6)\n",
        "folium.TileLayer(\n",
        "    tiles=mapid['tile_fetcher'].url_format,\n",
        "    attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "    overlay=True,\n",
        "    name='viirs dnb',\n",
        "  ).add_to(map)\n",
        "map.add_child(folium.LayerControl())\n",
        "map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTS7_ZzPDhhg"
      },
      "source": [
        "Stack the 2D images (Landsat composite and VIIRS DNB image) to create a single image from which samples can be taken.  Convert the image into an array image in which each pixel stores 256x256 patches of pixels for each band.  This is a key step that bears emphasis: to export training patches, convert a multi-band image to [an array image](https://developers.google.com/earth-engine/arrays_array_images#array-images) using [`neighborhoodToArray()`](https://developers.google.com/earth-engine/api_docs#eeimageneighborhoodtoarray), then sample the image at points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "eGHYsdAOipa4"
      },
      "outputs": [],
      "source": [
        "featureStack = ee.Image.cat([\n",
        "  image.select(BANDS),\n",
        "  viirs.select(RESPONSE)\n",
        "]).float()\n",
        "\n",
        "list = ee.List.repeat(1, KERNEL_SIZE)\n",
        "lists = ee.List.repeat(list, KERNEL_SIZE)\n",
        "kernel = ee.Kernel.fixed(KERNEL_SIZE, KERNEL_SIZE, lists)\n",
        "\n",
        "arrays = featureStack.neighborhoodToArray(kernel)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4djSxBRG2el"
      },
      "source": [
        "# Define Training and Evaluation Areas\n",
        "Use some pre-made geometries to sample the stack in strategic locations.  Specifically, these are hand-made polygons in which to take the 256x256 samples.  Display the sampling polygons on a map, red for training polygons, blue for evaluation. You'll need to create these polygons ahead of time yourself and save them as a feature collection. This is easiest to do in Google Earth Engine's code editor, but can also be done via the API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ure_WaD0itQY"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div style=\"width:100%;\"><div style=\"position:relative;width:100%;height:0;padding-bottom:60%;\"><span style=\"color:#565656\">Make this Notebook Trusted to load map: File -> Trust Notebook</span><iframe srcdoc=\"&lt;!DOCTYPE html&gt;\n",
              "&lt;html&gt;\n",
              "&lt;head&gt;\n",
              "    \n",
              "    &lt;meta http-equiv=&quot;content-type&quot; content=&quot;text/html; charset=UTF-8&quot; /&gt;\n",
              "    \n",
              "        &lt;script&gt;\n",
              "            L_NO_TOUCH = false;\n",
              "            L_DISABLE_3D = false;\n",
              "        &lt;/script&gt;\n",
              "    \n",
              "    &lt;style&gt;html, body {width: 100%;height: 100%;margin: 0;padding: 0;}&lt;/style&gt;\n",
              "    &lt;style&gt;#map {position:absolute;top:0;bottom:0;right:0;left:0;}&lt;/style&gt;\n",
              "    &lt;script src=&quot;https://cdn.jsdelivr.net/npm/leaflet@1.9.3/dist/leaflet.js&quot;&gt;&lt;/script&gt;\n",
              "    &lt;script src=&quot;https://code.jquery.com/jquery-3.7.1.min.js&quot;&gt;&lt;/script&gt;\n",
              "    &lt;script src=&quot;https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/js/bootstrap.bundle.min.js&quot;&gt;&lt;/script&gt;\n",
              "    &lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.js&quot;&gt;&lt;/script&gt;\n",
              "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/leaflet@1.9.3/dist/leaflet.css&quot;/&gt;\n",
              "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/css/bootstrap.min.css&quot;/&gt;\n",
              "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://netdna.bootstrapcdn.com/bootstrap/3.0.0/css/bootstrap.min.css&quot;/&gt;\n",
              "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.2.0/css/all.min.css&quot;/&gt;\n",
              "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.css&quot;/&gt;\n",
              "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/gh/python-visualization/folium/folium/templates/leaflet.awesome.rotate.min.css&quot;/&gt;\n",
              "    \n",
              "            &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width,\n",
              "                initial-scale=1.0, maximum-scale=1.0, user-scalable=no&quot; /&gt;\n",
              "            &lt;style&gt;\n",
              "                #map_744d7f73293654689a26bd1ea5a58ccd {\n",
              "                    position: relative;\n",
              "                    width: 100.0%;\n",
              "                    height: 100.0%;\n",
              "                    left: 0.0%;\n",
              "                    top: 0.0%;\n",
              "                }\n",
              "                .leaflet-container { font-size: 1rem; }\n",
              "            &lt;/style&gt;\n",
              "        \n",
              "&lt;/head&gt;\n",
              "&lt;body&gt;\n",
              "    \n",
              "    \n",
              "            &lt;div class=&quot;folium-map&quot; id=&quot;map_744d7f73293654689a26bd1ea5a58ccd&quot; &gt;&lt;/div&gt;\n",
              "        \n",
              "&lt;/body&gt;\n",
              "&lt;script&gt;\n",
              "    \n",
              "    \n",
              "            var map_744d7f73293654689a26bd1ea5a58ccd = L.map(\n",
              "                &quot;map_744d7f73293654689a26bd1ea5a58ccd&quot;,\n",
              "                {\n",
              "                    center: [0.0, 12.0],\n",
              "                    crs: L.CRS.EPSG3857,\n",
              "                    zoom: 5,\n",
              "                    zoomControl: true,\n",
              "                    preferCanvas: false,\n",
              "                }\n",
              "            );\n",
              "\n",
              "            \n",
              "\n",
              "        \n",
              "    \n",
              "            var tile_layer_841a35295968ed75d3699c38416121a3 = L.tileLayer(\n",
              "                &quot;https://tile.openstreetmap.org/{z}/{x}/{y}.png&quot;,\n",
              "                {&quot;attribution&quot;: &quot;\\u0026copy; \\u003ca href=\\&quot;https://www.openstreetmap.org/copyright\\&quot;\\u003eOpenStreetMap\\u003c/a\\u003e contributors&quot;, &quot;detectRetina&quot;: false, &quot;maxNativeZoom&quot;: 19, &quot;maxZoom&quot;: 19, &quot;minZoom&quot;: 0, &quot;noWrap&quot;: false, &quot;opacity&quot;: 1, &quot;subdomains&quot;: &quot;abc&quot;, &quot;tms&quot;: false}\n",
              "            );\n",
              "        \n",
              "    \n",
              "            tile_layer_841a35295968ed75d3699c38416121a3.addTo(map_744d7f73293654689a26bd1ea5a58ccd);\n",
              "        \n",
              "    \n",
              "            var tile_layer_e5d36d60dfb247662f2df140b603c185 = L.tileLayer(\n",
              "                &quot;https://earthengine.googleapis.com/v1/projects/earthengine-legacy/maps/89f13b8e474972f1cb143a8ae12ab0dc-2877979d521c7b11770e1da241e9592e/tiles/{z}/{x}/{y}&quot;,\n",
              "                {&quot;attribution&quot;: &quot;Map Data \\u0026copy; \\u003ca href=\\&quot;https://earthengine.google.com/\\&quot;\\u003eGoogle Earth Engine\\u003c/a\\u003e&quot;, &quot;detectRetina&quot;: false, &quot;maxNativeZoom&quot;: 18, &quot;maxZoom&quot;: 18, &quot;minZoom&quot;: 0, &quot;noWrap&quot;: false, &quot;opacity&quot;: 1, &quot;subdomains&quot;: &quot;abc&quot;, &quot;tms&quot;: false}\n",
              "            );\n",
              "        \n",
              "    \n",
              "            tile_layer_e5d36d60dfb247662f2df140b603c185.addTo(map_744d7f73293654689a26bd1ea5a58ccd);\n",
              "        \n",
              "    \n",
              "            var layer_control_8adbd2c11cea4b8faa81401b22a9978a_layers = {\n",
              "                base_layers : {\n",
              "                    &quot;openstreetmap&quot; : tile_layer_841a35295968ed75d3699c38416121a3,\n",
              "                },\n",
              "                overlays :  {\n",
              "                    &quot;training polygons&quot; : tile_layer_e5d36d60dfb247662f2df140b603c185,\n",
              "                },\n",
              "            };\n",
              "            let layer_control_8adbd2c11cea4b8faa81401b22a9978a = L.control.layers(\n",
              "                layer_control_8adbd2c11cea4b8faa81401b22a9978a_layers.base_layers,\n",
              "                layer_control_8adbd2c11cea4b8faa81401b22a9978a_layers.overlays,\n",
              "                {&quot;autoZIndex&quot;: true, &quot;collapsed&quot;: true, &quot;position&quot;: &quot;topright&quot;}\n",
              "            ).addTo(map_744d7f73293654689a26bd1ea5a58ccd);\n",
              "\n",
              "        \n",
              "&lt;/script&gt;\n",
              "&lt;/html&gt;\" style=\"position:absolute;width:100%;height:100%;left:0;top:0;border:none !important;\" allowfullscreen webkitallowfullscreen mozallowfullscreen></iframe></div></div>"
            ],
            "text/plain": [
              "<folium.folium.Map at 0x7ffa4d58b810>"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainingPolys = ee.FeatureCollection('projects/'+GCS_PROJECT+'/assets/trainingPolysAfricaBig')\n",
        "evalPolys = ee.FeatureCollection('projects/'+GCS_PROJECT+'/assets/evalPolysAfrica')\n",
        "\n",
        "polyImage = ee.Image(0).byte().paint(trainingPolys, 1).paint(evalPolys, 2)\n",
        "polyImage = polyImage.updateMask(polyImage)\n",
        "\n",
        "mapid = polyImage.getMapId({'min': 1, 'max': 2, 'palette': ['gold', 'blue']})\n",
        "map = folium.Map(location=[0, 12], zoom_start=5)\n",
        "folium.TileLayer(\n",
        "    tiles=mapid['tile_fetcher'].url_format,\n",
        "    attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "    overlay=True,\n",
        "    name='training polygons',\n",
        "  ).add_to(map)\n",
        "map.add_child(folium.LayerControl())\n",
        "map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZV890gPHeZqz"
      },
      "source": [
        "# Sampling\n",
        "\n",
        "The mapped data look reasonable so take a sample from each polygon and merge the results into a single export.  The key step is sampling the array image at points, to get all the pixels in a 256x256 neighborhood at each point.  It's worth noting that to build the training and testing data for the FCNN, you export a single TFRecord file that contains patches of pixel values in each record.  You do not need to export each training/testing patch to a different image.  Since each record potentially contains a lot of data (especially with big patches or many input bands), some manual sharding of the computation is necessary to avoid the `computed value too large` error.  Specifically, the following code takes multiple (smaller) samples within each geometry, merging the results to get a single export."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "cellView": "both",
        "id": "FyRpvwENxE-A"
      },
      "outputs": [],
      "source": [
        "# Convert the feature collections to lists for iteration.\n",
        "trainingPolysList = trainingPolys.toList(trainingPolys.size())\n",
        "evalPolysList = evalPolys.toList(evalPolys.size())\n",
        "\n",
        "# These numbers determined experimentally.\n",
        "n = 20 # Number of shards in each polygon. \n",
        "N = 200 # Total sample size in each polygon.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Export Training & Evaluation Data from Earth Engine to Google Cloud Storage\n",
        "This can be modified to export to local storage, just adjust the file paths."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 viirs-africa-many/ csci5922-proj training_patches_g0\n",
            "1 viirs-africa-many/ csci5922-proj training_patches_g1\n",
            "2 viirs-africa-many/ csci5922-proj training_patches_g2\n",
            "3 viirs-africa-many/ csci5922-proj training_patches_g3\n",
            "4 viirs-africa-many/ csci5922-proj training_patches_g4\n",
            "5 viirs-africa-many/ csci5922-proj training_patches_g5\n",
            "6 viirs-africa-many/ csci5922-proj training_patches_g6\n",
            "7 viirs-africa-many/ csci5922-proj training_patches_g7\n",
            "8 viirs-africa-many/ csci5922-proj training_patches_g8\n",
            "9 viirs-africa-many/ csci5922-proj training_patches_g9\n",
            "10 viirs-africa-many/ csci5922-proj training_patches_g10\n",
            "11 viirs-africa-many/ csci5922-proj training_patches_g11\n",
            "12 viirs-africa-many/ csci5922-proj training_patches_g12\n",
            "13 viirs-africa-many/ csci5922-proj training_patches_g13\n",
            "14 viirs-africa-many/ csci5922-proj training_patches_g14\n",
            "15 viirs-africa-many/ csci5922-proj training_patches_g15\n",
            "16 viirs-africa-many/ csci5922-proj training_patches_g16\n",
            "17 viirs-africa-many/ csci5922-proj training_patches_g17\n",
            "18 viirs-africa-many/ csci5922-proj training_patches_g18\n",
            "19 viirs-africa-many/ csci5922-proj training_patches_g19\n",
            "20 viirs-africa-many/ csci5922-proj training_patches_g20\n",
            "21 viirs-africa-many/ csci5922-proj training_patches_g21\n",
            "22 viirs-africa-many/ csci5922-proj training_patches_g22\n",
            "23 viirs-africa-many/ csci5922-proj training_patches_g23\n",
            "Training imagery tasks submitted. Check on the Tasks in GEE for their status (https://code.earthengine.google.com/tasks).\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Export all the training data (in many pieces), with one task\n",
        "# per geometry.\n",
        "for g in range(trainingPolys.size().getInfo()):\n",
        "  geomSample = ee.FeatureCollection([])\n",
        "  for i in range(n):\n",
        "    sample = arrays.sample(\n",
        "      region = ee.Feature(trainingPolysList.get(g)).geometry(),\n",
        "      scale = 30,\n",
        "      numPixels = N / n, # Size of the shard.\n",
        "      seed = i,\n",
        "      tileScale = 8\n",
        "    )\n",
        "    geomSample = geomSample.merge(sample)\n",
        "  \n",
        "  desc = TRAINING_BASE + '_g' + str(g)\n",
        "  print(g, FOLDER, BUCKET, desc)\n",
        "  \n",
        "  task = ee.batch.Export.table.toCloudStorage(\n",
        "    collection = geomSample,\n",
        "    description = desc,\n",
        "    bucket = BUCKET,\n",
        "    fileNamePrefix = FOLDER + desc,\n",
        "    fileFormat = 'TFRecord',\n",
        "    selectors = BANDS + [RESPONSE]\n",
        "  )\n",
        "  task.start()\n",
        "print('Training imagery tasks submitted. Check on the Tasks in GEE for their status (https://code.earthengine.google.com/tasks).')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 viirs-africa-many/ csci5922-proj eval_patches_g0\n",
            "1 viirs-africa-many/ csci5922-proj eval_patches_g1\n",
            "Eval imagery tasks submitted. Check on the Tasks in GEE for their status (https://code.earthengine.google.com/tasks).\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Export all the evaluation data.\n",
        "for g in range(evalPolys.size().getInfo()):\n",
        "  geomSample = ee.FeatureCollection([])\n",
        "  for i in range(n):\n",
        "    sample = arrays.sample(\n",
        "      region = ee.Feature(evalPolysList.get(g)).geometry(),\n",
        "      scale = 30,\n",
        "      numPixels = N / n,\n",
        "      seed = i,\n",
        "      tileScale = 8\n",
        "    )\n",
        "    geomSample = geomSample.merge(sample)\n",
        "\n",
        "  desc = EVAL_BASE + '_g' + str(g)\n",
        "  print(g, FOLDER, BUCKET, desc)\n",
        "  \n",
        "  task = ee.batch.Export.table.toCloudStorage(\n",
        "    collection = geomSample,\n",
        "    description = desc,\n",
        "    bucket = BUCKET,\n",
        "    fileNamePrefix = FOLDER + desc,\n",
        "    fileFormat = 'TFRecord',\n",
        "    selectors = BANDS + [RESPONSE]\n",
        "  )\n",
        "  task.start()\n",
        "print('Eval imagery tasks submitted. Check on the Tasks in GEE for their status (https://code.earthengine.google.com/tasks).')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Copy Files from Google Cloud Storage to Local Storage\n",
        "This moves the files to the local machine for training. Note that you need the Google Cloud SDK installed and authenticated to run this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Copy from your bucket to local path (note -r is for recursive call)\n",
        "!{GCLOUD_PATH}gsutil -m cp -r 'gs://'{BUCKET}'/'{FOLDER}{TRAINING_BASE}'*' {LOCAL_PATH}{DATA_PATH}{FOLDER}\n",
        "!{GCLOUD_PATH}gsutil -m cp -r 'gs://'{BUCKET}'/'{FOLDER}{EVAL_BASE}'*' {LOCAL_PATH}{DATA_PATH}{FOLDER}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Configure GPU Usage\n",
        "Use this to ensure you are using the GPUs that you intend for training. This assumes you have CUDA and cuDNN installed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# see what devices are available\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
        "print(\"GPUs Available: \", tf.config.list_physical_devices('GPU'))\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWXrvBE4607G"
      },
      "source": [
        "# Load Training and Evaluation Data in Memory for Training.\n",
        "\n",
        "Load the data exported from Earth Engine into a `tf.data.Dataset`.  The following are helper functions for that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WWZ0UXCVMyJP"
      },
      "outputs": [],
      "source": [
        "def parse_tfrecord(example_proto):\n",
        "  return tf.io.parse_single_example(example_proto, FEATURES_DICT)\n",
        "\n",
        "\n",
        "def to_tuple(inputs):\n",
        "  inputsList = [inputs.get(key) for key in FEATURES]\n",
        "  stacked = tf.stack(inputsList, axis=0)\n",
        "  # Convert from CHW to HWC\n",
        "  stacked = tf.transpose(stacked, [1, 2, 0])\n",
        "  return stacked[:,:,:len(BANDS)], stacked[:,:,len(BANDS):]\n",
        "\n",
        "\n",
        "def get_dataset(path, prefix, n_files):\n",
        "  glob = []\n",
        "  for i in range(0, n_files):\n",
        "    glob = path + prefix + '_g' + str(i) + '.tfrecord.gz'\n",
        "  dataset = tf.data.TFRecordDataset(glob, compression_type='GZIP')\n",
        "  dataset = dataset.map(parse_tfrecord, num_parallel_calls=5)\n",
        "  dataset = dataset.map(to_tuple, num_parallel_calls=5)\n",
        "  return dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xg1fa18336D2"
      },
      "source": [
        "Use the helpers to read in the training dataset.  Optionally print the first record to check. Careful, this will make tensorflow reserve as much GPU memory as is available to it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rm0qRF0fAYcC"
      },
      "outputs": [],
      "source": [
        "training = get_dataset(LOCAL_PATH + DATA_PATH + FOLDER, TRAINING_BASE, 24)\n",
        "training = training.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
        "# print(iter(training.take(1)).next())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-cQO5RL6vob"
      },
      "source": [
        "# Evaluation data\n",
        "\n",
        "Now do the same thing to get an evaluation dataset.  Note that unlike the training dataset, the evaluation dataset has a batch size of 1, is not repeated and is not shuffled."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ieKTCGiJ6xzo"
      },
      "outputs": [],
      "source": [
        "evaluation = get_dataset(LOCAL_PATH + DATA_PATH + FOLDER, EVAL_BASE, 2)\n",
        "evaluation = evaluation.batch(1).repeat()\n",
        "# print(iter(evaluation.take(1)).next()[0][0,:,:,1:3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# visualize of the of the training / evaluation image patch pairs\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def display_batch_of_images(databatch):\n",
        "    imgs, labels = databatch\n",
        "    # landsat in false color\n",
        "    plt.figure(figsize=(10,10))\n",
        "    label = labels.numpy().astype(np.float32)\n",
        "    for i in range(25):\n",
        "        plt.subplot(5,5,i+1)\n",
        "        plt.imshow(imgs[i,:,:,0:3]*8) # select landsat bands here\n",
        "        plt.axis('off')\n",
        "    plt.show()\n",
        "    # show viirs in grayscale\n",
        "    plt.figure(figsize=(10,10))\n",
        "    for i in range(25):\n",
        "        plt.subplot(5,5,i+1)\n",
        "        plt.imshow(labels[i,:,:,0], cmap='gray')\n",
        "        plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "display_batch_of_images(iter(training.take(1)).next())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JIE7Yl87lgU"
      },
      "source": [
        "# Data Analysis & Deep Learning Model\n",
        "\n",
        "Here we use the Keras implementation of the U-Net model.  The U-Net model takes 256x256 pixel patches as input and outputs per-pixel class probability, label or a continuous output.  We can implement the model essentially unmodified, but will use mean squared error loss on the sigmoidal output since we are treating this as a regression problem, rather than a classification problem. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wsnnnz56yS3l"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "from keras import losses\n",
        "from keras import models\n",
        "from keras import metrics\n",
        "from keras import optimizers\n",
        "\n",
        "def conv_block(input_tensor, num_filters):\n",
        "\tencoder = layers.Conv2D(num_filters, (3, 3), padding='same')(input_tensor)\n",
        "\tencoder = layers.BatchNormalization()(encoder)\n",
        "\tencoder = layers.Activation('relu')(encoder)\n",
        "\tencoder = layers.Conv2D(num_filters, (3, 3), padding='same')(encoder)\n",
        "\tencoder = layers.BatchNormalization()(encoder)\n",
        "\tencoder = layers.Activation('relu')(encoder)\n",
        "\treturn encoder\n",
        "\n",
        "def encoder_block(input_tensor, num_filters):\n",
        "\tencoder = conv_block(input_tensor, num_filters)\n",
        "\tencoder_pool = layers.MaxPooling2D((2, 2), strides=(2, 2))(encoder)\n",
        "\treturn encoder_pool, encoder\n",
        "\n",
        "\n",
        "def decoder_block(input_tensor, concat_tensor, num_filters):\n",
        "\tdecoder = layers.Conv2DTranspose(num_filters, (2, 2), strides=(2, 2), padding='same')(input_tensor)\n",
        "\tdecoder = layers.concatenate([concat_tensor, decoder], axis=-1)\n",
        "\tdecoder = layers.BatchNormalization()(decoder)\n",
        "\tdecoder = layers.Activation('relu')(decoder)\n",
        "\tdecoder = layers.Conv2D(num_filters, (3, 3), padding='same')(decoder)\n",
        "\tdecoder = layers.BatchNormalization()(decoder)\n",
        "\tdecoder = layers.Activation('relu')(decoder)\n",
        "\tdecoder = layers.Conv2D(num_filters, (3, 3), padding='same')(decoder)\n",
        "\tdecoder = layers.BatchNormalization()(decoder)\n",
        "\tdecoder = layers.Activation('relu')(decoder)\n",
        "\treturn decoder\n",
        "\n",
        "def get_model():\n",
        "\tinputs = layers.Input(shape=[None, None, len(BANDS)]) # 256\n",
        "\tencoder0_pool, encoder0 = encoder_block(inputs, 32) # 128\n",
        "\tencoder1_pool, encoder1 = encoder_block(encoder0_pool, 64) # 64\n",
        "\tencoder2_pool, encoder2 = encoder_block(encoder1_pool, 128) # 32\n",
        "\tencoder3_pool, encoder3 = encoder_block(encoder2_pool, 256) # 16\n",
        "\tencoder4_pool, encoder4 = encoder_block(encoder3_pool, 512) # 8\n",
        "\tcenter = conv_block(encoder4_pool, 1024) # center\n",
        "\tdecoder4 = decoder_block(center, encoder4, 512) # 16\n",
        "\tdecoder3 = decoder_block(decoder4, encoder3, 256) # 32\n",
        "\tdecoder2 = decoder_block(decoder3, encoder2, 128) # 64\n",
        "\tdecoder1 = decoder_block(decoder2, encoder1, 64) # 128\n",
        "\tdecoder0 = decoder_block(decoder1, encoder0, 32) # 256\n",
        "\n",
        "\t# to change your output activation function, change this part\n",
        "\toutputs = layers.Conv2D(1, (1, 1), activation='sigmoid')(decoder0)\n",
        "\n",
        "\tmodel = models.Model(inputs=[inputs], outputs=[outputs])\n",
        "\t\n",
        "\t# set optimizer with custom learning rate\n",
        "\tOPTIMIZER = 'SGD'\n",
        "\toptimizer = optimizers.get(OPTIMIZER)\n",
        "\t\n",
        "\t# to adjust the learning rate:\n",
        "\t# optimizer.learning_rate = 0.1\n",
        "\n",
        "\t# compile the model\n",
        "\tmodel.compile(\n",
        "\t\toptimizer=optimizer,\n",
        "\t\tloss=losses.get(LOSS),\n",
        "\t\tmetrics=[metrics.get(metric) for metric in METRICS])\n",
        "\n",
        "\treturn model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uu_E7OTDBCoS"
      },
      "source": [
        "# Training the model\n",
        "\n",
        "You train a Keras model by calling `.fit()` on it.  Here we're going to train for 10 epochs, which is suitable for demonstration purposes.  For production use, you probably want to optimize this parameter, for example through [hyperparamter tuning](https://cloud.google.com/ml-engine/docs/tensorflow/using-hyperparameter-tuning)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NzzaWxOhSxBy"
      },
      "outputs": [],
      "source": [
        "# print the current time to log how long it takes to train\n",
        "from datetime import datetime\n",
        "print(datetime.now())\n",
        "\n",
        "# get the model fresh and train it\n",
        "m = get_model()\n",
        "history = m.fit(\n",
        "    x=training,\n",
        "    epochs=EPOCHS,\n",
        "    steps_per_epoch=int(TRAIN_SIZE / BATCH_SIZE),\n",
        "    validation_data=evaluation,\n",
        "    validation_steps=EVAL_SIZE\n",
        ")\n",
        "\n",
        "# print the end time to log how long it takes to train\n",
        "print(datetime.now())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# set some parameters for saving the model\n",
        "MODEL_NAME = 'viirs_africa_many_e1.keras'\n",
        "HISTORY_NAME = 'viirs_africa_many_e1_history.pickle'\n",
        "ASSET_SUFFIX = 'viirs_africa_many_e1'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# save the model to a local file \n",
        "m.save(LOCAL_PATH + DATA_PATH + FOLDER + MODEL_NAME)\n",
        "\n",
        "# # save the history variable to a local file with pickle\n",
        "import pickle\n",
        "with open(LOCAL_PATH + DATA_PATH + FOLDER + HISTORY_NAME, 'wb') as f:\n",
        "  pickle.dump(history, f)\n",
        "\n",
        "# import a saved model if needed\n",
        "# m = tf.keras.models.load_model(LOCAL_PATH + DATA_PATH + FOLDER + MODEL_NAME)\n",
        "# import pickle\n",
        "# with open(HISTORY_NAME, 'rb') as f:\n",
        "# with open(LOCAL_PATH + DATA_PATH + FOLDER + HISTORY_NAME, 'rb') as f:\n",
        "#   history = pickle.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluation\n",
        "Let's look at how the model performed in terms of loss while it trained, and then evaluate it on the training and evaluation datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# plot the training and validation loss\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history['root_mean_squared_error'])\n",
        "plt.plot(history.history['val_root_mean_squared_error'])\n",
        "plt.title('Model RMSE for ' + ASSET_SUFFIX + ' for ' + str(EPOCHS) + ' epochs')\n",
        "plt.ylabel('RMSE')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Training RMSE', 'Validation RMSE'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## evaluate the model on the evaluation dataset to get the loss and RMSE\n",
        "val_loss, val_rmse = m.evaluate(evaluation, steps=EVAL_SIZE)\n",
        "print('Model validation loss: {:0.4f}. RMSE: {:0.4f}'.format(val_loss, val_rmse))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## evaluate the model on the evaluation dataset to get the loss and RMSE\n",
        "train_loss, train_rmse = m.evaluate(training, steps=10)\n",
        "print('Model train loss: {:0.4f}. RMSE: {:0.4f}'.format(train_loss, train_rmse))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1ySNup0xCqN"
      },
      "source": [
        "# Prediction\n",
        "\n",
        "Now we're going to do a qualitative assessment of the model by running it on an input image patch, specifically Brazzaville / Kinshasa. The prediction pipeline is:\n",
        "\n",
        "1.  Export imagery on which to do predictions from Earth Engine in TFRecord format to a Cloud Storage bucket.\n",
        "2.  Use the trained model to make the predictions.\n",
        "3.  Write the predictions to a TFRecord file in a Cloud Storage.\n",
        "4.  Upload the predictions TFRecord file to Earth Engine.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Specify Region and Outputs for Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FPANwc7B1-TS"
      },
      "outputs": [],
      "source": [
        "ee_user_folder = 'projects/'+GCS_PROJECT+'/assets'\n",
        "\n",
        "pred_image_base = 'FCNN_demo_kinshasa_384_'\n",
        "# Half this will extend on the sides of each patch.\n",
        "pred_kernel_buffer = [128, 128]\n",
        "# Kinshasa/Brazzaville\n",
        "pred_region = ee.Geometry.Polygon(\n",
        "  [[[15.02, -4.04],\n",
        "    [15.02, -4.54],\n",
        "    [15.52, -4.54],\n",
        "    [15.52, -4.04]]], \n",
        "  None, \n",
        "  False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Export Sample Imagery for Prediction\n",
        "This only needs to be run once.  It exports the sample imagery to a TFRecord file in a Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M3WDAa-RUpXP"
      },
      "outputs": [],
      "source": [
        "def doExport(out_image_base, kernel_buffer, region):\n",
        "  task = ee.batch.Export.image.toCloudStorage(\n",
        "    image = image.select(BANDS),\n",
        "    description = out_image_base,\n",
        "    bucket = BUCKET,\n",
        "    fileNamePrefix = FOLDER + out_image_base,\n",
        "    region = region.getInfo()['coordinates'],\n",
        "    scale = 30,\n",
        "    fileFormat = 'TFRecord',\n",
        "    maxPixels = 1e10,\n",
        "    formatOptions = {\n",
        "      'patchDimensions': KERNEL_SHAPE,\n",
        "      'kernelSize': kernel_buffer,\n",
        "      'compressed': True,\n",
        "      'maxFileSize': 104857600\n",
        "    }\n",
        "  )\n",
        "  task.start()\n",
        "  print('Prediction image export task submitted. Check on the task in GEE for its status (https://code.earthengine.google.com/tasks).')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lLNEOLkXWvSi"
      },
      "outputs": [],
      "source": [
        "# Run the export.\n",
        "doExport(pred_image_base, pred_kernel_buffer, pred_region)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# copy the prediction file from cloud storage to local\n",
        "!{GCLOUD_PATH}gsutil -m cp -r 'gs://'{BUCKET}'/'{FOLDER}{pred_image_base}'*' {LOCAL_PATH}{DATA_PATH}{FOLDER}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define Code for Running Model on Prediction Data\n",
        "Expects the prediction data to exist locally. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zb_9_FflygVw"
      },
      "outputs": [],
      "source": [
        "def doPrediction(out_image_base, kernel_buffer):\n",
        "  print('Looking for TFRecord files...')\n",
        "\n",
        "  # # Get a list of all the files in the output bucket.\n",
        "  filesList = !ls {LOCAL_PATH}{DATA_PATH}{FOLDER}\n",
        "\n",
        "  # Get only the files generated by the image export.\n",
        "  exportFilesList = [s for s in filesList if out_image_base in s]\n",
        "\n",
        "  # Get the list of image files and the JSON mixer file.\n",
        "  imageFilesList = []\n",
        "  jsonFile = None\n",
        "  for f in exportFilesList:\n",
        "    if f.endswith('.tfrecord.gz'):\n",
        "      imageFilesList.append(f)\n",
        "    elif f.endswith('.json'):\n",
        "      jsonFile = f\n",
        "  imageFilesList.sort()\n",
        "\n",
        "  from pprint import pprint\n",
        "  pprint(imageFilesList)\n",
        "  print(jsonFile)\n",
        "\n",
        "  import json\n",
        "  jsonText = !cat {LOCAL_PATH}{DATA_PATH}{FOLDER}{jsonFile}\n",
        "  print(jsonText)\n",
        "  mixer = json.loads(jsonText.nlstr)\n",
        "  pprint(mixer)\n",
        "  patches = mixer['totalPatches']\n",
        "\n",
        "  buffered_shape = [\n",
        "      KERNEL_SHAPE[0] + kernel_buffer[0],\n",
        "      KERNEL_SHAPE[1] + kernel_buffer[1]]\n",
        "\n",
        "  imageColumns = [\n",
        "    tf.io.FixedLenFeature(shape=buffered_shape, dtype=tf.float32)\n",
        "      for k in BANDS\n",
        "  ]\n",
        "\n",
        "  imageFeaturesDict = dict(zip(BANDS, imageColumns))\n",
        "\n",
        "  def parse_image(example_proto):\n",
        "    return tf.io.parse_single_example(example_proto, imageFeaturesDict)\n",
        "\n",
        "  def toTupleImage(inputs):\n",
        "    inputsList = [inputs.get(key) for key in BANDS]\n",
        "    stacked = tf.stack(inputsList, axis=0)\n",
        "    stacked = tf.transpose(stacked, [1, 2, 0])\n",
        "    return stacked\n",
        "\n",
        "  # append absolute path to imageFilesList\n",
        "  imageFilesList = [LOCAL_PATH + DATA_PATH + FOLDER + f for f in imageFilesList]\n",
        "  print(imageFilesList)\n",
        "  \n",
        "  # Create a dataset from the TFRecord file(s) in Cloud Storage.\n",
        "  imageDataset = tf.data.TFRecordDataset(imageFilesList, compression_type='GZIP')\n",
        "  imageDataset = imageDataset.map(parse_image, num_parallel_calls=5)\n",
        "  imageDataset = imageDataset.map(toTupleImage).batch(1)\n",
        "\n",
        "  # Perform inference.\n",
        "  print('Running predictions...')\n",
        "  predictions = m.predict(imageDataset, steps=patches, verbose=1)\n",
        "  # print(predictions[0])\n",
        "  return predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def writePredictions(predictions, out_image_base, kernel_buffer):\n",
        "  print('Writing predictions...')\n",
        "  out_image_file = 'gs://' + BUCKET + '/' + FOLDER + out_image_base + '.TFRecord'\n",
        "\n",
        "  # Get set up for prediction.\n",
        "  x_buffer = int(kernel_buffer[0] / 2)\n",
        "  y_buffer = int(kernel_buffer[1] / 2)\n",
        "  \n",
        "  \n",
        "  writer = tf.io.TFRecordWriter(out_image_file)\n",
        "  patches = 0\n",
        "  for predictionPatch in predictions:\n",
        "    # print('Writing patch ' + str(patches) + '...')\n",
        "    predictionPatch = predictionPatch[\n",
        "        x_buffer:x_buffer+KERNEL_SIZE, y_buffer:y_buffer+KERNEL_SIZE]\n",
        "\n",
        "    # Create an example.\n",
        "    example = tf.train.Example(\n",
        "      features=tf.train.Features(\n",
        "        feature={\n",
        "          'impervious': tf.train.Feature(\n",
        "            float_list=tf.train.FloatList(\n",
        "              value=predictionPatch.flatten()\n",
        "            )\n",
        "          )\n",
        "        }\n",
        "      )\n",
        "    )\n",
        "    # Write the example.\n",
        "    writer.write(example.SerializeToString())\n",
        "    patches += 1\n",
        "\n",
        "  print('Done writing predictions.')\n",
        "  writer.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def uploadPredictionsToEE(out_image_base, ee_user_folder):\n",
        "  # Start the upload.\n",
        "  print('Uploading to Google Earth Egnine...')\n",
        "  \n",
        "  # define file paths\n",
        "  out_image_file = 'gs://' + BUCKET + '/' + FOLDER + out_image_base + '.TFRecord'\n",
        "  out_image_asset = ee_user_folder + '/' + out_image_base + ASSET_SUFFIX\n",
        "  jsonFile = 'gs://' + BUCKET + '/' + FOLDER + out_image_base + 'mixer.json'\n",
        "  \n",
        "  print('out_image_asset: ', out_image_asset)\n",
        "  print('out_image_file: ', out_image_file)\n",
        "  print('jsonFile: ', jsonFile)\n",
        "  \n",
        "  # upload file to google earth engine from gcs\n",
        "  !earthengine upload image --asset_id={out_image_asset} {out_image_file} {jsonFile}\n",
        "\n",
        "  print('Prediction image upload task submitted. Check on the task in GEE for its status (https://code.earthengine.google.com/tasks).')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Run Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KxACnxKFrQ_J"
      },
      "outputs": [],
      "source": [
        "# Run the prediction.\n",
        "predictions = doPrediction(pred_image_base, pred_kernel_buffer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Write the predictions.\n",
        "writePredictions(predictions, pred_image_base, pred_kernel_buffer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# upload to earth engine\n",
        "uploadPredictionsToEE(pred_image_base, ee_user_folder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uj_G9OZ1xH6K"
      },
      "source": [
        "## Display the Prediction\n",
        "\n",
        "One the data has been exported, the model has made predictions and the predictions have been written to a file, and the image imported to Earth Engine, it's possible to display the resultant Earth Engine asset.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jgco6HJ4R5p2"
      },
      "outputs": [],
      "source": [
        "map = folium.Map(location=[-4.29, 15.27], zoom_start=11)\n",
        "\n",
        "# add landsat layer clipped to prediction region\n",
        "landsat = image\n",
        "landsat = landsat.clip(pred_region)\n",
        "mapid = landsat.getMapId({'bands': ['B4', 'B3', 'B2'], 'min': 0, 'max': 0.3})\n",
        "folium.TileLayer(\n",
        "    tiles=mapid['tile_fetcher'].url_format,\n",
        "    attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "    overlay=True,\n",
        "    name='landsat',\n",
        "  ).add_to(map)\n",
        "\n",
        "# actual viirs\n",
        "viirs = ee.ImageCollection('NOAA/VIIRS/DNB/MONTHLY_V1/VCMSLCFG').select('avg_rad').filterDate('2021-01-01', '2022-12-31').median().divide(30).float()\n",
        "viirs = viirs.clip(pred_region)\n",
        "# actual_image = viirs\n",
        "mapid = viirs.getMapId({'min': 0, 'max': 1}) # normally max 1.0\n",
        "folium.TileLayer(\n",
        "    tiles=mapid['tile_fetcher'].url_format,\n",
        "    attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "    overlay=True,\n",
        "    name='viirs dnb',\n",
        "  ).add_to(map)\n",
        "\n",
        "# prediction\n",
        "out_image = ee.Image(ee_user_folder + '/' + pred_image_base + ASSET_SUFFIX)\n",
        "mapid = out_image.getMapId({'min': 0, 'max': 1})\n",
        "folium.TileLayer(\n",
        "    tiles=mapid['tile_fetcher'].url_format,\n",
        "    attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "    overlay=True,\n",
        "    name='predicted nightlights',\n",
        "  ).add_to(map)\n",
        "\n",
        "\n",
        "# difference\n",
        "diff = out_image.subtract(viirs)\n",
        "mapid = diff.getMapId({'min': -0.5, 'max': 0.5, 'palette': ['FF0000', '000000', '00FF00']})\n",
        "folium.TileLayer(\n",
        "    tiles=mapid['tile_fetcher'].url_format,\n",
        "    attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "    overlay=True,\n",
        "    name='difference',\n",
        "  ).add_to(map)\n",
        "\n",
        "# visualize the map\n",
        "map.add_child(folium.LayerControl())\n",
        "map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Results\n",
        "\n",
        "The top performing model was the one trained on the small polygons around African cities for 20 epochs using Stochastic Gradient Descent, achieving an RMSE of 1.504. Surprisingly, increasing the samples in the training dataset by 10X did not improve on the results. Another surprise is that the train RMSE was consistently lower than the validation RMSE. This is highly atypical, and suggests that there is an issue with the training strategy. The full table of results is below.\n",
        "\n",
        "![results](./figs/results.png)\n",
        "\n",
        "On these RMSE by epoch graphs, we have a few interesting results. (1) shows the model training and validating in the USA, the only one where validation error followed a more regular trend. (2) shows the training with the Adam Optimizer, showing quite unstable training. (3) shows the stable training of one of the top performing models. However, it reaches quite close to the minimum in just about 15 epochs, and does not improve from there.\n",
        "\n",
        "Loss by Epoch for Small USA 50 Epochs |  Loss by Epoch for Small African Cities with Adam Optimizer | Loss by Epoch for Small African Cities with SGD Optimizer\n",
        ":-------------------------:|:-------------------------:|:-------------------------:\n",
        "![](./figs/loss1.png)  |  ![](./figs/loss2.png) |  ![](./figs/loss3.png)\n",
        "\n",
        "Finally, for each model, I ran a prediction pipeline for an untrained area, the twin cities of Kinshasa and Brazzaville between the Congo and the Democratic Republic of Congo for a qualitiative analysis. (1) shows the true color Landsat composite, (2) shows the VIIRS nightlights, (3) shows the predicted nightlights, and (4) shows the difference between (2) and (3). On this prediction, the model is picking up the features of urbanization in the cities, but it falsely classifies the island in the river delta as well lit while it is not populated. \n",
        "\n",
        "Input Landsat Image |  VIIRS Nightlights Label | Predicted Hi-Res Nightlights | Difference Image\n",
        ":-------------------------:|:-------------------------:|:-------------------------:|:-------------------------:\n",
        "![](./figs/pred1.png)      |  ![](./figs/pred2.png)    |  ![](./figs/pred3.png)   |  ![](./figs/pred4.png)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Conclusions\n",
        "\n",
        "In the pursuit of enhancing the accuracy and applicability of our night light prediction model, several key observations and avenues for future exploration have emerged. One notable finding revolves around the impact of training data size on model performance. Contrary to expectations, an increase in the size of the training data did not yield the anticipated improvement. This unexpected outcome prompts a deeper investigation into the relationship between training sample size, model parameters, batch size, learning rate, and the number of epochs. Unraveling this intricate interplay could offer valuable insights and potentially establish guidelines for optimizing model training.\n",
        "\n",
        "A second area of consideration is the resolution of the imagery used in the model. Although the results did not meet initial expectations, the opportunity to explore even higher resolution imagery, such as Sentinel-2 or Google Earth base maps, remains untapped. The potential gains from employing more detailed data sources could contribute significantly to refining the accuracy of our predictions, warranting further experimentation and analysis.\n",
        "\n",
        "Another noteworthy aspect pertains to the optimal number of epochs in training the model. Observations indicate that most models plateau well before reaching 50 epochs, suggesting a potential saturation point. This phenomenon is likely intertwined with the challenges posed by the training data size. Investigating the intricate dynamics among training data, model parameters, and epoch count is essential to discerning the optimal configuration for robust performance.\n",
        "\n",
        "Looking ahead, the goal extends beyond night light prediction alone. Once a model with satisfactory accuracy is achieved, the intention is to fine-tune it using ground surveys of household assets. This strategic refinement aims to enable a nuanced assessment of economic status through the lens of predicted night lights, reinforcing the socio-economic dimension of the research.\n",
        "\n",
        "As we chart the course for future work, a spectrum of possibilities emerges for both improving the current model's performance and exploring alternative models. Future endeavors may involve incorporating rural areas into the training and evaluation sets, exploring the impact of reducing samples per patch, and experimenting with image patch manipulations such as rotation and mirroring. Moreover, considerations extend to the redundancy in the current training data, prompting potential improvements through techniques like leave-one-out cross-validation and a closer examination of label distributions.\n",
        "\n",
        "Looking beyond incremental enhancements, future models could benefit from substituting Landsat 8 with Sentinel 2, reevaluating with Planet imagery, or exploring the application of Vision Transformers (ViTs). Additionally, the scope extends to examining wealth data rather than night lights, and investigating the influence of dataset size on model performance. These future directions collectively pave the way for a comprehensive and evolving exploration of the intersection between machine learning, satellite imagery, and socio-economic analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# References\n",
        "\n",
        "1. Landsat 8: https://www.usgs.gov/landsat-missions/landsat-8 \n",
        "1. Google Earth Engine: https://earthengine.google.com/\n",
        "1. Suomi NPP VIIRS Instrument: https://ncc.nesdis.noaa.gov/VIIRS/\n",
        "1. Using night light emissions for the prediction of local wealth: https://journals.sagepub.com/doi/full/10.1177/0022343316630359\n",
        "1. Using publicly available satellite imagery and deep learning to understand economic well-being in Africa: https://www.nature.com/articles/s41467-020-16185-w\n",
        "1. Fully Convolutional Networks for Semantic Segmentation: https://arxiv.org/abs/1411.4038\n",
        "1. U-Net: Convolutional Networks for Biomedical Image Segmentation: https://arxiv.org/abs/1505.04597\n",
        "1. VIIRS DNB Monthly Stray-Light Corrected Composite: https://eogdata.mines.edu/products/vnl/#monthly\n",
        "1. GEE Python API: https://developers.google.com/earth-engine/tutorials/community/intro-to-python-api\n",
        "1. Tensorflow: https://www.tensorflow.org/about/bib \n",
        "1. GitHub Code Repo: https://github.com/isaiahlg/csci5922/blob/main/proj/unet_regression_virrs.ipynb\n",
        "1. ESAs Copernicus Sentinel-2: https://sentinel.esa.int/web/sentinel/missions/sentinel-2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Appendix A: Export to HTML\n",
        "\n",
        "Below is a little code snippet that turns this Jupyter notebook into an HTML file to use as a webpage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# export to HTML for webpage\n",
        "import os\n",
        "\n",
        "# os.system('jupyter nbconvert --to html mod1.ipynb')\n",
        "os.system('jupyter nbconvert --to html unet_regression_virrs.ipynb --HTMLExporter.theme=dark')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Appendix B: To Do List\n",
        "This is a list I used to guide my work on this project, along with some future ideas for my own future reference!\n",
        "\n",
        "Decide a model to Use\n",
        "- [x] Figure out how to extract image patches for model training\n",
        "- [x] Extract Sentinel 2 and Planet Data\n",
        "- [x] Figure out which type of model to use\n",
        "- [x] Figure out how to run one of these scripts  \n",
        "\n",
        "Get a Simple Notebook Running\n",
        "- [x] Get the notebook running locally with the tfrecords saved locally  \n",
        "- [x] Train a model with small subset of the data  \n",
        "- [x] Get the model prediction and inference code to run with local files\n",
        "- [x] Get a simple version of the model trained\n",
        "- [x] Substitute NLCD with nightlights  \n",
        "- [x] Retrain a simple model with the new data  \n",
        "\n",
        "Get the Model Running on the Lab Computer\n",
        "- [x] Set up a python env on the lab computer  \n",
        "- [x] Set up SSH on the lab computer\n",
        "- [x] Adapt script to run on lab computer\n",
        "- [x] Run on lab computer  \n",
        "\n",
        "Scale Up the Model \n",
        "- [x] Visualize the model accuracy by epoch\n",
        "- [x] Compare a prediction to the actual image\n",
        "- [x] Create a difference image\n",
        "- [x] Scale up the model with original image patches\n",
        "\n",
        "Figure out why the model didn't train well..\n",
        "- [x] Resetup model to run with the small image patches - got to RMSE 0.16\n",
        "- [x] Try to set up with image patches in SSA - won't learn :/\n",
        "- [x] Try adjusting the optimizer\n",
        "- [x] Try with smaller image patches in SSA just arond cities\n",
        "- [x] Update Africa large to evaluate on small image patches\n",
        "- [x] Compare performance of large and small model\n",
        "- [ ] Try tripling the number of cities\n",
        "\n",
        "Generate Imagery and Figures:\n",
        "- [x] Run predictions on the validation sets as well\n",
        "- [x] Create a standardized test set\n",
        "- [x] Rvaluate the large model on the small patches to compare performance\n",
        "\n",
        "Future Ideas for Improving Model Performance\n",
        "- [ ] Try adding in rural areas for train and eval\n",
        "- [ ] Get fewer samples per patch to see if it still performs as well\n",
        "- [ ] Consider rotating and mirroring the image patches, easy in Pytorch\n",
        "- [ ] Probably a lot of redundancy right now in training data\n",
        "- [ ] Try leave one out cross validation\n",
        "- [ ] Plot distributions of labels\n",
        " \n",
        "\n",
        "Future Ideas for New Models:\n",
        "- [ ] Substitute Landsat 8 with Sentinel 2  \n",
        "- [ ] Consider running again with Planet  \n",
        "- [ ] Consider running again with a ViTs\n",
        "- [ ] Consider running again with \n",
        "- [ ] Look wealth instead of nightlights data  \n",
        "- [ ] Look at effect of dataset size on model performance\n",
        "\n",
        "Parameters Tested Out:\n",
        "- [ ] Number of epochs\n",
        "- [ ] Batch size\n",
        "- [ ] Learning rate\n",
        "- [ ] Optimizer\n",
        "- [ ] Loss function\n",
        "- [ ] Model architecture\n",
        "- [ ] Image patch size\n",
        "- [ ] Number of image patches per city\n",
        "- [ ] Number of cities\n",
        "- [ ] Location of cities"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
