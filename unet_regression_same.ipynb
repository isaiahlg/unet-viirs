{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Earth at Night from Day: Predicting Nighttime Radiance from Daytime Satellite Imagery\n",
        "Isaiah Lyons-Galante  \n",
        "University of Colorado Boulder\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SHAc5qbiR8l"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "The imprint of human activity on Earth has become unmistakable, offering a unique vantage point for population studies through the lens of space. Access to global high-resolution datasets, such as Landsat(1), has been made possible for the public through platforms like Google Earth Engine (3). These datasets open a window to observe and analyze human settlements, providing invaluable insights into the dynamics of our changing world. The image below is from Google Earth, showing the twin cities of Brazzaville and Kishasa on the banks of the Congo River. One can clearly see the densely package settlements on the banks, along with the unpopulated islands in the river.\n",
        "\n",
        "![brazzaville](./figs/brazzaville.png)\n",
        "\n",
        "During the night, human presence becomes even more apparent as settlements cast a faint glow into space. This nocturnal illumination is captured by the VIIRS instrument aboard the Suomi National Polar-orbiting Partnership (NPP) satellite. Despite its significance, the relatively weak signals limit the resolution to about 500 meters squared. This limitation poses a challenge to extracting fine-grained information about the distribution and characteristics of human settlements. The image below is of the same area as the one above, but at night. The resolution is significantly lower, with the pixels clearly visible, while the island in the river is completely dark.\n",
        "\n",
        "![brazzaville night](./figs/brazzavilleatnight.png)\n",
        "\n",
        "Compelling research underscores the correlation between night lights and mean household wealth, as measured by ground surveys. This correlation establishes night lights as a proxy for gauging livelihoods, making them a valuable tool for socio-economic analyses. This becomes particularly crucial in regions like Sub-Saharan Africa, where traditional population surveys are sparse. The scarcity of on-the-ground data highlights the potential of night lights as a surrogate measure for understanding the economic landscapes of communities. The graph below shows the relationship between night lights and mean household wealth in Sub-Saharan Africa. The data is from the Demographic and Health Surveys (DHS) Program, which conducts household surveys in developing countries. The graph shows a clear correlation between night lights and mean household wealth.\n",
        "\n",
        "![night lights and wealth](./figs/wealthcorrelations.jpg)\n",
        "\n",
        "The demand for higher-resolution night lights becomes evident when considering the need for detailed impact assessments. Such assessments can unravel the intricate ways in which individual communities are affected by economic development programs or global events. As technology advances, the quest for sharper and more detailed night light data gains significance, promising to enhance our understanding of human settlements and socio-economic patterns on a global scale. One example of this comes from a paper in Nature just last year that use wealth maps to assess the economic impact of access to electricity. High resolution nightlights would open up even finer grained analysis.\n",
        "\n",
        "![wealth map](./figs/WealthMap.png)\n",
        "\n",
        "Embarking on this exploration of human settlements and socio-economic patterns, I employ cutting-edge methodologies to surmount the resolution challenges posed by existing night light data. Leveraging the power of machine learning, my approach involves training a model to predict night lights from high-resolution daytime imagery. By undertaking this innovative process, I aim to transcend the limitations of the existing data and generate high-resolution nighttime imagery. This fusion of technological advancements with geospatial analysis not only promises to enrich our understanding of human settlements but also sets the stage for a more nuanced examination of the impact of economic development programs and global events on individual communities. This notebook will serve as a guide for the entire process, from method design to model training and evaluation, and finally, the results and conclusions. The code is available on [GitHub](https://github.com/isaiahlg/csci5922/blob/main/proj/unet_regression_virrs.ipynb).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Methods\n",
        "Because I are trying to predict one image from another, I can leverage a Fully Convolutional Network (6) with no dense layers. This allow us to use input imagery of any resolution. The U-Net (7) architecture, named for the shape of the diagram, is the most popular FCN and is well suited to our task. The output layer is modified to perform regression instead of classification so that I get a continuous radiance value instead of segment class. Below is a diagram of U-Net from the original paper -- notice the distintive U shape.\n",
        "\n",
        "![unet](./figs/unet.png)\n",
        "\n",
        "With the model in place, I then need to define our training data. In this project, four different training datasets were tested: both large training regions with 1000 image patch samples from each, as well as small regions with 100 image patch samples from each. Each of these were pulled from the United States and the African continent.  For each, there were 8 unique training regions, and 2 evaluation regions. For both areas, the polygons were centered around urban areas, as these have the greatest range of values in nighttime radiance. For consistency, the evaluation areas were also urban areas. Because the target is to predict nightlights in under-surveyed areas, the evaluation sites were Abuja, Nigeria, and Nairobi, Kenya. On the left, I have the large image patches from North America, and on the right from the African. Training patches are in yellow, and evaluation regions are in blue. The aim of having different continents as inputs was to test how transferrable the results are. Do well-lit areas in the US look like well-lit areas in Africa? \n",
        "\n",
        "American Cities             |  African Cities\n",
        ":-------------------------:|:-------------------------:\n",
        "![](./figs/usatrain.png)  |  ![](./figs/africatrain.png)\n",
        "\n",
        "\n",
        "For daytime imagery inputs, I pulled 9 bands from Landsat, 7 optical bands and 2 thermal bands. I created a cloud-masked image composite to create high-quality, cloud free input. For nightlight output, I also create a median composite of the average radiance band from stray-light corrected nightlights.8 The aim is that by pairing each daytime image patch of 256 x 256 with the corresponding nighttime image patch, I can predict nighttime image patches of 128x128 when the original resolution was about 16x16. This is almost 60X improvement in resolution.  Below on the left, I have a set of example input images from Landsat in false-color. The level of detail is significant compared with the coarse nightlight image patches on the right. \n",
        "\n",
        "Sample Training Inputs            |  Sample Training Labels\n",
        ":-------------------------:|:-------------------------:\n",
        "![](./figs/trainpatches.png)  |  ![](./figs/labelpatches.png)\n",
        "\n",
        "The other parameters that were experimented with were the number of epochs and the optimizer algorithm. For epochs, values between 1 and 50 were tried for most models. For the optimizer, a single experiment was run with one of the top performing models to see how it would compare. The imagery was compiled in Google Earth Engine via their Python API (9), exported to a Linux computer with a 48GB GPU. The model was built with tensorflow (10), code is available on GitHub (11).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MJ4kW1pEhwP"
      },
      "source": [
        "# Code for Implementing the Model\n",
        "\n",
        "Below are the chunks of code to gather the data. To run it yourself, copy and paste the .env.example file to .env and fill in your own file paths and Google Earth Engine credentials. You need an account on Google Cloud Platform with Storage Buckets and access to the Google Earth Engine API to run this, but it can be adapted to export to Google Drive instead before copying it locally."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configure the Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read in the .env file\n",
        "from dotenv import dotenv_values, load_dotenv\n",
        "\n",
        "# load env variables\n",
        "config = dotenv_values(\".env\")\n",
        "load_dotenv()\n",
        "\n",
        "# set env variables\n",
        "GCS_PROJECT = config['GCS_PROJECT']\n",
        "BUCKET = config['BUCKET']\n",
        "HOME_PATH = config['HOME_PATH']\n",
        "LOCAL_PATH = config['LOCAL_PATH']\n",
        "DATA_PATH = config['DATA_PATH']\n",
        "FOLDER = config['FOLDER']\n",
        "GCLOUD_PATH = config['GCLOUD_PATH']\n",
        "CUDA_VISIBLE_DEVICES = config['CUDA_VISIBLE_DEVICES']\n",
        "GPU_MEMORY_LIMIT = int(config['GPU_MEMORY_LIMIT'])\n",
        "GPU_MEMORY_CAPACITY = int(config['GPU_MEMORY_CAPACITY'])\n",
        "\n",
        "print(config)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import Key Libraries and Authenticate to Google Earth Engine\n",
        "\n",
        "We use Earth Engine to fetch, process, and display our imagery. We use tensorflow to build and train our model. Finally, we use folium to create interactive maps to display our results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jat01FEoUMqg"
      },
      "outputs": [],
      "source": [
        "# Import, authenticate and initialize the Earth Engine library.\n",
        "import ee\n",
        "ee.Authenticate()\n",
        "ee.Initialize()\n",
        "\n",
        "# Tensorflow setup.\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "# Folium setup.\n",
        "import folium\n",
        "print(folium.__version__)\n",
        "\n",
        "# supress warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmfKLl9XcnGJ"
      },
      "source": [
        "## Set other Global Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "psz7wJKalaoj"
      },
      "outputs": [],
      "source": [
        "# variables for image names\n",
        "TRAINING_BASE = 'training_patches'\n",
        "EVAL_BASE = 'eval_patches'\n",
        "\n",
        "# Specify inputs (Landsat bands) to the model and the response variable.\n",
        "opticalBands = ['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7']\n",
        "thermalBands = ['B10', 'B11']\n",
        "BANDS = opticalBands + thermalBands\n",
        "RESPONSE = 'avg_rad' ##### \n",
        "FEATURES = BANDS + [RESPONSE]\n",
        "\n",
        "# Specify the size and shape of patches expected by the model.\n",
        "KERNEL_SIZE = 256\n",
        "KERNEL_SHAPE = [KERNEL_SIZE, KERNEL_SIZE]\n",
        "COLUMNS = [\n",
        "  tf.io.FixedLenFeature(shape=KERNEL_SHAPE, dtype=tf.float32) for k in FEATURES\n",
        "]\n",
        "FEATURES_DICT = dict(zip(FEATURES, COLUMNS))\n",
        "\n",
        "# Sizes of the training and evaluation datasets.\n",
        "TRAIN_SIZE = 4800\n",
        "EVAL_SIZE = 200\n",
        "\n",
        "# Specify model training parameters.\n",
        "BATCH_SIZE = 48 ## TODO change to power of 2 (32)\n",
        "EPOCHS = 1\n",
        "BUFFER_SIZE = 2000\n",
        "OPTIMIZER = 'SGD' ## TODO: change to Adam\n",
        "LOSS = 'MeanSquaredError'\n",
        "METRICS = ['RootMeanSquaredError']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgoDc7Hilfc4"
      },
      "source": [
        "# Data Preparation\n",
        "\n",
        "Gather and setup the imagery to use for inputs (predictors).  This is a three-year, cloud-free, Landsat 8 composite.  Display it in the notebook for a sanity check."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-IlgXu-vcUEY"
      },
      "outputs": [],
      "source": [
        "# Use Landsat 8 surface reflectance data.\n",
        "l8sr = ee.ImageCollection('LANDSAT/LC08/C01/T1_SR')\n",
        "\n",
        "# Cloud masking function.\n",
        "def maskL8sr(image):\n",
        "  cloudShadowBitMask = ee.Number(2).pow(3).int()\n",
        "  cloudsBitMask = ee.Number(2).pow(5).int()\n",
        "  qa = image.select('pixel_qa')\n",
        "  mask1 = qa.bitwiseAnd(cloudShadowBitMask).eq(0).And(\n",
        "    qa.bitwiseAnd(cloudsBitMask).eq(0))\n",
        "  mask2 = image.mask().reduce('min')\n",
        "  mask3 = image.select(opticalBands).gt(0).And(\n",
        "          image.select(opticalBands).lt(10000)).reduce('min')\n",
        "  mask = mask1.And(mask2).And(mask3)\n",
        "  return image.select(opticalBands).divide(10000).addBands(\n",
        "          image.select(thermalBands).divide(10).clamp(273.15, 373.15)\n",
        "            .subtract(273.15).divide(100)).updateMask(mask)\n",
        "\n",
        "# The image input data is a cloud-masked median composite.\n",
        "image = l8sr.filterDate('2020-01-01', '2022-12-31').map(maskL8sr).median()\n",
        "\n",
        "# printn out the bands\n",
        "print(image.bandNames().getInfo())\n",
        "\n",
        "# Use folium to visualize the imagery.\n",
        "mapid = image.getMapId({'bands': ['B4', 'B3', 'B2'], 'min': 0, 'max': 0.3})\n",
        "map = folium.Map(location=[-1, 37], zoom_start=6)\n",
        "folium.TileLayer(\n",
        "    tiles=mapid['tile_fetcher'].url_format,\n",
        "    attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "    overlay=True,\n",
        "    name='median composite',\n",
        "  ).add_to(map)\n",
        "\n",
        "mapid = image.getMapId({'bands': ['B10'], 'min': 0, 'max': 0.5})\n",
        "folium.TileLayer(\n",
        "    tiles=mapid['tile_fetcher'].url_format,\n",
        "    attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "    overlay=True,\n",
        "    name='thermal',\n",
        "  ).add_to(map)\n",
        "map.add_child(folium.LayerControl())\n",
        "map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHznnctkJsZJ"
      },
      "source": [
        "Prepare the response (what we want to predict).  This is nighttime brightness (in nanoWatts/sr/cm^2) from the VIIRS dataset.  Display to check."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0wHDyxVirec"
      },
      "outputs": [],
      "source": [
        "viirs = ee.ImageCollection('NOAA/VIIRS/DNB/MONTHLY_V1/VCMSLCFG').select('avg_rad').filterDate('2020-01-01', '2022-12-31').median()\n",
        "\n",
        "mapid = viirs.getMapId({'min': 0, 'max': 1}) \n",
        "map = folium.Map(location=[-1, 37], zoom_start=6)\n",
        "folium.TileLayer(\n",
        "    tiles=mapid['tile_fetcher'].url_format,\n",
        "    attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "    overlay=True,\n",
        "    name='viirs dnb',\n",
        "  ).add_to(map)\n",
        "map.add_child(folium.LayerControl())\n",
        "map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTS7_ZzPDhhg"
      },
      "source": [
        "Stack the 2D images (Landsat composite and VIIRS DNB image) to create a single image from which samples can be taken.  Convert the image into an array image in which each pixel stores 256x256 patches of pixels for each band.  This is a key step that bears emphasis: to export training patches, convert a multi-band image to [an array image](https://developers.google.com/earth-engine/arrays_array_images#array-images) using [`neighborhoodToArray()`](https://developers.google.com/earth-engine/api_docs#eeimageneighborhoodtoarray), then sample the image at points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eGHYsdAOipa4"
      },
      "outputs": [],
      "source": [
        "featureStack = ee.Image.cat([\n",
        "  image.select(BANDS),\n",
        "  viirs.select(RESPONSE)\n",
        "]).float()\n",
        "\n",
        "list = ee.List.repeat(1, KERNEL_SIZE)\n",
        "lists = ee.List.repeat(list, KERNEL_SIZE)\n",
        "kernel = ee.Kernel.fixed(KERNEL_SIZE, KERNEL_SIZE, lists)\n",
        "\n",
        "arrays = featureStack.neighborhoodToArray(kernel)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4djSxBRG2el"
      },
      "source": [
        "# Define Training and Evaluation Areas\n",
        "Use some pre-made geometries to sample the stack in strategic locations.  Specifically, these are hand-made polygons in which to take the 256x256 samples.  Display the sampling polygons on a map, red for training polygons, blue for evaluation. You'll need to create these polygons ahead of time yourself and save them as a feature collection. This is easiest to do in Google Earth Engine's code editor, but can also be done via the API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ure_WaD0itQY"
      },
      "outputs": [],
      "source": [
        "trainingPolys = ee.FeatureCollection('projects/'+GCS_PROJECT+'/assets/trainingPolysAfricaBig')\n",
        "evalPolys = ee.FeatureCollection('projects/'+GCS_PROJECT+'/assets/evalPolysAfrica')\n",
        "\n",
        "polyImage = ee.Image(0).byte().paint(trainingPolys, 1).paint(evalPolys, 2)\n",
        "polyImage = polyImage.updateMask(polyImage)\n",
        "\n",
        "mapid = polyImage.getMapId({'min': 1, 'max': 2, 'palette': ['gold', 'blue']})\n",
        "map = folium.Map(location=[0, 12], zoom_start=5)\n",
        "folium.TileLayer(\n",
        "    tiles=mapid['tile_fetcher'].url_format,\n",
        "    attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "    overlay=True,\n",
        "    name='training polygons',\n",
        "  ).add_to(map)\n",
        "map.add_child(folium.LayerControl())\n",
        "map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZV890gPHeZqz"
      },
      "source": [
        "# Sampling\n",
        "\n",
        "The mapped data look reasonable so take a sample from each polygon and merge the results into a single export.  The key step is sampling the array image at points, to get all the pixels in a 256x256 neighborhood at each point.  It's worth noting that to build the training and testing data for the FCNN, you export a single TFRecord file that contains patches of pixel values in each record.  You do not need to export each training/testing patch to a different image.  Since each record potentially contains a lot of data (especially with big patches or many input bands), some manual sharding of the computation is necessary to avoid the `computed value too large` error.  Specifically, the following code takes multiple (smaller) samples within each geometry, merging the results to get a single export."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "both",
        "id": "FyRpvwENxE-A"
      },
      "outputs": [],
      "source": [
        "# Convert the feature collections to lists for iteration.\n",
        "trainingPolysList = trainingPolys.toList(trainingPolys.size())\n",
        "evalPolysList = evalPolys.toList(evalPolys.size())\n",
        "\n",
        "# These numbers determined experimentally.\n",
        "n = 20 # Number of shards in each polygon. \n",
        "N = 200 # Total sample size in each polygon.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Export Training & Evaluation Data from Earth Engine to Google Cloud Storage\n",
        "This can be modified to export to local storage, just adjust the file paths."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Export all the training data (in many pieces), with one task\n",
        "# per geometry.\n",
        "for g in range(trainingPolys.size().getInfo()):\n",
        "  geomSample = ee.FeatureCollection([])\n",
        "  for i in range(n):\n",
        "    sample = arrays.sample(\n",
        "      region = ee.Feature(trainingPolysList.get(g)).geometry(),\n",
        "      scale = 30,\n",
        "      numPixels = N / n, # Size of the shard.\n",
        "      seed = i,\n",
        "      tileScale = 8\n",
        "    )\n",
        "    geomSample = geomSample.merge(sample)\n",
        "  \n",
        "  desc = TRAINING_BASE + '_g' + str(g)\n",
        "  print(g, FOLDER, BUCKET, desc)\n",
        "  \n",
        "  task = ee.batch.Export.table.toCloudStorage(\n",
        "    collection = geomSample,\n",
        "    description = desc,\n",
        "    bucket = BUCKET,\n",
        "    fileNamePrefix = FOLDER + desc,\n",
        "    fileFormat = 'TFRecord',\n",
        "    selectors = BANDS + [RESPONSE]\n",
        "  )\n",
        "  task.start()\n",
        "print('Training imagery tasks submitted. Check on the Tasks in GEE for their status (https://code.earthengine.google.com/tasks).')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Export all the evaluation data.\n",
        "for g in range(evalPolys.size().getInfo()):\n",
        "  geomSample = ee.FeatureCollection([])\n",
        "  for i in range(n):\n",
        "    sample = arrays.sample(\n",
        "      region = ee.Feature(evalPolysList.get(g)).geometry(),\n",
        "      scale = 30,\n",
        "      numPixels = N / n,\n",
        "      seed = i,\n",
        "      tileScale = 8\n",
        "    )\n",
        "    geomSample = geomSample.merge(sample)\n",
        "\n",
        "  desc = EVAL_BASE + '_g' + str(g)\n",
        "  print(g, FOLDER, BUCKET, desc)\n",
        "  \n",
        "  task = ee.batch.Export.table.toCloudStorage(\n",
        "    collection = geomSample,\n",
        "    description = desc,\n",
        "    bucket = BUCKET,\n",
        "    fileNamePrefix = FOLDER + desc,\n",
        "    fileFormat = 'TFRecord',\n",
        "    selectors = BANDS + [RESPONSE]\n",
        "  )\n",
        "  task.start()\n",
        "print('Eval imagery tasks submitted. Check on the Tasks in GEE for their status (https://code.earthengine.google.com/tasks).')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Copy Files from Google Cloud Storage to Local Storage\n",
        "This moves the files to the local machine for training. Note that you need the Google Cloud SDK installed and authenticated to run this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Copy from your bucket to local path (note -r is for recursive call)\n",
        "!{GCLOUD_PATH}gsutil -m cp -r 'gs://'{BUCKET}'/'{FOLDER}{TRAINING_BASE}'*' {LOCAL_PATH}{DATA_PATH}{FOLDER}\n",
        "!{GCLOUD_PATH}gsutil -m cp -r 'gs://'{BUCKET}'/'{FOLDER}{EVAL_BASE}'*' {LOCAL_PATH}{DATA_PATH}{FOLDER}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Configure GPU Usage\n",
        "Use this to ensure you are using the GPUs that you intend for training. This assumes you have CUDA and cuDNN installed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# see what devices are available\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
        "print(\"GPUs Available: \", tf.config.list_physical_devices('GPU'))\n",
        "\n",
        "# Create a TensorFlow GPU configuration\n",
        "gpu_options = tf.compat.v1.GPUOptions(\n",
        "  per_process_gpu_memory_fraction=GPU_MEMORY_LIMIT / GPU_MEMORY_CAPACITY,\n",
        ")\n",
        "\n",
        "# Limit GPU memory allocation for TensorFlow\n",
        "config = tf.compat.v1.ConfigProto(\n",
        "  gpu_options=gpu_options\n",
        ")\n",
        "\n",
        "# Create a TensorFlow session with the configured GPU options\n",
        "sess = tf.compat.v1.Session(config=config)\n",
        "\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWXrvBE4607G"
      },
      "source": [
        "# Load Training and Evaluation Data in Memory for Training.\n",
        "\n",
        "Load the data exported from Earth Engine into a `tf.data.Dataset`.  The following are helper functions for that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WWZ0UXCVMyJP"
      },
      "outputs": [],
      "source": [
        "def parse_tfrecord(example_proto):\n",
        "  return tf.io.parse_single_example(example_proto, FEATURES_DICT)\n",
        "\n",
        "\n",
        "def to_tuple(inputs):\n",
        "  inputsList = [inputs.get(key) for key in FEATURES]\n",
        "  stacked = tf.stack(inputsList, axis=0)\n",
        "  # Convert from CHW to HWC\n",
        "  stacked = tf.transpose(stacked, [1, 2, 0])\n",
        "  return stacked[:,:,:len(BANDS)], stacked[:,:,len(BANDS):]\n",
        "\n",
        "\n",
        "def get_dataset(path, prefix, n_files):\n",
        "  glob = []\n",
        "  for i in range(0, n_files):\n",
        "    glob = path + prefix + '_g' + str(i) + '.tfrecord.gz'\n",
        "  dataset = tf.data.TFRecordDataset(glob, compression_type='GZIP')\n",
        "  dataset = dataset.map(parse_tfrecord, num_parallel_calls=5)\n",
        "  dataset = dataset.map(to_tuple, num_parallel_calls=5)\n",
        "  return dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xg1fa18336D2"
      },
      "source": [
        "Use the helpers to read in the training dataset.  Optionally print the first record to check. Careful, this will make tensorflow reserve as much GPU memory as is available to it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rm0qRF0fAYcC"
      },
      "outputs": [],
      "source": [
        "training = get_dataset(LOCAL_PATH + DATA_PATH + FOLDER, TRAINING_BASE, 24)\n",
        "training = training.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
        "# print(iter(training.take(1)).next())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-cQO5RL6vob"
      },
      "source": [
        "# Evaluation data\n",
        "\n",
        "Now do the same thing to get an evaluation dataset.  Note that unlike the training dataset, the evaluation dataset has a batch size of 1, is not repeated and is not shuffled."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ieKTCGiJ6xzo"
      },
      "outputs": [],
      "source": [
        "evaluation = get_dataset(LOCAL_PATH + DATA_PATH + FOLDER, EVAL_BASE, 2)\n",
        "evaluation = evaluation.batch(1).repeat()\n",
        "# print(iter(evaluation.take(1)).next()[0][0,:,:,1:3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# visualize of the of the training / evaluation image patch pairs\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def display_batch_of_images(databatch):\n",
        "    imgs, labels = databatch\n",
        "    # landsat in false color\n",
        "    plt.figure(figsize=(10,10))\n",
        "    label = labels.numpy().astype(np.float32)\n",
        "    for i in range(25):\n",
        "        plt.subplot(5,5,i+1)\n",
        "        plt.imshow(imgs[i,:,:,0:3]*8) # select landsat bands here\n",
        "        plt.axis('off')\n",
        "    plt.show()\n",
        "    # show viirs in grayscale\n",
        "    plt.figure(figsize=(10,10))\n",
        "    for i in range(25):\n",
        "        plt.subplot(5,5,i+1)\n",
        "        plt.imshow(labels[i,:,:,0], cmap='gray')\n",
        "        plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "display_batch_of_images(iter(training.take(1)).next())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JIE7Yl87lgU"
      },
      "source": [
        "# Data Analysis & Deep Learning Model\n",
        "\n",
        "Here we use the Keras implementation of the U-Net model.  The U-Net model takes 256x256 pixel patches as input and outputs per-pixel class probability, label or a continuous output.  We can implement the model essentially unmodified, but will use mean squared error loss on the sigmoidal output since we are treating this as a regression problem, rather than a classification problem. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wsnnnz56yS3l"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "from keras import losses\n",
        "from keras import models\n",
        "from keras import metrics\n",
        "from keras import optimizers\n",
        "\n",
        "def conv_block(input_tensor, num_filters):\n",
        "\tencoder = layers.Conv2D(num_filters, (3, 3), padding='same')(input_tensor)\n",
        "\tencoder = layers.BatchNormalization()(encoder)\n",
        "\tencoder = layers.Activation('relu')(encoder)\n",
        "\tencoder = layers.Conv2D(num_filters, (3, 3), padding='same')(encoder)\n",
        "\tencoder = layers.BatchNormalization()(encoder)\n",
        "\tencoder = layers.Activation('relu')(encoder)\n",
        "\treturn encoder\n",
        "\n",
        "def encoder_block(input_tensor, num_filters):\n",
        "\tencoder = conv_block(input_tensor, num_filters)\n",
        "\tencoder_pool = layers.MaxPooling2D((2, 2), strides=(2, 2))(encoder)\n",
        "\treturn encoder_pool, encoder\n",
        "\n",
        "\n",
        "def decoder_block(input_tensor, concat_tensor, num_filters):\n",
        "\tdecoder = layers.Conv2DTranspose(num_filters, (2, 2), strides=(2, 2), padding='same')(input_tensor)\n",
        "\tdecoder = layers.concatenate([concat_tensor, decoder], axis=-1)\n",
        "\tdecoder = layers.BatchNormalization()(decoder)\n",
        "\tdecoder = layers.Activation('relu')(decoder)\n",
        "\tdecoder = layers.Conv2D(num_filters, (3, 3), padding='same')(decoder)\n",
        "\tdecoder = layers.BatchNormalization()(decoder)\n",
        "\tdecoder = layers.Activation('relu')(decoder)\n",
        "\tdecoder = layers.Conv2D(num_filters, (3, 3), padding='same')(decoder)\n",
        "\tdecoder = layers.BatchNormalization()(decoder)\n",
        "\tdecoder = layers.Activation('relu')(decoder)\n",
        "\treturn decoder\n",
        "\n",
        "def get_model():\n",
        "\tinputs = layers.Input(shape=[None, None, len(BANDS)]) # 256\n",
        "\tencoder0_pool, encoder0 = encoder_block(inputs, 32) # 128\n",
        "\tencoder1_pool, encoder1 = encoder_block(encoder0_pool, 64) # 64\n",
        "\tencoder2_pool, encoder2 = encoder_block(encoder1_pool, 128) # 32\n",
        "\tencoder3_pool, encoder3 = encoder_block(encoder2_pool, 256) # 16\n",
        "\tencoder4_pool, encoder4 = encoder_block(encoder3_pool, 512) # 8\n",
        "\tcenter = conv_block(encoder4_pool, 1024) # center\n",
        "\tdecoder4 = decoder_block(center, encoder4, 512) # 16\n",
        "\tdecoder3 = decoder_block(decoder4, encoder3, 256) # 32\n",
        "\tdecoder2 = decoder_block(decoder3, encoder2, 128) # 64\n",
        "\tdecoder1 = decoder_block(decoder2, encoder1, 64) # 128\n",
        "\tdecoder0 = decoder_block(decoder1, encoder0, 32) # 256\n",
        "\n",
        "\t# to change your output activation function, change this part\n",
        "\toutputs = layers.Conv2D(1, (1, 1), activation='relu')(decoder0)\n",
        "\n",
        "\tmodel = models.Model(inputs=[inputs], outputs=[outputs])\n",
        "\t\n",
        "\t# set optimizer with custom learning rate\n",
        "\tOPTIMIZER = 'Adam'\n",
        "\toptimizer = optimizers.get(OPTIMIZER)\n",
        "\t\n",
        "\t# to adjust the learning rate:\n",
        "\t# optimizer.learning_rate = 0.1\n",
        "\n",
        "\t# compile the model\n",
        "\tmodel.compile(\n",
        "\t\toptimizer=optimizer,\n",
        "\t\tloss=losses.get(LOSS),\n",
        "\t\tmetrics=[metrics.get(metric) for metric in METRICS])\n",
        "\n",
        "\treturn model\n",
        "\n",
        "# get the model fresh\n",
        "m = get_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uu_E7OTDBCoS"
      },
      "source": [
        "# Training the model\n",
        "\n",
        "You train a Keras model by calling `.fit()` on it.  Here we're going to train for 10 epochs, which is suitable for demonstration purposes.  For production use, you probably want to optimize this parameter, for example through [hyperparamter tuning](https://cloud.google.com/ml-engine/docs/tensorflow/using-hyperparameter-tuning)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "EPOCHS = 0\n",
        "\n",
        "# print the current time to log how long it takes to train\n",
        "from datetime import datetime\n",
        "print(datetime.now())\n",
        "\n",
        "history = m.fit(\n",
        "    x=training,\n",
        "    epochs=EPOCHS,\n",
        "    steps_per_epoch=int(TRAIN_SIZE / BATCH_SIZE),\n",
        "    validation_data=evaluation,\n",
        "    validation_steps=EVAL_SIZE\n",
        ")\n",
        "\n",
        "# print the end time to log how long it takes to train\n",
        "print(datetime.now())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# set some parameters for saving the model\n",
        "MODEL_NAME = 'viirs_africa_many_relu_adam_e0.keras'\n",
        "HISTORY_NAME = 'viirs_africa_many_relu_adam_e0_history.pickle'\n",
        "ASSET_SUFFIX = 'viirs_africa_many_relu_adam_e0'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# save the model to a local file \n",
        "m.save(LOCAL_PATH + DATA_PATH + FOLDER + MODEL_NAME)\n",
        "\n",
        "# # save the history variable to a local file with pickle\n",
        "import pickle\n",
        "with open(LOCAL_PATH + DATA_PATH + FOLDER + HISTORY_NAME, 'wb') as f:\n",
        "  pickle.dump(history, f)\n",
        "\n",
        "# import a saved model if needed\n",
        "# m = tf.keras.models.load_model(LOCAL_PATH + DATA_PATH + FOLDER + MODEL_NAME)\n",
        "# import pickle\n",
        "# with open(HISTORY_NAME, 'rb') as f:\n",
        "# with open(LOCAL_PATH + DATA_PATH + FOLDER + HISTORY_NAME, 'rb') as f:\n",
        "#   history = pickle.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluation\n",
        "Let's look at how the model performed in terms of loss while it trained, and then evaluate it on the training and evaluation datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# plot the training and validation loss\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history['root_mean_squared_error'])\n",
        "plt.plot(history.history['val_root_mean_squared_error'])\n",
        "plt.title('Model RMSE for ' + ASSET_SUFFIX + ' for ' + str(EPOCHS) + ' epochs')\n",
        "plt.ylabel('RMSE')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Training RMSE', 'Validation RMSE'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## evaluate the model on the evaluation dataset to get the loss and RMSE\n",
        "val_loss, val_rmse = m.evaluate(evaluation, steps=EVAL_SIZE)\n",
        "print('Model validation loss: {:0.4f}. RMSE: {:0.4f}'.format(val_loss, val_rmse))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## evaluate the model on the evaluation dataset to get the loss and RMSE\n",
        "train_loss, train_rmse = m.evaluate(training, steps=10)\n",
        "print('Model train loss: {:0.4f}. RMSE: {:0.4f}'.format(train_loss, train_rmse))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1ySNup0xCqN"
      },
      "source": [
        "# Prediction\n",
        "\n",
        "Now we're going to do a qualitative assessment of the model by running it on an input image patch, specifically Brazzaville / Kinshasa. The prediction pipeline is:\n",
        "\n",
        "1.  Export imagery on which to do predictions from Earth Engine in TFRecord format to a Cloud Storage bucket.\n",
        "2.  Use the trained model to make the predictions.\n",
        "3.  Write the predictions to a TFRecord file in a Cloud Storage.\n",
        "4.  Upload the predictions TFRecord file to Earth Engine.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Specify Region and Outputs for Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FPANwc7B1-TS"
      },
      "outputs": [],
      "source": [
        "ee_user_folder = 'projects/'+GCS_PROJECT+'/assets'\n",
        "\n",
        "pred_image_base = 'FCNN_demo_kinshasa_384_'\n",
        "# Half this will extend on the sides of each patch.\n",
        "pred_kernel_buffer = [128, 128]\n",
        "# Kinshasa/Brazzaville\n",
        "pred_region = ee.Geometry.Polygon(\n",
        "  [[[15.02, -4.04],\n",
        "    [15.02, -4.54],\n",
        "    [15.52, -4.54],\n",
        "    [15.52, -4.04]]], \n",
        "  None, \n",
        "  False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Export Sample Imagery for Prediction\n",
        "This only needs to be run once.  It exports the sample imagery to a TFRecord file in a Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M3WDAa-RUpXP"
      },
      "outputs": [],
      "source": [
        "def doExport(out_image_base, kernel_buffer, region):\n",
        "  task = ee.batch.Export.image.toCloudStorage(\n",
        "    image = image.select(BANDS),\n",
        "    description = out_image_base,\n",
        "    bucket = BUCKET,\n",
        "    fileNamePrefix = FOLDER + out_image_base,\n",
        "    region = region.getInfo()['coordinates'],\n",
        "    scale = 30,\n",
        "    fileFormat = 'TFRecord',\n",
        "    maxPixels = 1e10,\n",
        "    formatOptions = {\n",
        "      'patchDimensions': KERNEL_SHAPE,\n",
        "      'kernelSize': kernel_buffer,\n",
        "      'compressed': True,\n",
        "      'maxFileSize': 104857600\n",
        "    }\n",
        "  )\n",
        "  task.start()\n",
        "  print('Prediction image export task submitted. Check on the task in GEE for its status (https://code.earthengine.google.com/tasks).')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lLNEOLkXWvSi"
      },
      "outputs": [],
      "source": [
        "# Run the export.\n",
        "doExport(pred_image_base, pred_kernel_buffer, pred_region)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# copy the prediction file from cloud storage to local\n",
        "!{GCLOUD_PATH}gsutil -m cp -r 'gs://'{BUCKET}'/'{FOLDER}{pred_image_base}'*' {LOCAL_PATH}{DATA_PATH}{FOLDER}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define Code for Running Model on Prediction Data\n",
        "Expects the prediction data to exist locally. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zb_9_FflygVw"
      },
      "outputs": [],
      "source": [
        "def doPrediction(out_image_base, kernel_buffer):\n",
        "  print('Looking for TFRecord files...')\n",
        "\n",
        "  # # Get a list of all the files in the output bucket.\n",
        "  filesList = !ls {LOCAL_PATH}{DATA_PATH}{FOLDER}\n",
        "\n",
        "  # Get only the files generated by the image export.\n",
        "  exportFilesList = [s for s in filesList if out_image_base in s]\n",
        "\n",
        "  # Get the list of image files and the JSON mixer file.\n",
        "  imageFilesList = []\n",
        "  jsonFile = None\n",
        "  for f in exportFilesList:\n",
        "    if f.endswith('.tfrecord.gz'):\n",
        "      imageFilesList.append(f)\n",
        "    elif f.endswith('.json'):\n",
        "      jsonFile = f\n",
        "  imageFilesList.sort()\n",
        "\n",
        "  from pprint import pprint\n",
        "  pprint(imageFilesList)\n",
        "  print(jsonFile)\n",
        "\n",
        "  import json\n",
        "  jsonText = !cat {LOCAL_PATH}{DATA_PATH}{FOLDER}{jsonFile}\n",
        "  print(jsonText)\n",
        "  mixer = json.loads(jsonText.nlstr)\n",
        "  pprint(mixer)\n",
        "  patches = mixer['totalPatches']\n",
        "\n",
        "  buffered_shape = [\n",
        "      KERNEL_SHAPE[0] + kernel_buffer[0],\n",
        "      KERNEL_SHAPE[1] + kernel_buffer[1]]\n",
        "\n",
        "  imageColumns = [\n",
        "    tf.io.FixedLenFeature(shape=buffered_shape, dtype=tf.float32)\n",
        "      for k in BANDS\n",
        "  ]\n",
        "\n",
        "  imageFeaturesDict = dict(zip(BANDS, imageColumns))\n",
        "\n",
        "  def parse_image(example_proto):\n",
        "    return tf.io.parse_single_example(example_proto, imageFeaturesDict)\n",
        "\n",
        "  def toTupleImage(inputs):\n",
        "    inputsList = [inputs.get(key) for key in BANDS]\n",
        "    stacked = tf.stack(inputsList, axis=0)\n",
        "    stacked = tf.transpose(stacked, [1, 2, 0])\n",
        "    return stacked\n",
        "\n",
        "  # append absolute path to imageFilesList\n",
        "  imageFilesList = [LOCAL_PATH + DATA_PATH + FOLDER + f for f in imageFilesList]\n",
        "  print(imageFilesList)\n",
        "  \n",
        "  # Create a dataset from the TFRecord file(s) in Cloud Storage.\n",
        "  imageDataset = tf.data.TFRecordDataset(imageFilesList, compression_type='GZIP')\n",
        "  imageDataset = imageDataset.map(parse_image, num_parallel_calls=5)\n",
        "  imageDataset = imageDataset.map(toTupleImage).batch(1)\n",
        "\n",
        "  # Perform inference.\n",
        "  print('Running predictions...')\n",
        "  predictions = m.predict(imageDataset, steps=patches, verbose=1)\n",
        "  # print(predictions[0])\n",
        "  return predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def writePredictions(predictions, out_image_base, kernel_buffer):\n",
        "  print('Writing predictions...')\n",
        "  out_image_file = 'gs://' + BUCKET + '/' + FOLDER + out_image_base + '.TFRecord'\n",
        "\n",
        "  # Get set up for prediction.\n",
        "  x_buffer = int(kernel_buffer[0] / 2)\n",
        "  y_buffer = int(kernel_buffer[1] / 2)\n",
        "  \n",
        "  \n",
        "  writer = tf.io.TFRecordWriter(out_image_file)\n",
        "  patches = 0\n",
        "  for predictionPatch in predictions:\n",
        "    # print('Writing patch ' + str(patches) + '...')\n",
        "    predictionPatch = predictionPatch[\n",
        "        x_buffer:x_buffer+KERNEL_SIZE, y_buffer:y_buffer+KERNEL_SIZE]\n",
        "\n",
        "    # Create an example.\n",
        "    example = tf.train.Example(\n",
        "      features=tf.train.Features(\n",
        "        feature={\n",
        "          'impervious': tf.train.Feature(\n",
        "            float_list=tf.train.FloatList(\n",
        "              value=predictionPatch.flatten()\n",
        "            )\n",
        "          )\n",
        "        }\n",
        "      )\n",
        "    )\n",
        "    # Write the example.\n",
        "    writer.write(example.SerializeToString())\n",
        "    patches += 1\n",
        "\n",
        "  print('Done writing predictions.')\n",
        "  writer.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def uploadPredictionsToEE(out_image_base, ee_user_folder):\n",
        "  # Start the upload.\n",
        "  print('Uploading to Google Earth Egnine...')\n",
        "  \n",
        "  # define file paths\n",
        "  out_image_file = 'gs://' + BUCKET + '/' + FOLDER + out_image_base + '.TFRecord'\n",
        "  out_image_asset = ee_user_folder + '/' + out_image_base + ASSET_SUFFIX\n",
        "  jsonFile = 'gs://' + BUCKET + '/' + FOLDER + out_image_base + 'mixer.json'\n",
        "  \n",
        "  print('out_image_asset: ', out_image_asset)\n",
        "  print('out_image_file: ', out_image_file)\n",
        "  print('jsonFile: ', jsonFile)\n",
        "  \n",
        "  # upload file to google earth engine from gcs\n",
        "  !earthengine upload image --asset_id={out_image_asset} {out_image_file} {jsonFile}\n",
        "\n",
        "  print('Prediction image upload task submitted. Check on the task in GEE for its status (https://code.earthengine.google.com/tasks).')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Run Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KxACnxKFrQ_J"
      },
      "outputs": [],
      "source": [
        "# Run the prediction.\n",
        "predictions = doPrediction(pred_image_base, pred_kernel_buffer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Write the predictions.\n",
        "writePredictions(predictions, pred_image_base, pred_kernel_buffer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# upload to earth engine\n",
        "uploadPredictionsToEE(pred_image_base, ee_user_folder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uj_G9OZ1xH6K"
      },
      "source": [
        "## Display the Prediction\n",
        "\n",
        "One the data has been exported, the model has made predictions and the predictions have been written to a file, and the image imported to Earth Engine, it's possible to display the resultant Earth Engine asset.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jgco6HJ4R5p2"
      },
      "outputs": [],
      "source": [
        "map = folium.Map(location=[-4.29, 15.27], zoom_start=11)\n",
        "\n",
        "# add landsat layer clipped to prediction region\n",
        "landsat = image\n",
        "landsat = landsat.clip(pred_region)\n",
        "mapid = landsat.getMapId({'bands': ['B4', 'B3', 'B2'], 'min': 0, 'max': 0.3})\n",
        "folium.TileLayer(\n",
        "    tiles=mapid['tile_fetcher'].url_format,\n",
        "    attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "    overlay=True,\n",
        "    name='landsat',\n",
        "  ).add_to(map)\n",
        "\n",
        "# actual viirs\n",
        "viirs = ee.ImageCollection('NOAA/VIIRS/DNB/MONTHLY_V1/VCMSLCFG').select('avg_rad').filterDate('2021-01-01', '2022-12-31').median().divide(30).float()\n",
        "viirs = viirs.clip(pred_region)\n",
        "# actual_image = viirs\n",
        "mapid = viirs.getMapId({'min': 0, 'max': 1}) # normally max 1.0\n",
        "folium.TileLayer(\n",
        "    tiles=mapid['tile_fetcher'].url_format,\n",
        "    attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "    overlay=True,\n",
        "    name='viirs dnb',\n",
        "  ).add_to(map)\n",
        "\n",
        "# prediction\n",
        "out_image = ee.Image(ee_user_folder + '/' + pred_image_base + ASSET_SUFFIX)\n",
        "mapid = out_image.getMapId({'min': 0, 'max': 1})\n",
        "folium.TileLayer(\n",
        "    tiles=mapid['tile_fetcher'].url_format,\n",
        "    attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "    overlay=True,\n",
        "    name='predicted nightlights',\n",
        "  ).add_to(map)\n",
        "\n",
        "\n",
        "# difference\n",
        "diff = out_image.subtract(viirs)\n",
        "mapid = diff.getMapId({'min': -1, 'max': 1, 'palette': ['FF0000', '000000', '00FF00']})\n",
        "folium.TileLayer(\n",
        "    tiles=mapid['tile_fetcher'].url_format,\n",
        "    attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "    overlay=True,\n",
        "    name='difference',\n",
        "  ).add_to(map)\n",
        "\n",
        "# visualize the map\n",
        "map.add_child(folium.LayerControl())\n",
        "map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Results\n",
        " \n",
        "Run results are being recorded in Google Sheets [here](https://docs.google.com/spreadsheets/d/1n0RufjQcEEjNUUc_Lp2-_LRLIZxZ00QmgIZ8ZHBVPE4/edit?usp=sharing). \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# References\n",
        "\n",
        "1. Landsat 8: https://www.usgs.gov/landsat-missions/landsat-8 \n",
        "1. Google Earth Engine: https://earthengine.google.com/\n",
        "1. Suomi NPP VIIRS Instrument: https://ncc.nesdis.noaa.gov/VIIRS/\n",
        "1. Using night light emissions for the prediction of local wealth: https://journals.sagepub.com/doi/full/10.1177/0022343316630359\n",
        "1. Using publicly available satellite imagery and deep learning to understand economic well-being in Africa: https://www.nature.com/articles/s41467-020-16185-w\n",
        "1. Fully Convolutional Networks for Semantic Segmentation: https://arxiv.org/abs/1411.4038\n",
        "1. U-Net: Convolutional Networks for Biomedical Image Segmentation: https://arxiv.org/abs/1505.04597\n",
        "1. VIIRS DNB Monthly Stray-Light Corrected Composite: https://eogdata.mines.edu/products/vnl/#monthly\n",
        "1. GEE Python API: https://developers.google.com/earth-engine/tutorials/community/intro-to-python-api\n",
        "1. Tensorflow: https://www.tensorflow.org/about/bib \n",
        "1. GitHub Code Repo: https://github.com/isaiahlg/csci5922/blob/main/proj/unet_regression_virrs.ipynb\n",
        "1. ESA’s Copernicus Sentinel-2: https://sentinel.esa.int/web/sentinel/missions/sentinel-2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Appendix A: To Do List\n",
        "This is a list I used to guide my work on this project, along with some future ideas for my own future reference!\n",
        "\n",
        "Decide a model to Use\n",
        "- [x] Figure out how to extract image patches for model training\n",
        "- [x] Extract Sentinel 2 and Planet Data\n",
        "- [x] Figure out which type of model to use\n",
        "- [x] Figure out how to run one of these scripts  \n",
        "\n",
        "Get a Simple Notebook Running\n",
        "- [x] Get the notebook running locally with the tfrecords saved locally  \n",
        "- [x] Train a model with small subset of the data  \n",
        "- [x] Get the model prediction and inference code to run with local files\n",
        "- [x] Get a simple version of the model trained\n",
        "- [x] Substitute NLCD with nightlights  \n",
        "- [x] Retrain a simple model with the new data  \n",
        "\n",
        "Get the Model Running on the Lab Computer\n",
        "- [x] Set up a python env on the lab computer  \n",
        "- [x] Set up SSH on the lab computer\n",
        "- [x] Adapt script to run on lab computer\n",
        "- [x] Run on lab computer  \n",
        "\n",
        "Scale Up the Model \n",
        "- [x] Visualize the model accuracy by epoch\n",
        "- [x] Compare a prediction to the actual image\n",
        "- [x] Create a difference image\n",
        "- [x] Scale up the model with original image patches\n",
        "\n",
        "Figure out why the model didn't train well..\n",
        "- [x] Resetup model to run with the small image patches - got to RMSE 0.16\n",
        "- [x] Try to set up with image patches in SSA - won't learn :/\n",
        "- [x] Try adjusting the optimizer\n",
        "- [x] Try with smaller image patches in SSA just arond cities\n",
        "- [x] Update Africa large to evaluate on small image patches\n",
        "- [x] Compare performance of large and small model\n",
        "- [ ] Try tripling the number of cities\n",
        "\n",
        "Generate Imagery and Figures:\n",
        "- [x] Run predictions on the validation sets as well\n",
        "- [x] Create a standardized test set\n",
        "- [x] Rvaluate the large model on the small patches to compare performance\n",
        "\n",
        "Future Ideas for Improving Model Performance\n",
        "- [ ] Try adding in rural areas for train and eval\n",
        "- [ ] Get fewer samples per patch to see if it still performs as well\n",
        "- [ ] Consider rotating and mirroring the image patches, easy in Pytorch\n",
        "- [ ] Probably a lot of redundancy right now in training data\n",
        "- [ ] Try leave one out cross validation\n",
        "- [ ] Plot distributions of labels\n",
        " \n",
        "\n",
        "Future Ideas for New Models:\n",
        "- [ ] Substitute Landsat 8 with Sentinel 2  \n",
        "- [ ] Consider running again with Planet  \n",
        "- [ ] Consider running again with a ViTs\n",
        "- [ ] Consider running again with \n",
        "- [ ] Look wealth instead of nightlights data  \n",
        "- [ ] Look at effect of dataset size on model performance\n",
        "\n",
        "Parameters Tested Out:\n",
        "- [ ] Number of epochs\n",
        "- [ ] Batch size\n",
        "- [ ] Learning rate\n",
        "- [ ] Optimizer\n",
        "- [ ] Loss function\n",
        "- [ ] Model architecture\n",
        "- [ ] Image patch size\n",
        "- [ ] Number of image patches per city\n",
        "- [ ] Number of cities\n",
        "- [ ] Location of cities"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
